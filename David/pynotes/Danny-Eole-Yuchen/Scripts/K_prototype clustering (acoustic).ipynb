{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# K_prototype clustering on acoustic data\n",
    "\n",
    "K prototype clustering is introduced by Zhexue Huang, aiming perform k-means clustering analysis on large scale data set with mixed numeric and catgorical dataset.\n",
    "\n",
    "### k prototype= k means(on mumerical vars) + k mode(on categorical vars)\n",
    "\n",
    "\n",
    "reference:\n",
    "Extensions to the k-Means Algorithm for Clustering Large Data Sets with Categorical Values, ZHEXUE HUANG\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### k mode clustering\n",
    "\n",
    "It is applied to cluster categorical data sets by measure the dissimilarity of documents.\n",
    "\n",
    "### dissimilarity: the total mismatch of the corresponding attributes of two objects.\n",
    "The smaller the number of dissimiliarity, the more similar of two object\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### k mode install using pip\n",
    "\n",
    "pip install kmodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from kmodes import kmodes\n",
    "from collections import defaultdict\n",
    "from scipy import sparse\n",
    "from sklearn.utils.validation import check_array"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### define euclidean distance for dissimilarity\n",
    "\n",
    "Function reference: https://www.daniweb.com/programming/software-development/threads/502098/k-prototype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from kmodes import kmodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "save_load_path = '/Users/Dannyhsiao/Dropbox/MA755 Public (1)/pynotes/Danny-Eole-Yuchen/Pickles'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 117)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mss_df = pd.read_pickle(save_load_path+'/mss_df.pkl')\n",
    "mss_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>artist_latitude</th>\n",
       "      <th>artist_location</th>\n",
       "      <th>artist_longitude</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NaN</td>\n",
       "      <td>b'Wisner, LA'</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>NaN</td>\n",
       "      <td>b''</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>NaN</td>\n",
       "      <td>b'Edmonton, Alberta, Canada'</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>NaN</td>\n",
       "      <td>b'United Kingdom'</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>NaN</td>\n",
       "      <td>b''</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   artist_latitude               artist_location  artist_longitude\n",
       "0              NaN                 b'Wisner, LA'               NaN\n",
       "1              NaN                           b''               NaN\n",
       "2              NaN  b'Edmonton, Alberta, Canada'               NaN\n",
       "3              NaN             b'United Kingdom'               NaN\n",
       "4              NaN                           b''               NaN"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mss_df_location=mss_df[['artist_latitude','artist_location','artist_longitude']]\n",
    "mss_df_location.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "source: https://github.com/nicodv/kmodes/blob/master/kmodes/kmodes.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "import numpy as np\n",
    "from scipy import sparse\n",
    "from sklearn.base import BaseEstimator, ClusterMixin\n",
    "from sklearn.utils.validation import check_array\n",
    "\n",
    "\n",
    "def get_max_value_key(dic):\n",
    "    \"\"\"Fast method to get key for maximum value in dict.\"\"\"\n",
    "    v = list(dic.values())\n",
    "    k = list(dic.keys())\n",
    "    return k[v.index(max(v))]\n",
    "\n",
    "\n",
    "def matching_dissim(a, b):\n",
    "    \"\"\"Simple matching dissimilarity function\"\"\"\n",
    "    return np.sum(a != b, axis=1)\n",
    "\n",
    "\n",
    "def init_huang(X, n_clusters):\n",
    "    \"\"\"Initialize centroids according to method by Huang [1997].\"\"\"\n",
    "    nattrs = X.shape[1]\n",
    "    centroids = np.empty((n_clusters, nattrs), dtype='object')\n",
    "    # determine frequencies of attributes\n",
    "    for iattr in range(nattrs):\n",
    "        freq = defaultdict(int)\n",
    "        for curattr in X[:, iattr]:\n",
    "            freq[curattr] += 1\n",
    "        # Sample centroids using the probabilities of attributes.\n",
    "        # (I assume that's what's meant in the Huang [1998] paper; it works,\n",
    "        # at least)\n",
    "        # Note: sampling using population in static list with as many choices\n",
    "        # as frequency counts. Since the counts are small integers,\n",
    "        # memory consumption is low.\n",
    "        choices = [chc for chc, wght in freq.items() for _ in range(wght)]\n",
    "        centroids[:, iattr] = np.random.choice(choices, n_clusters)\n",
    "    # The previously chosen centroids could result in empty clusters,\n",
    "    # so set centroid to closest point in X.\n",
    "    for ik in range(n_clusters):\n",
    "        ndx = np.argsort(matching_dissim(X, centroids[ik]))\n",
    "        # We want the centroid to be unique.\n",
    "        while np.all(X[ndx[0]] == centroids, axis=1).any():\n",
    "            ndx = np.delete(ndx, 0)\n",
    "        centroids[ik] = X[ndx[0]]\n",
    "\n",
    "    return centroids\n",
    "\n",
    "\n",
    "def init_cao(X, n_clusters):\n",
    "    \"\"\"Initialize centroids according to method by Cao et al. [2009].\n",
    "    Note: O(N * attr * n_clusters**2), so watch out with large n_clusters\n",
    "    \"\"\"\n",
    "    npoints, nattrs = X.shape\n",
    "    centroids = np.empty((n_clusters, nattrs), dtype='object')\n",
    "    # Method is base don determining density of points.\n",
    "    dens = np.zeros(npoints)\n",
    "    for iattr in range(nattrs):\n",
    "        freq = defaultdict(int)\n",
    "        for val in X[:, iattr]:\n",
    "            freq[val] += 1\n",
    "        for ipoint in range(npoints):\n",
    "            dens[ipoint] += freq[X[ipoint, iattr]] / float(nattrs)\n",
    "    dens /= npoints\n",
    "\n",
    "    # Choose initial centroids based on distance and density.\n",
    "    centroids[0] = X[np.argmax(dens)]\n",
    "    if n_clusters > 1:\n",
    "        # For the remaining centroids, choose maximum dens * dissim to the\n",
    "        # (already assigned) centroid with the lowest dens * dissim.\n",
    "        for ik in range(1, n_clusters):\n",
    "            dd = np.empty((ik, npoints))\n",
    "            for ikk in range(ik):\n",
    "                dd[ikk] = matching_dissim(X, centroids[ikk]) * dens\n",
    "            centroids[ik] = X[np.argmax(np.min(dd, axis=0))]\n",
    "\n",
    "    return centroids\n",
    "\n",
    "\n",
    "def move_point_cat(point, ipoint, to_clust, from_clust, cl_attr_freq,\n",
    "                   membship, centroids):\n",
    "    \"\"\"Move point between clusters, categorical attributes.\"\"\"\n",
    "    membship[to_clust, ipoint] = 1\n",
    "    membship[from_clust, ipoint] = 0\n",
    "    # Update frequencies of attributes in cluster.\n",
    "    for iattr, curattr in enumerate(point):\n",
    "        to_attr_counts = cl_attr_freq[to_clust][iattr]\n",
    "        from_attr_counts = cl_attr_freq[from_clust][iattr]\n",
    "\n",
    "        # Increment the attribute count for the new \"to\" cluster\n",
    "        to_attr_counts[curattr] += 1\n",
    "\n",
    "        current_attribute_value_freq = to_attr_counts[curattr]\n",
    "        current_centroid_value = centroids[to_clust][iattr]\n",
    "        current_centroid_freq = to_attr_counts[current_centroid_value]\n",
    "        if current_centroid_freq < current_attribute_value_freq:\n",
    "            # We have incremented this value to the new mode. Update the centroid.\n",
    "            centroids[to_clust][iattr] = curattr\n",
    "\n",
    "        # Decrement the attribute count for the old \"from\" cluster\n",
    "        from_attr_counts[curattr] -= 1\n",
    "\n",
    "        old_centroid_value = centroids[from_clust][iattr]\n",
    "        if old_centroid_value == curattr:\n",
    "            # We have just removed a count from the old centroid value. We need to \n",
    "            # recalculate the centroid as it may no longer be the maximum\n",
    "            centroids[from_clust][iattr] = get_max_value_key(from_attr_counts)\n",
    "\n",
    "    return cl_attr_freq, membship, centroids\n",
    "\n",
    "\n",
    "def encode_features(X, enc_map=None):\n",
    "    \"\"\"Converts categorical values in each column of X to integers in the range\n",
    "    [0, n_unique_values_in_column - 1], if X is not already of integer type.\n",
    "    Unknown values get a value of -1.\n",
    "    If mapping is not provided, it is calculated based on the valus in X.\n",
    "    \"\"\"\n",
    "    if np.issubdtype(X.dtype, np.integer):\n",
    "        # Already integer type. do nothing.\n",
    "        return X, enc_map\n",
    "\n",
    "    if enc_map is None:\n",
    "        fit = True\n",
    "        # We will calculate enc_map, so initialize the list of column mappings.\n",
    "        enc_map = []\n",
    "    else:\n",
    "        fit = False\n",
    "\n",
    "    Xenc = np.zeros(X.shape).astype('int')\n",
    "    for ii in range(X.shape[1]):\n",
    "        if fit:\n",
    "            enc_map.append({val: jj for jj, val in enumerate(np.unique(X[:, ii]))})\n",
    "        # Unknown categories when predicting all get a value of -1.\n",
    "        Xenc[:, ii] = np.array([enc_map[ii].get(x, -1) for x in X[:, ii]])\n",
    "\n",
    "    return Xenc, enc_map\n",
    "\n",
    "\n",
    "def _labels_cost(X, centroids):\n",
    "    \"\"\"Calculate labels and cost function given a matrix of points and\n",
    "    a list of centroids for the k-modes algorithm.\n",
    "    \"\"\"\n",
    "\n",
    "    X = check_array(X)\n",
    "\n",
    "    npoints = X.shape[0]\n",
    "    cost = 0.\n",
    "    labels = np.empty(npoints, dtype=np.uint8)\n",
    "    for ipoint, curpoint in enumerate(X):\n",
    "        diss = matching_dissim(centroids, curpoint)\n",
    "        clust = np.argmin(diss)\n",
    "        labels[ipoint] = clust\n",
    "        cost += diss[clust]\n",
    "\n",
    "    return labels, cost\n",
    "\n",
    "\n",
    "def _k_modes_iter(X, centroids, cl_attr_freq, membship):\n",
    "    \"\"\"Single iteration of k-modes clustering algorithm\"\"\"\n",
    "    moves = 0\n",
    "    for ipoint, curpoint in enumerate(X):\n",
    "        clust = np.argmin(matching_dissim(centroids, curpoint))\n",
    "        if membship[clust, ipoint]:\n",
    "            # Point is already in its right place.\n",
    "            continue\n",
    "\n",
    "        # Move point, and update old/new cluster frequencies and centroids.\n",
    "        moves += 1\n",
    "        old_clust = np.argwhere(membship[:, ipoint])[0][0]\n",
    "\n",
    "        cl_attr_freq, membship, centroids = move_point_cat(\n",
    "            curpoint, ipoint, clust, old_clust, cl_attr_freq, membship, centroids\n",
    "        )\n",
    "\n",
    "        # In case of an empty cluster, reinitialize with a random point\n",
    "        # from the largest cluster.\n",
    "        if np.sum(membship[old_clust, :]) == 0:\n",
    "            from_clust = membship.sum(axis=1).argmax()\n",
    "            choices = [ii for ii, ch in enumerate(membship[from_clust, :]) if ch]\n",
    "            rindx = np.random.choice(choices)\n",
    "\n",
    "            cl_attr_freq, membship, centroids = move_point_cat(\n",
    "                X[rindx], rindx, old_clust, from_clust, cl_attr_freq, membship, centroids\n",
    "            )\n",
    "\n",
    "    return centroids, moves\n",
    "\n",
    "\n",
    "def k_modes(X, n_clusters, init, n_init, max_iter, verbose):\n",
    "    \"\"\"k-modes algorithm\"\"\"\n",
    "\n",
    "    if sparse.issparse(X):\n",
    "        raise TypeError(\"k-modes does not support sparse data.\")\n",
    "\n",
    "    X = check_array(X, dtype=None)\n",
    "\n",
    "    # Convert the categorical values in X to integers for speed.\n",
    "    # Based on the unique values in X, we can make a mapping to achieve this.\n",
    "    X, enc_map = encode_features(X)\n",
    "\n",
    "    npoints, nattrs = X.shape\n",
    "    assert n_clusters < npoints, \"More clusters than data points?\"\n",
    "\n",
    "    all_centroids = []\n",
    "    all_labels = []\n",
    "    all_costs = []\n",
    "    all_n_iters = []\n",
    "    for init_no in range(n_init):\n",
    "\n",
    "        # _____ INIT _____\n",
    "        if verbose:\n",
    "            print(\"Init: initializing centroids\")\n",
    "        if isinstance(init, str) and init == 'Huang':\n",
    "            centroids = init_huang(X, n_clusters)\n",
    "        elif isinstance(init, str) and init == 'Cao':\n",
    "            centroids = init_cao(X, n_clusters)\n",
    "        elif isinstance(init, str) and init == 'random':\n",
    "            seeds = np.random.choice(range(npoints), n_clusters)\n",
    "            centroids = X[seeds]\n",
    "        elif hasattr(init, '__array__'):\n",
    "            assert init.shape[0] == n_clusters, \"Too many initial centroids in init.\"\n",
    "            assert init.shape[1] == nattrs, \"Too many attributes in init for X.\"\n",
    "            centroids = np.asarray(init, dtype=np.uint8)\n",
    "        else:\n",
    "            raise NotImplementedError\n",
    "\n",
    "        if verbose:\n",
    "            print(\"Init: initializing clusters\")\n",
    "        membship = np.zeros((n_clusters, npoints), dtype=np.uint8)\n",
    "        # cl_attr_freq is a list of lists with dictionaries that contain the\n",
    "        # frequencies of values per cluster and attribute.\n",
    "        cl_attr_freq = [[defaultdict(int) for _ in range(nattrs)]\n",
    "                        for _ in range(n_clusters)]\n",
    "        for ipoint, curpoint in enumerate(X):\n",
    "            # Initial assignment to clusters\n",
    "            clust = np.argmin(matching_dissim(centroids, curpoint))\n",
    "            membship[clust, ipoint] = 1\n",
    "            # Count attribute values per cluster.\n",
    "            for iattr, curattr in enumerate(curpoint):\n",
    "                cl_attr_freq[clust][iattr][curattr] += 1\n",
    "        # Perform an initial centroid update.\n",
    "        for ik in range(n_clusters):\n",
    "            for iattr in range(nattrs):\n",
    "                if sum(membship[ik]) == 0:\n",
    "                    # Empty centroid, choose randomly\n",
    "                    centroids[ik, iattr] = np.random.choice(X[:, iattr])\n",
    "                else:\n",
    "                    centroids[ik, iattr] = get_max_value_key(cl_attr_freq[ik][iattr])\n",
    "\n",
    "        # _____ ITERATION _____\n",
    "        if verbose:\n",
    "            print(\"Starting iterations...\")\n",
    "        itr = 0\n",
    "        converged = False\n",
    "        cost = np.Inf\n",
    "        while itr <= max_iter and not converged:\n",
    "            itr += 1\n",
    "            centroids, moves = _k_modes_iter(X, centroids, cl_attr_freq, membship)\n",
    "            # All points seen in this iteration\n",
    "            labels, ncost = _labels_cost(X, centroids)\n",
    "            converged = (moves == 0) or (ncost >= cost)\n",
    "            cost = ncost\n",
    "            if verbose:\n",
    "                print(\"Run {}, iteration: {}/{}, moves: {}, cost: {}\"\n",
    "                      .format(init_no + 1, itr, max_iter, moves, cost))\n",
    "\n",
    "        # Store result of current run.\n",
    "        all_centroids.append(centroids)\n",
    "        all_labels.append(labels)\n",
    "        all_costs.append(cost)\n",
    "        all_n_iters.append(itr)\n",
    "\n",
    "    best = np.argmin(all_costs)\n",
    "    if n_init > 1 and verbose:\n",
    "        print(\"Best run was number {}\".format(best + 1))\n",
    "\n",
    "    return all_centroids[best], all_labels[best], all_costs[best], \\\n",
    "        all_n_iters[best], enc_map\n",
    "\n",
    "\n",
    "class KModes(BaseEstimator, ClusterMixin):\n",
    "\n",
    "    \"\"\"k-modes clustering algorithm for categorical data.\n",
    "    Parameters\n",
    "    -----------\n",
    "    n_clusters : int, optional, default: 8\n",
    "        The number of clusters to form as well as the number of\n",
    "        centroids to generate.\n",
    "    max_iter : int, default: 300\n",
    "        Maximum number of iterations of the k-modes algorithm for a\n",
    "        single run.\n",
    "    n_init : int, default: 10\n",
    "        Number of time the k-modes algorithm will be run with different\n",
    "        centroid seeds. The final results will be the best output of\n",
    "        n_init consecutive runs in terms of cost.\n",
    "    init : {'Huang', 'Cao', 'random' or an ndarray}\n",
    "        Method for initialization:\n",
    "        'Huang': Method in Huang [1997, 1998]\n",
    "        'Cao': Method in Cao et al. [2009]\n",
    "        'random': choose 'n_clusters' observations (rows) at random from\n",
    "        data for the initial centroids.\n",
    "        If an ndarray is passed, it should be of shape (n_clusters, n_features)\n",
    "        and gives the initial centroids.\n",
    "    verbose : integer, optional\n",
    "        Verbosity mode.\n",
    "    Attributes\n",
    "    ----------\n",
    "    cluster_centroids_ : array, [n_clusters, n_features]\n",
    "        Categories of cluster centroids\n",
    "    labels_ :\n",
    "        Labels of each point\n",
    "    cost_ : float\n",
    "        Clustering cost, defined as the sum distance of all points to\n",
    "        their respective cluster centroids.\n",
    "    Notes\n",
    "    -----\n",
    "    See:\n",
    "    Huang, Z.: Extensions to the k-modes algorithm for clustering large\n",
    "    data sets with categorical values, Data Mining and Knowledge\n",
    "    Discovery 2(3), 1998.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, n_clusters=8, init='Cao', n_init=1, max_iter=100, verbose=0):\n",
    "\n",
    "        self.n_clusters = n_clusters\n",
    "        self.init = init\n",
    "        self.n_init = n_init\n",
    "        self.verbose = verbose\n",
    "        if ((isinstance(self.init, str) and self.init == 'Cao') or\n",
    "                hasattr(self.init, '__array__')) and self.n_init > 1:\n",
    "            if self.verbose:\n",
    "                print(\"Initialization method and algorithm are deterministic. \"\n",
    "                      \"Setting n_init to 1.\")\n",
    "            self.n_init = 1\n",
    "\n",
    "        self.max_iter = max_iter\n",
    "\n",
    "    def fit(self, X, y=None, **kwargs):\n",
    "        \"\"\"Compute k-modes clustering.\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : array-like, shape=[n_samples, n_features]\n",
    "        \"\"\"\n",
    "\n",
    "        self.cluster_centroids_, self.labels_, self.cost_, self.n_iter_, self.enc_map_ = \\\n",
    "            k_modes(X,\n",
    "                    self.n_clusters,\n",
    "                    self.init,\n",
    "                    self.n_init,\n",
    "                    self.max_iter,\n",
    "                    self.verbose)\n",
    "        return self\n",
    "\n",
    "    def fit_predict(self, X, y=None, **kwargs):\n",
    "        \"\"\"Compute cluster centroids and predict cluster index for each sample.\n",
    "        Convenience method; equivalent to calling fit(X) followed by\n",
    "        predict(X).\n",
    "        \"\"\"\n",
    "        return self.fit(X, **kwargs).labels_\n",
    "\n",
    "    def predict(self, X, **kwargs):\n",
    "        \"\"\"Predict the closest cluster each sample in X belongs to.\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : array-like, shape = [n_samples, n_features]\n",
    "            New data to predict.\n",
    "        Returns\n",
    "        -------\n",
    "        labels : array, shape [n_samples,]\n",
    "            Index of the cluster each sample belongs to.\n",
    "        \"\"\"\n",
    "        assert hasattr(self, 'cluster_centroids_'), \"Model not yet fitted.\"\n",
    "        X = check_array(X, dtype=None)\n",
    "        X, _ = encode_features(X, enc_map=self.enc_map_)\n",
    "        return _labels_cost(X, self.cluster_centroids_)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
