{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "//\n",
    "//                       _oo0oo_\n",
    "//                      o8888888o\n",
    "//                      88\" . \"88\n",
    "//                      (| -_- |)\n",
    "//                      0\\  =  /0\n",
    "//                    ___/`---'\\___\n",
    "//                  .' \\\\|     |// '.\n",
    "//                 / \\\\|||  :  |||// \\\n",
    "//                / _||||| -:- |||||- \\\n",
    "//               |   | \\\\\\  -  /// |   |\n",
    "//               | \\_|  ''\\---/''  |_/ |\n",
    "//               \\  .-\\__  '-'  ___/-. /\n",
    "//             ___'. .'  /--.--\\  `. .'___\n",
    "//          .\"\" '<  `.___\\_<|>_/___.' >' \"\".\n",
    "//         | | :  `- \\`.;`\\ _ /`;.`/ - ` : | |\n",
    "//         \\  \\ `_.   \\_ __\\ /__ _/   .-` /  /\n",
    "//     =====`-.____`.___ \\_____/___.-`___.-'=====\n",
    "//                       `=---='\n",
    "//\n",
    "//\n",
    "//     ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "//\n",
    "//               佛祖保佑         永无BUG\n",
    "//\n",
    "//"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CLUSTER ANALYSIS\n",
    "\n",
    "## Inputs\n",
    "1. lyrics data only\n",
    "1. song data only\n",
    "1. hybrid lyrics and song data\n",
    "\n",
    "## Weighting\n",
    "1. Tf\n",
    "1. Tf-Idf\n",
    "1. Binary\n",
    "\n",
    "## PCA\n",
    "\n",
    "## Clustering Technique\n",
    "1. PAM\n",
    "1. DBSCAN\n",
    "1. K-Means\n",
    "\n",
    "# ------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Choose user: ['Eole', 'Danny', 'Cindy']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Choose a user:\n",
    "USER = 'Eole'\n",
    "\n",
    "# |\n",
    "# |\n",
    "# |\n",
    "# |\n",
    "# |\n",
    "# |\n",
    "# v\n",
    "\n",
    "# Check input user...\n",
    "USER_LIST = ['Eole', 'Danny', 'Cindy']\n",
    "assert (USER in USER_LIST), \\\n",
    "\"`USER` value is invalid: {user_val} \\nChoose value in: {user_list}\".format(user_val = USER, user_list = USER_LIST)\n",
    "\n",
    "# Define file path\n",
    "save_load_path = None\n",
    "\n",
    "if USER == 'Eole':\n",
    "    save_load_path = '/home/eolus/Dropbox/MA755 Public/pynotes/Danny-Eole-Yuchen/Pickles'\n",
    "elif USER == 'Danny':\n",
    "    save_load_path = '/Users/Dannyhsiao/Dropbox/MA755 Public (1)/pynotes/Danny-Eole-Yuchen/Pickles'\n",
    "elif USER == 'Cindy':\n",
    "    save_load_path = '/Users/YuchenZhou/Dropbox/MA755 Public/pynotes/Danny-Eole-Yuchen/Pickles'\n",
    "        \n",
    "# PATHS...\n",
    "# '/Users/Dannyhsiao/Dropbox/MA755 Public/pynotes/Danny-Eole-Yuchen/Pickles'\n",
    "# '/home/eolus/Dropbox/MA755 Public/pynotes/Danny-Eole-Yuchen/Pickles'\n",
    "# '/Users/YuchenZhou/Dropbox/MA755 Public/pynotes/Danny-Eole-Yuchen/Pickles'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Input selection: ['lyrics', 'song', 'hybrid']\n",
    "\n",
    "+ lyrics data only\n",
    "+ song data only\n",
    "+ lyrics and song hybrid (not available yet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Choose data source here:\n",
    "DATA = 'lyrics'\n",
    "\n",
    "# |\n",
    "# |\n",
    "# |\n",
    "# |\n",
    "# |\n",
    "# |\n",
    "# v\n",
    "\n",
    "\n",
    "DATA_LIST = ['lyrics', 'song', 'hybrid']\n",
    "input_file = None\n",
    "\n",
    "\n",
    "assert (DATA in DATA_LIST), \\\n",
    "\"`DATA` value is invalid: {data_val} \\nChoose value in: {data_list}\".format(data_val = DATA, data_list = DATA_LIST)\n",
    "\n",
    "if DATA == 'lyrics':\n",
    "    input_file = 'mss_lyrics_pvt_df.pkl'\n",
    "elif DATA == 'song':\n",
    "    input_file = 'mss_song_df.pkl' # Confirm input file name /!\\\n",
    "elif DATA == 'hybrid':\n",
    "    input_file = 'mss_hybrid_df.pkl' # Confirm input file name /!\\\n",
    "else:\n",
    "    exception"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>word</th>\n",
       "      <th>&amp;</th>\n",
       "      <th>000</th>\n",
       "      <th>1</th>\n",
       "      <th>10</th>\n",
       "      <th>100</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>15</th>\n",
       "      <th>16</th>\n",
       "      <th>2</th>\n",
       "      <th>...</th>\n",
       "      <th>è</th>\n",
       "      <th>é</th>\n",
       "      <th>él</th>\n",
       "      <th>és</th>\n",
       "      <th>était</th>\n",
       "      <th>être</th>\n",
       "      <th>ô</th>\n",
       "      <th>über</th>\n",
       "      <th>–</th>\n",
       "      <th>‘caus</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>track</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>TRAAABD128F429CF47</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TRAAAEF128F4273421</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TRAAAFD128F92F423A</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TRAAARJ128F9320760</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TRAABJV128F1460C49</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 4735 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "word                &  000  1  10  100  12  13  15  16  2  ...    è  é  él  \\\n",
       "track                                                      ...               \n",
       "TRAAABD128F429CF47  0    0  0   0    0   0   0   0   0  0  ...    0  0   0   \n",
       "TRAAAEF128F4273421  0    0  0   0    0   0   0   0   0  0  ...    0  0   0   \n",
       "TRAAAFD128F92F423A  0    0  0   0    0   0   0   0   0  0  ...    0  0   0   \n",
       "TRAAARJ128F9320760  0    0  0   0    0   0   0   0   0  0  ...    0  0   0   \n",
       "TRAABJV128F1460C49  0    0  0   0    0   0   0   0   0  0  ...    0  0   0   \n",
       "\n",
       "word                és  était  être  ô  über  –  ‘caus  \n",
       "track                                                   \n",
       "TRAAABD128F429CF47   0      0     0  0     0  0      0  \n",
       "TRAAAEF128F4273421   0      0     0  0     0  0      0  \n",
       "TRAAAFD128F92F423A   0      0     0  0     0  0      0  \n",
       "TRAAARJ128F9320760   0      0     0  0     0  0      0  \n",
       "TRAABJV128F1460C49   0      0     0  0     0  0      0  \n",
       "\n",
       "[5 rows x 4735 columns]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mss_df = pd.read_pickle(save_load_path+'/mss_lyrics_pvt_df.pkl')\n",
    "mss_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import user-defined stop words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading stop words...\n",
      "Number of new stop words: 2091 \n",
      "\n",
      "Sample:\n",
      "know\n",
      "like\n",
      "time\n",
      "go\n",
      "one\n",
      "see\n",
      "get\n",
      "come\n",
      "got\n",
      "never\n",
      "say\n",
      "make\n",
      "way\n",
      "take\n",
      "want\n",
      "ca\n",
      "let\n",
      "feel\n",
      "day\n",
      "would\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "from itertools import chain\n",
    "\n",
    "\n",
    "list_csv = []\n",
    "\n",
    "print(\"Loading stop words...\")\n",
    "with open(save_load_path+'/custom_stop_words.csv', 'r') as f:\n",
    "    reader = csv.reader(f)\n",
    "    list_csv = list(reader)\n",
    "    \n",
    "custom_stop_words = list(chain.from_iterable(list_csv))\n",
    "\n",
    "print(\"Number of new stop words: {len_stop} \\n\".format(len_stop = len(custom_stop_words)))\n",
    "\n",
    "print('Sample:')\n",
    "for i in range(20):\n",
    "    print(custom_stop_words[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remove custom stop-words from lyrics dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removing custom stop words...\n",
      "Number of terms before removing stop words: 4735\n",
      "Number of terms after removing stop words: 2644\n"
     ]
    }
   ],
   "source": [
    "print('Removing custom stop words...')\n",
    "print('Number of terms before removing stop words: {df_width}'.format(df_width = mss_df.shape[1]))\n",
    "\n",
    "for stopword in custom_stop_words:\n",
    "    mss_df = mss_df.drop(stopword, 1)\n",
    "\n",
    "print('Number of terms after removing stop words: {df_width}'.format(df_width = mss_df.shape[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# save cleaned dataframe as pickle file\n",
    "mss_df.to_pickle(save_load_path+'/mss_lyrics_pvt_cleaned_v1_df.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remove non-english documents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PCA - MIKE®\n",
    "\n",
    "Thanks to Mike's presentation on PCA, we apply the function.\n",
    "Resource:\n",
    "http://sebastianraschka.com/Articles/2014_pca_step_by_step.html\n",
    "\n",
    "Recap:\n",
    "1. Turn original data into matrix. (for later product of PC score)\n",
    "2. Apply the function of Principle Component\n",
    "3. Extract Eigen Value\n",
    "4. Extract Eigen Vector (PCA loading to original variables)\n",
    "5. Build loading matrix (w_matrix)\n",
    "6. Attain PC scores for original rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-4-a788e0521ca9>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mitertools\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mre\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 9\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpreprocessing\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mscale\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     10\u001b[0m \u001b[0mget_ipython\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmagic\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'matplotlib inline'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/eolus/anaconda3/lib/python3.5/site-packages/sklearn/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     55\u001b[0m \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     56\u001b[0m     \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0m__check_build\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 57\u001b[1;33m     \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0mbase\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mclone\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     58\u001b[0m     \u001b[0m__check_build\u001b[0m  \u001b[1;31m# avoid flakes unused variable error\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     59\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/eolus/anaconda3/lib/python3.5/site-packages/sklearn/base.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 9\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mscipy\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0msparse\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     10\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0mexternals\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0msix\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfixes\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0msignature\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/eolus/anaconda3/lib/python3.5/site-packages/scipy/sparse/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m    223\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    224\u001b[0m \u001b[1;31m# for backward compatibility with v0.10.  This function is marked as deprecated\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 225\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0mcsgraph\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mcs_graph_components\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    226\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    227\u001b[0m \u001b[1;31m#from spfuncs import *\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/eolus/anaconda3/lib/python3.5/site-packages/scipy/sparse/csgraph/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m    168\u001b[0m     \u001b[0mbreadth_first_tree\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdepth_first_tree\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mconnected_components\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    169\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0m_min_spanning_tree\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mminimum_spanning_tree\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 170\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0m_reordering\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mreverse_cuthill_mckee\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmaximum_bipartite_matching\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    171\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0m_tools\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mconstruct_dist_matrix\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreconstruct_path\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0;31m\\\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    172\u001b[0m     \u001b[0mcsgraph_from_dense\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcsgraph_to_dense\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcsgraph_masked_from_dense\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0;31m\\\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mscipy/sparse/csgraph/_reordering.pyx\u001b[0m in \u001b[0;36minit scipy.sparse.csgraph._reordering (scipy/sparse/csgraph/_reordering.c:24503)\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32m/home/eolus/anaconda3/lib/python3.5/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_find_and_load\u001b[1;34m(name, import_)\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import os\n",
    "import itertools as it\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "import h5py \n",
    "import os\n",
    "import itertools\n",
    "import re\n",
    "from sklearn.preprocessing import scale\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Turn our original data into matrix\n",
    "\n",
    "dtm = mss_df.drop('track',1) #Drop the track index\n",
    "dtm_matrix = dtm.as_matrix()\n",
    "\n",
    "print (\"document term matrix:\")\n",
    "print ()\n",
    "print (dtm_matrix)\n",
    "print ()\n",
    "print (\"shape:{shape}\".format(shape = dtm_matrix.shape))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Term weighting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Binary weighting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Document term frequency matrix binary weighted\n",
    "dtm_matrix_binary = dtm_matrix\n",
    "dtm_matrix_binary[dtm_matrix_binary > 1] = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tf-Idf  weighting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Document term frequency matrix `tfIdf` weighted\n",
    "\n",
    "# <...CODE...>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "weighting_option_list = ['binary', 'tf', 'tfidf']\n",
    "\n",
    "# [0] - 'binary'\n",
    "# [1] - 'tf'\n",
    "# [2] - 'tfidf'\n",
    "weighting_option = 0\n",
    "\n",
    "if weighting_option_list[weighting_option] == 'binary':\n",
    "    dtm_matrix = dtm_matrix_binary\n",
    "elif weighting_option_list[weighting_option] == 'tfidf':\n",
    "      \n",
    "# else: <...>\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Returns series of eign values & eign vectors -- MIKE®\n",
    "\n",
    "def pcf(matrix):\n",
    "    #scale the data\n",
    "    A_s = scale(matrix, with_mean=True,with_std=True,axis=0)\n",
    "    #covariance matrix\n",
    "    A_c = np.cov(A_s.T)\n",
    "    #Get the eign values and eign vectors\n",
    "    eig_val_cov, eig_vec_cov = np.linalg.eig(A_c)\n",
    "    eig_pairs = [(np.abs(eig_val_cov[i]), eig_vec_cov[:,i])\n",
    "             for i in range(len(eig_val_cov))]\n",
    "    # Sort the (eigenvalue, eigenvector) tuples from high to low\n",
    "    eig_pairs.sort\n",
    "    eig_pairs.reverse\n",
    "    return(eig_pairs)\n",
    "\n",
    "pc = pcf(dtm_matrix) # Each PC is a list of two items: \n",
    "                     #         1. the 'Eigen Value' \n",
    "                     #         2. an array of the 'Eigen vector' (loadings to each variable)\n",
    "\n",
    "print(pc[0]) # We got the first Principle Component (PC1: the one with the highest eigen value) and the its eigen vector.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# This function extracts the eign value out of the eign value / vector pair -- MIKE®\n",
    "\n",
    "def extract_eign(pair):\n",
    "    eig_val=[0]*len(pair)\n",
    "    index=0\n",
    "    for i in pair:\n",
    "        eig_val[index]=i[0]\n",
    "        index+=1\n",
    "    return(eig_val)\n",
    "\n",
    "# Now we have all the eigne values of PCs. We decide how many PCs we want to apply (How many PCs we want to drop). \n",
    "# (The fewer, the better. But it trades of ability to explain the data)\n",
    "\n",
    "pceign=extract_eign(pc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# We plot the eign values of all PCs. \n",
    "# We drop PCs if its eign value lower than 1. (rule of thumb)\n",
    "\n",
    "plt.plot(pceign)\n",
    "plt.axis([0,1500,0,20])\n",
    "plt.axhline(y=1,color='r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Here we want to know how much data is explained in a certain number of PCs we applied.\n",
    "\n",
    "def var_explained(eig,i):\n",
    "    cum_per=0\n",
    "    per_var=0\n",
    "    for e_val in (eig[0:i]):\n",
    "        per_var = round((e_val / len(eig)),3)\n",
    "        cum_per+=per_var\n",
    "    return(cum_per)\n",
    "\n",
    "# 690 PCs: PC1, PC2, PC3, PC4 and PC5 ... PC690 provide 81.4% of the information. \n",
    "# PC691 and the following PCs contribute very little to explain the variables.\n",
    "var_explained(pceign,690)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PC scores\n",
    "\n",
    "Here is the part Mike didn't cover in his presentation: To get the PC scores.\n",
    "\n",
    "1. Extract Eigen vector of PC1, PC2 .....PC690\n",
    "2. Form the loading matrix in 4734 * 690 (w_matrix)\n",
    "3. Product loading matrix to the original 2350 * 4734 matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Choose number of PCs based on graph\n",
    "num_pc = 690\n",
    "\n",
    "# Eigen vector len = number of track_IDs\n",
    "len_vector = dtm_matrix.shape[1]\n",
    "\n",
    "# w_matrix stores eigen vectors vertically (col = PC, row = track_id)  ***it is not track_id but original variables\n",
    "\n",
    "# Initialize\n",
    "w_matrix = np.empty([len_vector,1], dtype = float)\n",
    "\n",
    "# Fill by stacking eigen vectors vertically\n",
    "for i in range(num_pc):\n",
    "    pc_col = pc[i][1].reshape(len_vector,1)\n",
    "    w_matrix = np.hstack((w_matrix, pc_col))\n",
    "\n",
    "# Delete initialized empty column on left hand side   ***I think we delete it from the begining\n",
    "w_matrix2 = np.delete(w_matrix,0,1).astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# w_matrix: rows (original variables in 4734)\n",
    "#           columns (selected PCs in 690)\n",
    "\n",
    "print(w_matrix2.shape)\n",
    "print('****************************************************')\n",
    "print(w_matrix2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Finally, reset each rows (each song) according to the loading of PCs. \n",
    "# It is the PC score of each row to each PC. \n",
    "\n",
    "pc_scores = dtm_matrix.dot(w_matrix2)\n",
    "pc_scores.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pc_scores\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Checkpoint - pickle pickle pickle!\n",
    "import pickle as pk\n",
    "\n",
    "f1 = open(save_load_path+'/w_matrix.pkl','wb')\n",
    "pk.dump(w_matrix,f1)\n",
    "\n",
    "f1 = open(save_load_path+'/pc_scores.pkl','wb')\n",
    "pk.dump(pc_scores,f1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cluster Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#  Read pickles\n",
    "w_matrix = np.load(save_load_path+'/w_matrix.pkl')\n",
    "pc_scores = np.load(save_load_path+'/pc_scores.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### K-means clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from scipy.cluster.vq import kmeans,vq,kmeans2\n",
    "from sklearn.metrics import *\n",
    "from numpy import random\n",
    "from sklearn.cluster import KMeans\n",
    "from scipy.spatial.distance import cdist, pdist\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.metrics import silhouette_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Silhoette coefficients near +1 indicate that the sample is far away from the neighboring clusters.\n",
    "#0 indicates that the sample is on or very close to the decision boundary between two neighboring clusters and negative \n",
    "#values indicate that those samples might have been assigned to the wrong cluster.\n",
    "\n",
    "# Try K from K=2 to K=6\n",
    "random.seed((1000,2000))\n",
    "for i in range(2,9):\n",
    "\n",
    "    # computing K-Means with k = i\n",
    "    centroids,_ = kmeans(pc_scores, i)\n",
    "\n",
    "    # assign each sample to a cluster\n",
    "    idx,_ = vq(pc_scores,centroids)\n",
    "\n",
    "    print(\"Clusters: %r  |  Silhouette Coeff: %0.3f\"\n",
    "          % (i, silhouette_score(pc_scores, idx, metric='euclidean')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#centroids,_ = kmeans(pc_scores,5)\n",
    "#idx,_ = vq(pc_scores, centroids)\n",
    "#idx\n",
    "\n",
    "# Generate 5 centroids and accordingly cluster lables\n",
    "random.seed((1000,2000))\n",
    "\n",
    "centroids,label=kmeans2(pc_scores,5)\n",
    "label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "kmeans = KMeans()\n",
    "kmeans.fit(pc_scores)\n",
    "k_range=range(2,500)\n",
    "k_means_var=[KMeans(n_clusters=k).fit(pc_scores) for k in k_range]\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "centroids=[X.cluster_centers_ for X in k_means_var]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# calculate the euclidian distance from each point to each cluster center\n",
    "k_euclid = [cdist(pc_scores, cent, 'euclidean') for cent in centroids]\n",
    "dist = [np.min(ke, axis=1) for ke in k_euclid]\n",
    "\n",
    "#Total within-cluster sum of squares\n",
    "wcss = [sum(d**2) for d in dist]\n",
    "\n",
    "#Total sum of squares\n",
    "tss = sum(pdist(pc_scores)**2/pc_scores.shape[0])\n",
    "\n",
    "#Between-cluster sum of squares\n",
    "bss = tss - wcss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fig=plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "ax.plot(k_range, bss/tss*100, 'b*-')\n",
    "ax.set_ylim((0,100))\n",
    "plt.grid(True)\n",
    "plt.xlabel('n_clusters')\n",
    "plt.ylabel('Percentage of variance explained')\n",
    "plt.title('Variance Explained vs k')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "k_means = KMeans(n_clusters=7)\n",
    "k_means.fit(pc_scores)\n",
    "labels = k_means.labels_\n",
    "labels.count()\n",
    "silhouette_score(pc_scores, labels, metric='euclidean')#this is bad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "# x_min, x_max = pc_scores[:, 0].min() - 5, pc_scores[:, 0].max() - 1\n",
    "# y_min, y_max = pc_scores[:, 1].min() + 1, pc_scores[:, 1].max() + 5\n",
    "# xx, yy = np.meshgrid(np.arange(x_min, x_max, .02), np.arange(y_min, y_max, .02))\n",
    "# Z = k_means.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "# Z = Z.reshape(xx.shape)\n",
    "\n",
    "# plt.figure(1)\n",
    "# plt.clf()\n",
    "# plt.imshow(Z, interpolation='nearest',\n",
    "#           extent=(xx.min(), xx.max(), yy.min(), yy.max()),\n",
    "#           cmap=plt.cm.Paired,\n",
    "#           aspect='auto', origin='lower')\n",
    "\n",
    "# plt.plot(pc_scores[:, 0], pc_scores[:, 1], 'k.', markersize=4)\n",
    "# # Plot the centroids as a white X\n",
    "# centroids = k_means.cluster_centers_\n",
    "# inert = k_means.inertia_\n",
    "# plt.scatter(centroids[:, 0], centroids[:, 1],\n",
    "#            marker='x', s=169, linewidths=3,\n",
    "#            color='w', zorder=8)\n",
    "# plt.xlim(x_min, x_max)\n",
    "# plt.ylim(y_min, y_max)\n",
    "# plt.xticks(())\n",
    "# plt.yticks(())\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.cluster import DBSCAN\n",
    "\n",
    "# Compute DBSCAN\n",
    "db = DBSCAN(eps=10, metric='euclidean', min_samples=2).fit(pc_scores)\n",
    "# core_samples_mask = np.zeros_like(db.labels_, dtype=bool)\n",
    "# core_samples_mask[db.core_sample_indices_] = True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "labels = db.labels_\n",
    "\n",
    "res = Counter(labels)\n",
    "\n",
    "print('\"-1\" is unlabelled group')\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Number of clusters in labels, ignoring noise if present.\n",
    "n_clusters_ = len(set(labels)) - (1 if -1 in labels else 0)\n",
    "\n",
    "print('Estimated number of clusters: %d' % n_clusters_)\n",
    "print(\"Silhouette Coefficient: %0.3f\"\n",
    "      % silhouette_score(pc_scores, labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# WORK IN PROGRESS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### CRIME SCENE DO NOT CROSS - CRIME SCENE DO NOT CROSS - CRIME SCEN.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# import scipy package for clustering\n",
    "\n",
    "from scipy.cluster.hierarchy import dendrogram, linkage\n",
    "\n",
    "# Asume our new PCs data sets are mss_df_nu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# drop categorical variables\n",
    "mss_df_nu=mss_df.drop(['key','mode','time_signature','genre','title','artist_location','artist_latitude','releaseartist_longitude'],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Nan values cannot be processed by preprocessing. Check Nan values\n",
    "mss_df_nu.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# replace Nan values with the corresponding mean\n",
    "mss_df_nunew=mss_df_nu.fillna(mss_df_nu.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# check if all Nan Values are all replaced\n",
    "mss_df_nunew.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# standardizing the dataset\n",
    "sd_mss_df_nu = preprocessing.scale(mss_df_nunew)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# scipy.cluster.hierarchy require input data in DataFrame format\n",
    "sd_mss_cluster=pd.DataFrame(sd_mss_df_nu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# scipy package for hierarchical clustering\n",
    "from scipy.spatial.distance import pdist\n",
    "from scipy.cluster.hierarchy import linkage, dendrogram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# pdist: Pairwise distances between observations in n-dimensional space.\n",
    "\n",
    "distanceMatrix = pdist(sd_mss_cluster) #'euclidean' is default measure\n",
    "\n",
    "# linkage: define measure between clusters\n",
    "# complete is the farest distance between clusters\n",
    "dend = dendrogram(linkage(distanceMatrix, method='complete'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# k-means in scipy\n",
    "from scipy.cluster.vq import kmeans,vq,kmeans2\n",
    "\n",
    "# Generate 4 centroids and accordingly cluster lables\n",
    "centroids,label=kmeans2(sd_mss_cluster,4)\n",
    "label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# join new label to the dataset\n",
    "mss_df_nu['kmeans_4']= label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# group by k-mean label\n",
    "km4groupd=mss_df_nu.groupby(mss_df_nu['kmeans_4'])\n",
    "km4groupd.size()\n",
    "km4groupd.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# compare the clusters with box plots\n",
    "bp_artist_fami = mss_df_nu.boxplot(column='artist_familiarity',by='kmeans_4')"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Raw Cell Format",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
