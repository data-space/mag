{
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  },
  "name": "",
  "signature": "sha256:ada5e8f387d4a6e524877090bd339470757520f5f681a02eaceea4c058749790"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "Goal: Recommend similar songs from the Million songs subset using Network Graphs"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# Outline\n",
      "\n",
      "+ Use the million songs subset of 10,000 songs.\n",
      "+ Identify similar songs using network graphs.\n",
      "+ Use this approach as a building block to build a recommendation engine prototype, that incorporated user transactions history.\n",
      "+ Adjust the parameters to improve the performance of the recommendation engine."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# Approach : Similarity using Graphs (and relationships) - using Metadata (and mbtags) and Analysis Data\n",
      "\n",
      "## Data\n",
      "\n",
      "For all of the steps to follow, we will be using the Million Songs Subset (10,000) songs for consistency. In the event we run into computational issues, we may choose to cut it down to a random sample of these 10,000 songs."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# Step 1: Build a graph using Metadata.\n",
      "\n",
      "Build a graph (network) with the following as the node types.\n",
      "\n",
      "- Track_ID :Echonest ID of the particular track.\n",
      "- Artist_mbid : Artist ID from musicbrainz.org\n",
      "- Artist Terms: Artist_terms (and the artist_terms_freq and weight, if needed). We could also include artist_mbtags and its count as well.\n",
      "- Year : The Year in which the song was released.\n",
      "- *Song_ID : Not the same as Track_ID. This actually links many tracks with very slight audio differences)*\n",
      "\n",
      "For all of the 10,000 songs, we look for all the associated nodes and place in in the graph. As a next step, we make the reationships between the various nodes, based on the dataset. For example, a track will have a track_ID node, artist_mbid node, few artist_terms and one year node. \n",
      "\n",
      "## Goal\n",
      "\n",
      "The *goal* is to *establish a relationship (link) between the tracks* (there will never be a direct relationship) using the known relationships between the other nodes. For example, each song will be related to an artist. And a song can be related to another song through the relationship shared between the artists, which in turn can be through artist_terms, similar_artist or the exact same artist for the two songs. Also two songs can be related using the year of release. The possible relationship levels between two songs are provided below, in order of weightage (initial idea).\n",
      "\n",
      "+ Same artist for both the songs.\n",
      "+ Similar_artist - they are related based on the metadata similar_artist.\n",
      "+ Same Year of release (or we can adjust it to same decade).\n",
      "+ Same artist_terms.\n",
      "\n",
      "## Tools and Library\n",
      "\n",
      "The Neo4j will be used, which will be connected and accessed from within Python, using one of the two available libraries.\n",
      "\n",
      "- Py2neo\n",
      "- Neo4jRestClient\n",
      "\n",
      "Using NetworkX is also an option and we are working on it.\n",
      "\n",
      "## Final Outcome\n",
      "\n",
      "Once the graph is built, we will be able to build a relationship between songs as described above (and the weights of these relationships will be based on some rule-based scores). Once we have that, we can actually eliminate all of the other nodes and only have the Songs (track_id) nodes and the relationships.\n",
      "\n",
      "We can then get a dataframe with *Track_ID1*, *Track_ID2* and the *Weight_Relationship* as the three columns. So, based on these relationship weights, song recommendation is possible. For example, if the first song is some track_id *TR123*, \n",
      "\n",
      "    + We pull all the rows where Track_ID1 or Track_ID2 is TR123\n",
      "    + Identify the distinct relationships, basically dedup.\n",
      "    + Rank order based on the scores (relationship weights or can be thought of as distance).\n",
      "    + The top 5 or 10 songs will be the recommendations (just based on similarity or closeness of the metadata).\n",
      "\n"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# Step 2: Build a graph using Analysis data\n",
      "\n",
      "Build a graph using the Songs (Track_ID) and those **Analysis data** fields that are one per song, as the nodes. Since they are numerical fields, they will be binned and then used to build the graphs. Pitch, Loudness, Duration, Tempo, etc., can be the potential nodes. Not sure if we would be including the song hotness.\n",
      "\n",
      "The goal, the tools and the final outcome will be similar to what were described in Step 1, earlier.\n",
      "\n",
      "Hence, there will be another recommendation method, based on the *Analysis or Acoustic Data*.\n"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# Step 3: Compare and integrate the graphs in Steps 1 and 2\n",
      "\n",
      "Comparisons can be made using measures like a **correlation matrix**, to see if the relationship weights are correlated. Now that we will be having two networks with all the track_ID and the relationships with different weights, we can integrate them to get an improved recommendation. The multiplicative factor for the weights, from both these graphs, can be adjusted to change the behavior of the recommendation engine."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# Step 4: Simulate/ Demonstrate how user transacational history can be included to improve the recommendations\n",
      "\n",
      "+ We will demonstrate how the user transactions can be captured to result in another graph, with the following node types. \n",
      "    + Track_ID\n",
      "+ The relationships between the tracks, however, will be based on the user attributes and preferences. The different relationships between two tracks, will be defined by the combinations that exists between the following potential fields below.\n",
      "    + User_Age_Bin : say 4 bins.\n",
      "    + User_Language : say 2 languages.\n",
      "    + User_Region : say 2 regions.\n",
      "    + User_Gender : 2.\n",
      "\n",
      "Hence, each pair of tracks will have (4X2X2X2 = 32) 32 relationships and the weights of each will be the number of users, in that attribute group, who chose to listen to one track_ID after the other.\n"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# Additional Notes:\n",
      "\n",
      "- This approach and the steps will help us to actually modify the recommendation based on minor changes in the parameters for the scoring. \n",
      "- Hence, we believe at this point, that this will give us a great deal of flexibility in tweaking the performance of the recommendation engine with time and additional insights."
     ]
    }
   ],
   "metadata": {}
  }
 ]
}