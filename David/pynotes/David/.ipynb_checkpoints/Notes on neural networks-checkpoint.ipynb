{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notes on Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear regression\n",
    "\n",
    "Suppose you have a dataset $D$ with numeric predictor variables and a numeric target variable.\n",
    "\n",
    "For a specific row: \n",
    "- The values of the predictor variables are denoted $x_1$, $x_2$, ..., $x_n$. Let $x_0=1$. \n",
    "- The value of the target variable is denoted $y_a$\n",
    "\n",
    "Create a linear _prediction_ function, also called a model,\n",
    "$$ L_w(x) = w_0 x_0 + w_1 x_1 + w_2 x_2 + ... + w_n x_n = w\\cdot x\n",
    "$$ \n",
    "using a set of numeric weights $w = w_0$, $w_2$, ..., $w_n$. The weights $w$ determine the function. The $x$ values are input to the function.\n",
    "\n",
    "This function predicts $y$ values, where $y_p = L_w(x)$ is the predicted value and where $x$ is an array/row of predictor values. \n",
    "\n",
    "The difference $|y_a - y_p|$ between the actual value $y_a$ and the predicted value $y_p$ is the _error_ or _residual_. \n",
    "\n",
    "The _cost_ of a model $L$ determined by weights $w$ and given a dataset $D$ is the sum of the square of the error for each row:\n",
    "$$ C(L,D) = \\sum_{x, y_a \\in D} \\left[ y_a - L_w(x) \\right]^2\n",
    "$$\n",
    "\n",
    "The goal of linear regression, given a dataset with numeric predictors and target, is to find weights which minimize this cost."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic regression\n",
    "\n",
    "Supppose that, though the predictor variables remain numeric, the target variable is binary, in this case with values `0` and `1`.\n",
    "\n",
    "The standard logistic function \n",
    "$$ \\sigma(t) = \\frac{1}{1+e^{-t}}\n",
    "$$\n",
    "has a range between `0` and `1`. \n",
    "\n",
    "A _prediction_ function can be created using $\\sigma$ and $L_w$\n",
    "$$ P(x) = \\sigma(L_w(x))\n",
    "$$\n",
    "\n",
    "The function $P$ is determined by the weights $w$ since $L_w$ is determined by these weights.\n",
    "\n",
    "Another prediction function can be created \n",
    "$$ P(x) = \\operatorname{tanh}(L(x))\n",
    "$$\n",
    "using the $\\operatorname{tanh}$ function \n",
    "$$ \\operatorname{tanh}(t) = \\frac{e^t - e^{-t}}{e^t + e^{-t}}\n",
    "$$\n",
    "which has a range between `-1` and `+1`.\n",
    "\n",
    "In general we write \n",
    "$$ A_w(x) = A(L_w(x)) = A(w^j\\cdot x)\n",
    "$$\n",
    "where $A$ is an _activation function_ such as $\\sigma$ or $\\operatorname{tanh}$.\n",
    "\n",
    "There is a cost function $C(P,D)$ associated with these logistic regression prediction functions. \n",
    "\n",
    "Wikipedia: \n",
    "[Logistic regression](https://en.wikipedia.org/wiki/Logistic_regression),\n",
    "[Logistic function](https://en.wikipedia.org/wiki/Logistic_function#Derivative)\n",
    "and [graph](https://en.wikipedia.org/wiki/Logistic_function#/media/File:Logistic-curve.svg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Keras \n",
    "\n",
    "There are two types of Keras models:\n",
    "1. The _sequential_ models, which will be discussed below\n",
    "1. The _functional_ (complex) models, which will not be discussed below\n",
    "\n",
    "These Keras models have some common API calls. \n",
    "See [About Keras Models](http://keras.io/models/about-keras-models/). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sequential model\n",
    "\n",
    "The [Sequential Model](http://keras.io/models/sequential/) (see also [Sequential model guide](http://keras.io/getting-started/sequential-model-guide/)) consists of a sequence of layers and each layer consists of a sequence of nodes. See this [example](https://www.google.com/imgres?imgurl=https://upload.wikimedia.org/wikipedia/commons/thumb/4/46/Colored_neural_network.svg/2000px-Colored_neural_network.svg.png&imgrefurl=https://en.wikipedia.org/wiki/Artificial_neural_network&h=2405&w=2000&tbnid=yXQTmNEYaTljzM:&tbnh=160&tbnw=133&docid=ynYusGDc2AddHM&itg=1&client=safari&usg=__3Yu82IJzvxUUl1bq40OteHlp0qg=&sa=X&ved=0ahUKEwifjZqw9anMAhXEaz4KHfT1C2wQ9QEIIzAA#h=2405&tbnh=160&tbnw=133&w=2000). \n",
    "The first layer corresponds to the input and has one node for each input variable. The output layer has one node for each numeric target variable and one node for each class of each categorical target variable. I haven't constructed a model for a dataset with multiple multi value target variables or with mixed target variables.\n",
    "\n",
    "There are several steps to creating and working with models:\n",
    "1. Create the model and its layers\n",
    "1. Compile the model by specifying, at least, an optimization function and a loss function\n",
    "1. Fit/train the model on a dataset\n",
    "1. Predict target values for each row of a test dataset\n",
    "1. Evaluate the model\n",
    "\n",
    "These steps are demonstrated using a simple dataset constructed below. The target variable is continuous, which makes this a regression problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Import the required packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas    as pd\n",
    "import numpy     as np\n",
    "import itertools as it\n",
    "\n",
    "from keras.utils       import np_utils \n",
    "from keras.models      import Sequential\n",
    "from keras.layers.core import Dense, Activation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create the demonstration dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((125,), dtype('float64'), (125, 4), dtype('float64'))"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "N_samples      =    10  # training sample size\n",
    "N_variables    =     3  # number of input variables\n",
    "training_steps = 10000  # number of training iterations\n",
    "\n",
    "x_start        = 0\n",
    "x_stop         = 1\n",
    "x_numof        = 5\n",
    "\n",
    "rnd_mul  = 0.001\n",
    "\n",
    "x_gen    = np.random.randint(1, 4, 1+N_variables)\n",
    "x_gen\n",
    "\n",
    "x_train = np.column_stack([np.ones(x_numof**N_variables),\n",
    "                           np.array(list(it.product(np.linspace(x_start, \n",
    "                                                                x_stop, \n",
    "                                                                x_numof),\n",
    "                                                    repeat=N_variables)))])\n",
    "\n",
    "y_train = x_train.dot(x_gen) + np.random.randn(x_numof**N_variables) * rnd_mul\n",
    "y_train.shape, y_train.dtype, x_train.shape, x_train.dtype"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create the model and its layers\n",
    "\n",
    "A sequential model is a sequence of layers. See this [example](https://www.google.com/imgres?imgurl=https://upload.wikimedia.org/wikipedia/commons/thumb/4/46/Colored_neural_network.svg/2000px-Colored_neural_network.svg.png&imgrefurl=https://en.wikipedia.org/wiki/Artificial_neural_network&h=2405&w=2000&tbnid=yXQTmNEYaTljzM:&tbnh=160&tbnw=133&docid=ynYusGDc2AddHM&itg=1&client=safari&usg=__3Yu82IJzvxUUl1bq40OteHlp0qg=&sa=X&ved=0ahUKEwifjZqw9anMAhXEaz4KHfT1C2wQ9QEIIzAA#h=2405&tbnh=160&tbnw=133&w=2000) for a graphical representation of a seqeuntial model.\n",
    "There is one input layer (first in the sequence), one output layer (last in the sequence) and zero or more hidden layers. A layer is a sequence of functions denoted \n",
    "$$ A^j_w(x) = A(L_w(x)) = A(w^j\\cdot x)\n",
    "$$\n",
    "where $A$ is an _activation_ function and $w$ is a vector of weights. \n",
    "\n",
    "- The input of each function in the input layer is a vector $x$ of values from the predictor variables. \n",
    "- The output of the set of functions in the input layer is the single value\n",
    "- The output of each function in a layer is a single value, which is the value of the activation function \n",
    "\n",
    "- The output of all functions in the output layer are predicted values of the target variables. \n",
    "- The input of each function in a layer is a vector of output values from all functions in the previous layer. \n",
    "\n",
    "There are two parameters that must be specified for each layer:\n",
    "\n",
    "1. [Initializations](http://keras.io/initializations/): \n",
    "    An initialization method for the weights of that layer. \n",
    "    Options: `glorot_uniform`, `uniform`, `zero`, ...\n",
    "    \n",
    "1. [Activations](http://keras.io/activations/): \n",
    "    The activation function for that layer. \n",
    "    Options: `linear`, `sigmoid`, `tanh`, `relu`, ...\n",
    "    \n",
    "\n",
    "Recall, the input of the activation function is the dot product of the vector of output values, of the previous layer, with the vector of weights; examples of activation functions are the sigmoid (aka logistic function) and the hyperbolic tangent (tanh)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(input_dim  =  4, init       = \"uniform\",\n",
    "                output_dim = 10, activation = 'linear'))\n",
    "\n",
    "#model.add(Dense(input_dim  = 10, init       = \"glorot_uniform\",\n",
    "#                output_dim = 10, activation = 'tanh'))\n",
    "\n",
    "model.add(Dense(input_dim  = 10, init       = \"uniform\",\n",
    "                output_dim =  1, activation = 'linear'))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compile the model\n",
    "\n",
    "The two required parameters to compile a model are the optimizer and the objective function.\n",
    "\n",
    "- [Objectives](http://keras.io/objectives/): The loss or cost function to minimize, which measures the difference between the predicted and actual target values. \n",
    "Options: `mean_squared_error`, `mean_absolute_error`, ...\n",
    "- [Optimizers](http://keras.io/optimizers/): The optimization method to minimize the objective function. Options: `sgd`, `rmsprop`, ...\n",
    "- [Regularizers](http://keras.io/regularizers/): Adds penalties to the objective function,  which often constrain the weights. More on this later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model.compile(optimizer='sgd',\n",
    "              loss     ='mean_squared_error')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fit (train) the model on a dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "125/125 [==============================] - 0s - loss: 51.6898\n",
      "Epoch 2/100\n",
      "125/125 [==============================] - 0s - loss: 49.6558\n",
      "Epoch 3/100\n",
      "125/125 [==============================] - 0s - loss: 47.6833\n",
      "Epoch 4/100\n",
      "125/125 [==============================] - 0s - loss: 45.7505\n",
      "Epoch 5/100\n",
      "125/125 [==============================] - 0s - loss: 43.8340\n",
      "Epoch 6/100\n",
      "125/125 [==============================] - 0s - loss: 41.9062\n",
      "Epoch 7/100\n",
      "125/125 [==============================] - 0s - loss: 39.9326\n",
      "Epoch 8/100\n",
      "125/125 [==============================] - 0s - loss: 37.8706\n",
      "Epoch 9/100\n",
      "125/125 [==============================] - 0s - loss: 35.6689\n",
      "Epoch 10/100\n",
      "125/125 [==============================] - 0s - loss: 33.2685\n",
      "Epoch 11/100\n",
      "125/125 [==============================] - 0s - loss: 30.6083\n",
      "Epoch 12/100\n",
      "125/125 [==============================] - 0s - loss: 27.6364\n",
      "Epoch 13/100\n",
      "125/125 [==============================] - 0s - loss: 24.3290\n",
      "Epoch 14/100\n",
      "125/125 [==============================] - 0s - loss: 20.7183\n",
      "Epoch 15/100\n",
      "125/125 [==============================] - 0s - loss: 16.9186\n",
      "Epoch 16/100\n",
      "125/125 [==============================] - 0s - loss: 13.1372\n",
      "Epoch 17/100\n",
      "125/125 [==============================] - 0s - loss: 9.6457\n",
      "Epoch 18/100\n",
      "125/125 [==============================] - 0s - loss: 6.7061\n",
      "Epoch 19/100\n",
      "125/125 [==============================] - 0s - loss: 4.4777\n",
      "Epoch 20/100\n",
      "125/125 [==============================] - 0s - loss: 2.9652\n",
      "Epoch 21/100\n",
      "125/125 [==============================] - 0s - loss: 2.0428\n",
      "Epoch 22/100\n",
      "125/125 [==============================] - 0s - loss: 1.5306\n",
      "Epoch 23/100\n",
      "125/125 [==============================] - 0s - loss: 1.2658\n",
      "Epoch 24/100\n",
      "125/125 [==============================] - 0s - loss: 1.1341\n",
      "Epoch 25/100\n",
      "125/125 [==============================] - 0s - loss: 1.0680\n",
      "Epoch 26/100\n",
      "125/125 [==============================] - 0s - loss: 1.0324\n",
      "Epoch 27/100\n",
      "125/125 [==============================] - 0s - loss: 1.0103\n",
      "Epoch 28/100\n",
      "125/125 [==============================] - 0s - loss: 0.9941\n",
      "Epoch 29/100\n",
      "125/125 [==============================] - 0s - loss: 0.9804\n",
      "Epoch 30/100\n",
      "125/125 [==============================] - 0s - loss: 0.9678\n",
      "Epoch 31/100\n",
      "125/125 [==============================] - 0s - loss: 0.9558\n",
      "Epoch 32/100\n",
      "125/125 [==============================] - 0s - loss: 0.9441\n",
      "Epoch 33/100\n",
      "125/125 [==============================] - 0s - loss: 0.9325\n",
      "Epoch 34/100\n",
      "125/125 [==============================] - 0s - loss: 0.9211\n",
      "Epoch 35/100\n",
      "125/125 [==============================] - 0s - loss: 0.9099\n",
      "Epoch 36/100\n",
      "125/125 [==============================] - 0s - loss: 0.8987\n",
      "Epoch 37/100\n",
      "125/125 [==============================] - 0s - loss: 0.8876\n",
      "Epoch 38/100\n",
      "125/125 [==============================] - 0s - loss: 0.8767\n",
      "Epoch 39/100\n",
      "125/125 [==============================] - 0s - loss: 0.8659\n",
      "Epoch 40/100\n",
      "125/125 [==============================] - 0s - loss: 0.8551\n",
      "Epoch 41/100\n",
      "125/125 [==============================] - 0s - loss: 0.8445\n",
      "Epoch 42/100\n",
      "125/125 [==============================] - 0s - loss: 0.8340\n",
      "Epoch 43/100\n",
      "125/125 [==============================] - 0s - loss: 0.8236\n",
      "Epoch 44/100\n",
      "125/125 [==============================] - 0s - loss: 0.8133\n",
      "Epoch 45/100\n",
      "125/125 [==============================] - 0s - loss: 0.8031\n",
      "Epoch 46/100\n",
      "125/125 [==============================] - 0s - loss: 0.7930\n",
      "Epoch 47/100\n",
      "125/125 [==============================] - 0s - loss: 0.7830\n",
      "Epoch 48/100\n",
      "125/125 [==============================] - 0s - loss: 0.7731\n",
      "Epoch 49/100\n",
      "125/125 [==============================] - 0s - loss: 0.7633\n",
      "Epoch 50/100\n",
      "125/125 [==============================] - 0s - loss: 0.7535\n",
      "Epoch 51/100\n",
      "125/125 [==============================] - 0s - loss: 0.7439\n",
      "Epoch 52/100\n",
      "125/125 [==============================] - 0s - loss: 0.7344\n",
      "Epoch 53/100\n",
      "125/125 [==============================] - 0s - loss: 0.7250\n",
      "Epoch 54/100\n",
      "125/125 [==============================] - 0s - loss: 0.7156\n",
      "Epoch 55/100\n",
      "125/125 [==============================] - 0s - loss: 0.7064\n",
      "Epoch 56/100\n",
      "125/125 [==============================] - 0s - loss: 0.6972\n",
      "Epoch 57/100\n",
      "125/125 [==============================] - 0s - loss: 0.6882\n",
      "Epoch 58/100\n",
      "125/125 [==============================] - 0s - loss: 0.6792\n",
      "Epoch 59/100\n",
      "125/125 [==============================] - 0s - loss: 0.6703\n",
      "Epoch 60/100\n",
      "125/125 [==============================] - 0s - loss: 0.6615\n",
      "Epoch 61/100\n",
      "125/125 [==============================] - 0s - loss: 0.6528\n",
      "Epoch 62/100\n",
      "125/125 [==============================] - 0s - loss: 0.6442\n",
      "Epoch 63/100\n",
      "125/125 [==============================] - 0s - loss: 0.6357\n",
      "Epoch 64/100\n",
      "125/125 [==============================] - 0s - loss: 0.6272\n",
      "Epoch 65/100\n",
      "125/125 [==============================] - 0s - loss: 0.6189\n",
      "Epoch 66/100\n",
      "125/125 [==============================] - 0s - loss: 0.6106\n",
      "Epoch 67/100\n",
      "125/125 [==============================] - 0s - loss: 0.6024\n",
      "Epoch 68/100\n",
      "125/125 [==============================] - 0s - loss: 0.5943\n",
      "Epoch 69/100\n",
      "125/125 [==============================] - 0s - loss: 0.5863\n",
      "Epoch 70/100\n",
      "125/125 [==============================] - 0s - loss: 0.5783\n",
      "Epoch 71/100\n",
      "125/125 [==============================] - 0s - loss: 0.5705\n",
      "Epoch 72/100\n",
      "125/125 [==============================] - 0s - loss: 0.5627\n",
      "Epoch 73/100\n",
      "125/125 [==============================] - 0s - loss: 0.5550\n",
      "Epoch 74/100\n",
      "125/125 [==============================] - 0s - loss: 0.5473\n",
      "Epoch 75/100\n",
      "125/125 [==============================] - 0s - loss: 0.5398\n",
      "Epoch 76/100\n",
      "125/125 [==============================] - 0s - loss: 0.5323\n",
      "Epoch 77/100\n",
      "125/125 [==============================] - 0s - loss: 0.5249\n",
      "Epoch 78/100\n",
      "125/125 [==============================] - 0s - loss: 0.5176\n",
      "Epoch 79/100\n",
      "125/125 [==============================] - 0s - loss: 0.5104\n",
      "Epoch 80/100\n",
      "125/125 [==============================] - 0s - loss: 0.5032\n",
      "Epoch 81/100\n",
      "125/125 [==============================] - 0s - loss: 0.4962\n",
      "Epoch 82/100\n",
      "125/125 [==============================] - 0s - loss: 0.4891\n",
      "Epoch 83/100\n",
      "125/125 [==============================] - 0s - loss: 0.4822\n",
      "Epoch 84/100\n",
      "125/125 [==============================] - 0s - loss: 0.4754\n",
      "Epoch 85/100\n",
      "125/125 [==============================] - 0s - loss: 0.4686\n",
      "Epoch 86/100\n",
      "125/125 [==============================] - 0s - loss: 0.4619\n",
      "Epoch 87/100\n",
      "125/125 [==============================] - 0s - loss: 0.4552\n",
      "Epoch 88/100\n",
      "125/125 [==============================] - 0s - loss: 0.4487\n",
      "Epoch 89/100\n",
      "125/125 [==============================] - 0s - loss: 0.4422\n",
      "Epoch 90/100\n",
      "125/125 [==============================] - 0s - loss: 0.4357\n",
      "Epoch 91/100\n",
      "125/125 [==============================] - 0s - loss: 0.4294\n",
      "Epoch 92/100\n",
      "125/125 [==============================] - 0s - loss: 0.4231\n",
      "Epoch 93/100\n",
      "125/125 [==============================] - 0s - loss: 0.4169\n",
      "Epoch 94/100\n",
      "125/125 [==============================] - 0s - loss: 0.4108\n",
      "Epoch 95/100\n",
      "125/125 [==============================] - 0s - loss: 0.4047\n",
      "Epoch 96/100\n",
      "125/125 [==============================] - 0s - loss: 0.3987\n",
      "Epoch 97/100\n",
      "125/125 [==============================] - 0s - loss: 0.3927\n",
      "Epoch 98/100\n",
      "125/125 [==============================] - 0s - loss: 0.3869\n",
      "Epoch 99/100\n",
      "125/125 [==============================] - 0s - loss: 0.3811\n",
      "Epoch 100/100\n",
      "125/125 [==============================] - 0s - loss: 0.3753\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x128288f98>"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(x_train,  #predictors\n",
    "          y_train,  #target\n",
    "          nb_epoch  =100, \n",
    "          batch_size=125, \n",
    "          verbose   =True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predict target values for each row of a test dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>x0</th>\n",
       "      <th>x1</th>\n",
       "      <th>x2</th>\n",
       "      <th>x3</th>\n",
       "      <th>y_actual</th>\n",
       "      <th>y_predicted</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>3.000575</td>\n",
       "      <td>4.514</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.25</td>\n",
       "      <td>3.748740</td>\n",
       "      <td>4.984</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.50</td>\n",
       "      <td>4.498506</td>\n",
       "      <td>5.454</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.75</td>\n",
       "      <td>5.249418</td>\n",
       "      <td>5.924</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>5.999339</td>\n",
       "      <td>6.393</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.00</td>\n",
       "      <td>3.749023</td>\n",
       "      <td>4.982</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.25</td>\n",
       "      <td>4.502320</td>\n",
       "      <td>5.452</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.50</td>\n",
       "      <td>5.249595</td>\n",
       "      <td>5.922</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.75</td>\n",
       "      <td>5.998899</td>\n",
       "      <td>6.392</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.25</td>\n",
       "      <td>1.00</td>\n",
       "      <td>6.748396</td>\n",
       "      <td>6.861</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.00</td>\n",
       "      <td>4.500460</td>\n",
       "      <td>5.450</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.25</td>\n",
       "      <td>5.249193</td>\n",
       "      <td>5.920</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.50</td>\n",
       "      <td>6.001393</td>\n",
       "      <td>6.390</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.75</td>\n",
       "      <td>6.749163</td>\n",
       "      <td>6.859</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.50</td>\n",
       "      <td>1.00</td>\n",
       "      <td>7.500947</td>\n",
       "      <td>7.329</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.75</td>\n",
       "      <td>0.00</td>\n",
       "      <td>5.249597</td>\n",
       "      <td>5.918</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.75</td>\n",
       "      <td>0.25</td>\n",
       "      <td>6.001072</td>\n",
       "      <td>6.388</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.75</td>\n",
       "      <td>0.50</td>\n",
       "      <td>6.750475</td>\n",
       "      <td>6.858</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.75</td>\n",
       "      <td>0.75</td>\n",
       "      <td>7.500851</td>\n",
       "      <td>7.327</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.75</td>\n",
       "      <td>1.00</td>\n",
       "      <td>8.249842</td>\n",
       "      <td>7.797</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>5.999336</td>\n",
       "      <td>6.386</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.25</td>\n",
       "      <td>6.751384</td>\n",
       "      <td>6.856</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.50</td>\n",
       "      <td>7.498356</td>\n",
       "      <td>7.325</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.75</td>\n",
       "      <td>8.249772</td>\n",
       "      <td>7.795</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>8.999874</td>\n",
       "      <td>8.265</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>3.498267</td>\n",
       "      <td>4.863</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.25</td>\n",
       "      <td>4.250264</td>\n",
       "      <td>5.332</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.50</td>\n",
       "      <td>4.999016</td>\n",
       "      <td>5.802</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.75</td>\n",
       "      <td>5.750429</td>\n",
       "      <td>6.272</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>6.499335</td>\n",
       "      <td>6.742</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.75</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>7.501600</td>\n",
       "      <td>7.431</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.75</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.25</td>\n",
       "      <td>8.249487</td>\n",
       "      <td>7.900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.75</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.50</td>\n",
       "      <td>9.000660</td>\n",
       "      <td>8.370</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.75</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.75</td>\n",
       "      <td>9.750670</td>\n",
       "      <td>8.840</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.75</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>10.500777</td>\n",
       "      <td>9.310</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>5.000817</td>\n",
       "      <td>5.907</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>101</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.25</td>\n",
       "      <td>5.749374</td>\n",
       "      <td>6.377</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>102</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.50</td>\n",
       "      <td>6.497568</td>\n",
       "      <td>6.847</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>103</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.75</td>\n",
       "      <td>7.250102</td>\n",
       "      <td>7.316</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>104</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>7.999511</td>\n",
       "      <td>7.786</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>105</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.00</td>\n",
       "      <td>5.751191</td>\n",
       "      <td>6.375</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>106</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.25</td>\n",
       "      <td>6.501729</td>\n",
       "      <td>6.845</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>107</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.50</td>\n",
       "      <td>7.249307</td>\n",
       "      <td>7.315</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>108</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.75</td>\n",
       "      <td>8.001543</td>\n",
       "      <td>7.784</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>109</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.25</td>\n",
       "      <td>1.00</td>\n",
       "      <td>8.750658</td>\n",
       "      <td>8.254</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>110</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.00</td>\n",
       "      <td>6.498743</td>\n",
       "      <td>6.843</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>111</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.25</td>\n",
       "      <td>7.250145</td>\n",
       "      <td>7.313</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>112</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.50</td>\n",
       "      <td>8.000515</td>\n",
       "      <td>7.782</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>113</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.75</td>\n",
       "      <td>8.748026</td>\n",
       "      <td>8.252</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>114</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.50</td>\n",
       "      <td>1.00</td>\n",
       "      <td>9.498392</td>\n",
       "      <td>8.722</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>115</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.75</td>\n",
       "      <td>0.00</td>\n",
       "      <td>7.250848</td>\n",
       "      <td>7.311</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>116</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.75</td>\n",
       "      <td>0.25</td>\n",
       "      <td>8.002168</td>\n",
       "      <td>7.781</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>117</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.75</td>\n",
       "      <td>0.50</td>\n",
       "      <td>8.750271</td>\n",
       "      <td>8.250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>118</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.75</td>\n",
       "      <td>0.75</td>\n",
       "      <td>9.499542</td>\n",
       "      <td>8.720</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>119</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.75</td>\n",
       "      <td>1.00</td>\n",
       "      <td>10.248225</td>\n",
       "      <td>9.190</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>120</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>7.999217</td>\n",
       "      <td>7.779</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>121</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.25</td>\n",
       "      <td>8.748228</td>\n",
       "      <td>8.249</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>122</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.50</td>\n",
       "      <td>9.499416</td>\n",
       "      <td>8.718</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>123</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.75</td>\n",
       "      <td>10.249264</td>\n",
       "      <td>9.188</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>124</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>10.999929</td>\n",
       "      <td>9.658</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>125 rows Ã— 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      x0    x1    x2    x3   y_actual  y_predicted\n",
       "0    1.0  0.00  0.00  0.00   3.000575        4.514\n",
       "1    1.0  0.00  0.00  0.25   3.748740        4.984\n",
       "2    1.0  0.00  0.00  0.50   4.498506        5.454\n",
       "3    1.0  0.00  0.00  0.75   5.249418        5.924\n",
       "4    1.0  0.00  0.00  1.00   5.999339        6.393\n",
       "5    1.0  0.00  0.25  0.00   3.749023        4.982\n",
       "6    1.0  0.00  0.25  0.25   4.502320        5.452\n",
       "7    1.0  0.00  0.25  0.50   5.249595        5.922\n",
       "8    1.0  0.00  0.25  0.75   5.998899        6.392\n",
       "9    1.0  0.00  0.25  1.00   6.748396        6.861\n",
       "10   1.0  0.00  0.50  0.00   4.500460        5.450\n",
       "11   1.0  0.00  0.50  0.25   5.249193        5.920\n",
       "12   1.0  0.00  0.50  0.50   6.001393        6.390\n",
       "13   1.0  0.00  0.50  0.75   6.749163        6.859\n",
       "14   1.0  0.00  0.50  1.00   7.500947        7.329\n",
       "15   1.0  0.00  0.75  0.00   5.249597        5.918\n",
       "16   1.0  0.00  0.75  0.25   6.001072        6.388\n",
       "17   1.0  0.00  0.75  0.50   6.750475        6.858\n",
       "18   1.0  0.00  0.75  0.75   7.500851        7.327\n",
       "19   1.0  0.00  0.75  1.00   8.249842        7.797\n",
       "20   1.0  0.00  1.00  0.00   5.999336        6.386\n",
       "21   1.0  0.00  1.00  0.25   6.751384        6.856\n",
       "22   1.0  0.00  1.00  0.50   7.498356        7.325\n",
       "23   1.0  0.00  1.00  0.75   8.249772        7.795\n",
       "24   1.0  0.00  1.00  1.00   8.999874        8.265\n",
       "25   1.0  0.25  0.00  0.00   3.498267        4.863\n",
       "26   1.0  0.25  0.00  0.25   4.250264        5.332\n",
       "27   1.0  0.25  0.00  0.50   4.999016        5.802\n",
       "28   1.0  0.25  0.00  0.75   5.750429        6.272\n",
       "29   1.0  0.25  0.00  1.00   6.499335        6.742\n",
       "..   ...   ...   ...   ...        ...          ...\n",
       "95   1.0  0.75  1.00  0.00   7.501600        7.431\n",
       "96   1.0  0.75  1.00  0.25   8.249487        7.900\n",
       "97   1.0  0.75  1.00  0.50   9.000660        8.370\n",
       "98   1.0  0.75  1.00  0.75   9.750670        8.840\n",
       "99   1.0  0.75  1.00  1.00  10.500777        9.310\n",
       "100  1.0  1.00  0.00  0.00   5.000817        5.907\n",
       "101  1.0  1.00  0.00  0.25   5.749374        6.377\n",
       "102  1.0  1.00  0.00  0.50   6.497568        6.847\n",
       "103  1.0  1.00  0.00  0.75   7.250102        7.316\n",
       "104  1.0  1.00  0.00  1.00   7.999511        7.786\n",
       "105  1.0  1.00  0.25  0.00   5.751191        6.375\n",
       "106  1.0  1.00  0.25  0.25   6.501729        6.845\n",
       "107  1.0  1.00  0.25  0.50   7.249307        7.315\n",
       "108  1.0  1.00  0.25  0.75   8.001543        7.784\n",
       "109  1.0  1.00  0.25  1.00   8.750658        8.254\n",
       "110  1.0  1.00  0.50  0.00   6.498743        6.843\n",
       "111  1.0  1.00  0.50  0.25   7.250145        7.313\n",
       "112  1.0  1.00  0.50  0.50   8.000515        7.782\n",
       "113  1.0  1.00  0.50  0.75   8.748026        8.252\n",
       "114  1.0  1.00  0.50  1.00   9.498392        8.722\n",
       "115  1.0  1.00  0.75  0.00   7.250848        7.311\n",
       "116  1.0  1.00  0.75  0.25   8.002168        7.781\n",
       "117  1.0  1.00  0.75  0.50   8.750271        8.250\n",
       "118  1.0  1.00  0.75  0.75   9.499542        8.720\n",
       "119  1.0  1.00  0.75  1.00  10.248225        9.190\n",
       "120  1.0  1.00  1.00  0.00   7.999217        7.779\n",
       "121  1.0  1.00  1.00  0.25   8.748228        8.249\n",
       "122  1.0  1.00  1.00  0.50   9.499416        8.718\n",
       "123  1.0  1.00  1.00  0.75  10.249264        9.188\n",
       "124  1.0  1.00  1.00  1.00  10.999929        9.658\n",
       "\n",
       "[125 rows x 6 columns]"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.concat([pd.DataFrame(x_train,                            columns=['x0','x1','x2','x3']),\n",
    "           pd.DataFrame(y_train,                            columns=['y_actual']),\n",
    "           pd.DataFrame(np.round(model.predict(x_train),3), columns=['y_predicted'])\n",
    "          ],\n",
    "          axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate the model\n",
    "\n",
    "More on this later."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural networks categorical example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "n_samples = 10\n",
    "col_data = {'x1' : np.random.randint(2, size=n_samples),\n",
    "            'x2' : np.random.randint(2, size=n_samples),\n",
    "            'y'  : np.random.randint(2, size=n_samples)}\n",
    "data       = np.asarray(pd.DataFrame(col_data)[[0,1]])\n",
    "labels     = np.asarray(pd.DataFrame(col_data)[[2]])\n",
    "labels_bin = np_utils.to_categorical(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(input_dim  =  2, init       = \"glorot_uniform\",\n",
    "                output_dim = 10, activation = 'tanh'))\n",
    "\n",
    "#model.add(Dense(input_dim  = 10, init       = \"glorot_uniform\",\n",
    "#                output_dim = 10, activation = 'tanh'))\n",
    "\n",
    "model.add(Dense(input_dim  = 10, init       = \"glorot_uniform\",\n",
    "                output_dim =  1, activation = 'sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# for a regression problem\n",
    "model.compile(optimizer='sgd',\n",
    "              loss='mse')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# for a binary classification problem\n",
    "model.compile(optimizer='sgd',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# for a multi classification problem\n",
    "model.compile(optimizer='sgd',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "10/10 [==============================] - 0s - loss: 0.2661\n",
      "Epoch 2/100\n",
      "10/10 [==============================] - 0s - loss: 0.2659\n",
      "Epoch 3/100\n",
      "10/10 [==============================] - 0s - loss: 0.2658\n",
      "Epoch 4/100\n",
      "10/10 [==============================] - 0s - loss: 0.2656\n",
      "Epoch 5/100\n",
      "10/10 [==============================] - 0s - loss: 0.2655\n",
      "Epoch 6/100\n",
      "10/10 [==============================] - 0s - loss: 0.2653\n",
      "Epoch 7/100\n",
      "10/10 [==============================] - 0s - loss: 0.2652\n",
      "Epoch 8/100\n",
      "10/10 [==============================] - 0s - loss: 0.2650\n",
      "Epoch 9/100\n",
      "10/10 [==============================] - 0s - loss: 0.2649\n",
      "Epoch 10/100\n",
      "10/10 [==============================] - 0s - loss: 0.2647\n",
      "Epoch 11/100\n",
      "10/10 [==============================] - 0s - loss: 0.2646\n",
      "Epoch 12/100\n",
      "10/10 [==============================] - 0s - loss: 0.2645\n",
      "Epoch 13/100\n",
      "10/10 [==============================] - 0s - loss: 0.2643\n",
      "Epoch 14/100\n",
      "10/10 [==============================] - 0s - loss: 0.2642\n",
      "Epoch 15/100\n",
      "10/10 [==============================] - 0s - loss: 0.2640\n",
      "Epoch 16/100\n",
      "10/10 [==============================] - 0s - loss: 0.2639\n",
      "Epoch 17/100\n",
      "10/10 [==============================] - 0s - loss: 0.2638\n",
      "Epoch 18/100\n",
      "10/10 [==============================] - 0s - loss: 0.2636\n",
      "Epoch 19/100\n",
      "10/10 [==============================] - 0s - loss: 0.2635\n",
      "Epoch 20/100\n",
      "10/10 [==============================] - 0s - loss: 0.2634\n",
      "Epoch 21/100\n",
      "10/10 [==============================] - 0s - loss: 0.2632\n",
      "Epoch 22/100\n",
      "10/10 [==============================] - 0s - loss: 0.2631\n",
      "Epoch 23/100\n",
      "10/10 [==============================] - 0s - loss: 0.2630\n",
      "Epoch 24/100\n",
      "10/10 [==============================] - 0s - loss: 0.2629\n",
      "Epoch 25/100\n",
      "10/10 [==============================] - 0s - loss: 0.2627\n",
      "Epoch 26/100\n",
      "10/10 [==============================] - 0s - loss: 0.2626\n",
      "Epoch 27/100\n",
      "10/10 [==============================] - 0s - loss: 0.2625\n",
      "Epoch 28/100\n",
      "10/10 [==============================] - 0s - loss: 0.2623\n",
      "Epoch 29/100\n",
      "10/10 [==============================] - 0s - loss: 0.2622\n",
      "Epoch 30/100\n",
      "10/10 [==============================] - 0s - loss: 0.2621\n",
      "Epoch 31/100\n",
      "10/10 [==============================] - 0s - loss: 0.2620\n",
      "Epoch 32/100\n",
      "10/10 [==============================] - 0s - loss: 0.2618\n",
      "Epoch 33/100\n",
      "10/10 [==============================] - 0s - loss: 0.2617\n",
      "Epoch 34/100\n",
      "10/10 [==============================] - 0s - loss: 0.2616\n",
      "Epoch 35/100\n",
      "10/10 [==============================] - 0s - loss: 0.2615\n",
      "Epoch 36/100\n",
      "10/10 [==============================] - 0s - loss: 0.2614\n",
      "Epoch 37/100\n",
      "10/10 [==============================] - 0s - loss: 0.2612\n",
      "Epoch 38/100\n",
      "10/10 [==============================] - 0s - loss: 0.2611\n",
      "Epoch 39/100\n",
      "10/10 [==============================] - 0s - loss: 0.2610\n",
      "Epoch 40/100\n",
      "10/10 [==============================] - 0s - loss: 0.2609\n",
      "Epoch 41/100\n",
      "10/10 [==============================] - 0s - loss: 0.2608\n",
      "Epoch 42/100\n",
      "10/10 [==============================] - 0s - loss: 0.2607\n",
      "Epoch 43/100\n",
      "10/10 [==============================] - 0s - loss: 0.2605\n",
      "Epoch 44/100\n",
      "10/10 [==============================] - 0s - loss: 0.2604\n",
      "Epoch 45/100\n",
      "10/10 [==============================] - 0s - loss: 0.2603\n",
      "Epoch 46/100\n",
      "10/10 [==============================] - 0s - loss: 0.2602\n",
      "Epoch 47/100\n",
      "10/10 [==============================] - 0s - loss: 0.2601\n",
      "Epoch 48/100\n",
      "10/10 [==============================] - 0s - loss: 0.2600\n",
      "Epoch 49/100\n",
      "10/10 [==============================] - 0s - loss: 0.2599\n",
      "Epoch 50/100\n",
      "10/10 [==============================] - 0s - loss: 0.2598\n",
      "Epoch 51/100\n",
      "10/10 [==============================] - 0s - loss: 0.2597\n",
      "Epoch 52/100\n",
      "10/10 [==============================] - 0s - loss: 0.2595\n",
      "Epoch 53/100\n",
      "10/10 [==============================] - 0s - loss: 0.2594\n",
      "Epoch 54/100\n",
      "10/10 [==============================] - 0s - loss: 0.2593\n",
      "Epoch 55/100\n",
      "10/10 [==============================] - 0s - loss: 0.2592\n",
      "Epoch 56/100\n",
      "10/10 [==============================] - 0s - loss: 0.2591\n",
      "Epoch 57/100\n",
      "10/10 [==============================] - 0s - loss: 0.2590\n",
      "Epoch 58/100\n",
      "10/10 [==============================] - 0s - loss: 0.2589\n",
      "Epoch 59/100\n",
      "10/10 [==============================] - 0s - loss: 0.2588\n",
      "Epoch 60/100\n",
      "10/10 [==============================] - 0s - loss: 0.2587\n",
      "Epoch 61/100\n",
      "10/10 [==============================] - 0s - loss: 0.2586\n",
      "Epoch 62/100\n",
      "10/10 [==============================] - 0s - loss: 0.2585\n",
      "Epoch 63/100\n",
      "10/10 [==============================] - 0s - loss: 0.2584\n",
      "Epoch 64/100\n",
      "10/10 [==============================] - 0s - loss: 0.2583\n",
      "Epoch 65/100\n",
      "10/10 [==============================] - 0s - loss: 0.2582\n",
      "Epoch 66/100\n",
      "10/10 [==============================] - 0s - loss: 0.2581\n",
      "Epoch 67/100\n",
      "10/10 [==============================] - 0s - loss: 0.2580\n",
      "Epoch 68/100\n",
      "10/10 [==============================] - 0s - loss: 0.2579\n",
      "Epoch 69/100\n",
      "10/10 [==============================] - 0s - loss: 0.2578\n",
      "Epoch 70/100\n",
      "10/10 [==============================] - 0s - loss: 0.2577\n",
      "Epoch 71/100\n",
      "10/10 [==============================] - 0s - loss: 0.2576\n",
      "Epoch 72/100\n",
      "10/10 [==============================] - 0s - loss: 0.2575\n",
      "Epoch 73/100\n",
      "10/10 [==============================] - 0s - loss: 0.2574\n",
      "Epoch 74/100\n",
      "10/10 [==============================] - 0s - loss: 0.2573\n",
      "Epoch 75/100\n",
      "10/10 [==============================] - 0s - loss: 0.2573\n",
      "Epoch 76/100\n",
      "10/10 [==============================] - 0s - loss: 0.2572\n",
      "Epoch 77/100\n",
      "10/10 [==============================] - 0s - loss: 0.2571\n",
      "Epoch 78/100\n",
      "10/10 [==============================] - 0s - loss: 0.2570\n",
      "Epoch 79/100\n",
      "10/10 [==============================] - 0s - loss: 0.2569\n",
      "Epoch 80/100\n",
      "10/10 [==============================] - 0s - loss: 0.2568\n",
      "Epoch 81/100\n",
      "10/10 [==============================] - 0s - loss: 0.2567\n",
      "Epoch 82/100\n",
      "10/10 [==============================] - 0s - loss: 0.2566\n",
      "Epoch 83/100\n",
      "10/10 [==============================] - 0s - loss: 0.2565\n",
      "Epoch 84/100\n",
      "10/10 [==============================] - 0s - loss: 0.2564\n",
      "Epoch 85/100\n",
      "10/10 [==============================] - 0s - loss: 0.2564\n",
      "Epoch 86/100\n",
      "10/10 [==============================] - 0s - loss: 0.2563\n",
      "Epoch 87/100\n",
      "10/10 [==============================] - 0s - loss: 0.2562\n",
      "Epoch 88/100\n",
      "10/10 [==============================] - 0s - loss: 0.2561\n",
      "Epoch 89/100\n",
      "10/10 [==============================] - 0s - loss: 0.2560\n",
      "Epoch 90/100\n",
      "10/10 [==============================] - 0s - loss: 0.2559\n",
      "Epoch 91/100\n",
      "10/10 [==============================] - 0s - loss: 0.2558\n",
      "Epoch 92/100\n",
      "10/10 [==============================] - 0s - loss: 0.2558\n",
      "Epoch 93/100\n",
      "10/10 [==============================] - 0s - loss: 0.2557\n",
      "Epoch 94/100\n",
      "10/10 [==============================] - 0s - loss: 0.2556\n",
      "Epoch 95/100\n",
      "10/10 [==============================] - 0s - loss: 0.2555\n",
      "Epoch 96/100\n",
      "10/10 [==============================] - 0s - loss: 0.2554\n",
      "Epoch 97/100\n",
      "10/10 [==============================] - 0s - loss: 0.2554\n",
      "Epoch 98/100\n",
      "10/10 [==============================] - 0s - loss: 0.2553\n",
      "Epoch 99/100\n",
      "10/10 [==============================] - 0s - loss: 0.2552\n",
      "Epoch 100/100\n",
      "10/10 [==============================] - 0s - loss: 0.2551\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x126f90dd8>"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(data,   # predictors\n",
    "          labels, # target\n",
    "          nb_epoch=100, \n",
    "          batch_size=n_samples, \n",
    "          verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10/10 [==============================] - 0s\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>x1</th>\n",
       "      <th>x2</th>\n",
       "      <th>y</th>\n",
       "      <th>class</th>\n",
       "      <th>value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.445</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.588</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.488</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.533</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.445</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.588</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.445</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.533</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.488</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.533</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   x1  x2  y  class  value\n",
       "0   1   0  1      0  0.445\n",
       "1   0   1  1      1  0.588\n",
       "2   1   1  0      0  0.488\n",
       "3   0   0  0      1  0.533\n",
       "4   1   0  1      0  0.445\n",
       "5   0   1  0      1  0.588\n",
       "6   1   0  0      0  0.445\n",
       "7   0   0  1      1  0.533\n",
       "8   1   1  1      0  0.488\n",
       "9   0   0  1      1  0.533"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.concat([pd.DataFrame(col_data),\n",
    "           pd.DataFrame(model.predict_classes(data)    ,columns=['class']),\n",
    "           pd.DataFrame(np.round(model.predict(data),3),columns=['value'])\n",
    "          ],\n",
    "         axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Frequency counts: [5 5]\n"
     ]
    }
   ],
   "source": [
    "y_pred = model.predict_classes(data, \n",
    "                               verbose=False\n",
    "                              ).reshape(n_samples,)\n",
    "y_init = labels.reshape(n_samples,)\n",
    "y_freq = np.bincount(abs(y_pred - y_init))\n",
    "\n",
    "print(\"Frequency counts:\", y_freq)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NEW STUFF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# STOP HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Keras-introduction to neural networks\n",
    "\n",
    "\n",
    "Source: \n",
    "\n",
    "Designing Machine Learning Systems with Python <br>\n",
    "by David Julian <br>\n",
    "Publisher: Packt Publishing <br>\n",
    "Release Date: April 2016 <br>\n",
    "ISBN: 9781785882951\n",
    "\n",
    "See \n",
    "- Chapter 5. Linear Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "With one target/dependent variable $y$, one feature (independent variable) $x_1$ and two weight values $w_0$ and $w_1$, then the function $h$ can be used to predict $y$. All we need to do is find the best weights. \n",
    "\n",
    "$$ h(x,w_0,w_1) = w_0 + w_1 x\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Batch gradient descent (single variable)\n",
    "\n",
    "For future reference the partial derivatives of $h(x,w)$ are calculated with respect to $w_0$ and to $w_1$:\n",
    "\n",
    "$$ \\frac{\\partial h}{\\partial w_0} \n",
    "    = \\frac{\\partial}{\\partial w_0} \\left( w_0 + w_1 x \\right) \n",
    "    = 1\n",
    "$$ \n",
    "and\n",
    "$$ \\frac{\\partial h}{\\partial w_1} \n",
    "    = \\frac{\\partial}{\\partial w_1} \\left( w_0 + w_1 x \\right) \n",
    "    = x\n",
    "$$\n",
    "\n",
    "The _optimal weights_ are determined by minimizing the cost function: \n",
    "\n",
    "$$ \\begin{align}\n",
    "    C(w_0,w_1) = & \\frac{1}{2m} \\sum_{i=1}^m \\left[ h(x^i,w_0,w_1) - y^i \\right]^2\n",
    "\\\\             = & \\frac{1}{2m} \\sum_{i=1}^m \\left[ w_0 + w_1 x^i - y^i\\right]^2\n",
    "\\end{align} $$\n",
    "which is quadratic in $w_0$ and $w_1$ and has an absolute minimum, which we can easily find\n",
    "by following the negative of the gradient. A two variable function creates a surface and the gradient function returns the direction of steepest ascent for any given pair of input values for that function. The reverse direction is the steepest descent ...\n",
    "\n",
    "with respect to the weights where $m$ is the number of samples and $x^i$ and $y^i$ are the independent and dependent variables for the $i$-th sample/row.\n",
    "\n",
    "For each pair of values $x^i$ and $y^i$ the \n",
    "\n",
    "The partial derivatives of $C$ with respect to $w_0$ and to $w_1$ are calculated:\n",
    "\n",
    "$$ \n",
    "\\begin{align}\n",
    "\\frac{\\partial C}{\\partial w_0} = & \\frac{1}{2m} \\sum_{i=1}^m 2 \\left[ h(x^i,w) - y^i \\right]\n",
    "\\\\\n",
    "                                = & \\frac{1}{m} \\sum_{i=1}^m \\left[ h(x^i,w) - y^i \\right]\n",
    "\\\\\n",
    "\\frac{\\partial C}{\\partial w_1} = & \\frac{1}{2m} \\sum_{i=1}^m 2 \\left[ h(x^i,w) - y^i \\right]\n",
    "                                     x^i\n",
    "\\\\\n",
    "                                = & \\frac{1}{m} \\sum_{i=1}^m \\left[ h(x^i,w) - y^i \\right]\n",
    "                                     x^i\n",
    "\\\\\n",
    "                                = & \\frac{1}{m} \\sum_{i=1}^m \\left[ w_0 + w_1 x^i - y^i \\right]\n",
    "                                     x^i\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "The update rule(s):\n",
    "\n",
    "$$ \\begin{align}\n",
    "    w_j \\colon = & w_j - a \\frac{\\partial C}{\\partial w_j} \\qquad \\text{for $j=0,1$}\n",
    "\\\\  w_0 \\colon = & w_0 - a \\frac{\\partial C}{\\partial w_0}\n",
    "\\\\  w_1 \\colon = & w_1 - a \\frac{\\partial C}{\\partial w_1}\n",
    "\\end{align} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Logistic regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The logistic function is $h_W(x) = s(W^T x)\n",
    "$\n",
    "where\n",
    "$ s(t) = \\frac{1}{1 + e^{-t}}\n",
    "$\n",
    "which maps\n",
    "- negative numbers to the range $(0,0.5)$\n",
    "- positive numbers to the range $(0.5,1)$\n",
    "- zero $0$ to $0.5$\n",
    "\n",
    "For more on the function $s$ see [Sigmoid function](https://en.wikipedia.org/wiki/Sigmoid_function) at Wikipedia."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The cost function, for a single pair of values $x, y$, is\n",
    "$$ C(x) = \\frac{1}{2} \\left[ h_W(x) - y \\right]^2\n",
    "$$\n",
    "doesn't work, but the following does:\n",
    "$$ \\begin{align}\n",
    "    C(x,y) = & - \\log(h_W(x))                          & \\text{if $y=1$}\n",
    "\\\\  C(x,y) = & - \\log(1-h_W(x))                        & \\text{if $y=0$}\n",
    "\\\\  C(x,y) = & - y \\log(h_W(x)) - (1-y) \\log(1-h_W(x)) & \n",
    "\\end{align} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now put this together to obtain a cost function for all samples and targets:\n",
    "\n",
    "$$ \\begin{align}\n",
    "    C(x,y) = & \\frac{-1}{m} \\left[ \\sum_{i=1}^m y^i \\log(h_W(x^i)) - (1-y^i) \\log(1-h_W(x^i))\\right] \n",
    "\\end{align} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "- https://getpocket.com/a/read/824013193\n",
    "- http://keras.io/activations/\n",
    "- http://keras.io/layers/core/\n",
    "- http://keras.io/layers/about-keras-layers/\n",
    "- "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Activation functions\n",
    "- happen after the matrix product of weights with node or input values\n",
    "- sigmoid is the logistic function above\n",
    "- there are other options\n",
    "- tanh \n",
    "- Rectified Linear Unit (ReLU) - zero when negative, otherwise same"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Models have:\n",
    "- Optimization methods\n",
    "- Loss/cost functions\n",
    "\n",
    "Layers (?) have:\n",
    "- Activitation\n",
    "- Init\n",
    "- Weights \n",
    "- Regularization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From [CS231n Convolutional Neural Networks for Visual Recognition](https://getpocket.com/a/read/824013193)  \n",
    "    \n",
    "\"To give you some context, modern Convolutional Networks contain on orders of 100 million parameters and are usually made up of approximately 10 to 20 layers (hence deep learning).\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
