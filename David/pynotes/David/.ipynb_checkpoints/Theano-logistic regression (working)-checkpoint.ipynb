{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# title?\n",
    "\n",
    "Sourced from: \n",
    "- Fundamentals of Deep Learning<br>\n",
    "by Nikhil Buduma<br>\n",
    "Published by O'Reilly Media, Inc., 2016"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  9.,  35.,  91.])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import theano.tensor as T\n",
    "import theano as th\n",
    "from theano.printing import pydotprint\n",
    "\n",
    "a = T.dvector('a')\n",
    "b = T.dvector('b')\n",
    "c = a**3 + b**3\n",
    "f = th.function(inputs=[a,b], outputs=c)\n",
    "f([1, 2, 3], [2, 3, 4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import theano.tensor as T \n",
    "from theano import function\n",
    "from theano import shared\n",
    "\n",
    "state = shared(0)\n",
    "query = T.dvector('query')\n",
    "W = T.dvector('W') # model parameter vector\n",
    "result = T.dot(W, query) > 0\n",
    "sentiment = function(inputs=[query], \n",
    "                     outputs=result, \n",
    "                     updates=[(state, state + 1)],\n",
    "                     givens={W : np.array([1, -2, 3, -0.5, 1])})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(2)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "state.get_value()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(0, dtype=int8)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentiment([1, 6, 0, 9, 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(3)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "state.get_value()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(1, dtype=int8)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentiment([1, -6, 0, -9, 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(4)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "state.get_value()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic regression (FDL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def __init__(self, input, input_dim, output_dim):\n",
    "    \"\"\"\n",
    "    PARAM input : theano.tensor.TensorType\n",
    "    A symbolic variable that we'll use to represent one minibatch of our\n",
    "    dataset\n",
    "\n",
    "    PARAM input_dim : int\n",
    "    This will represent the number of input neurons in our model\n",
    "\n",
    "    PARAM ouptut_dim : int \n",
    "    This will represent the number of neurons in the output layer (i.e. \n",
    "    the number of possible classifications for the input)\n",
    "    \"\"\"\n",
    "    \n",
    "    # We initialize the weight matrix W of size (input_dim, output_dim)\n",
    "    self.W = theano.shared(\n",
    "            value=np.zeros((input_dim, output_dim)),\n",
    "            name='W',\n",
    "            borrow=True\n",
    "        )\n",
    "\n",
    "    # We initialize a bias vector for the neurons of the output layer\n",
    "    self.b = theano.shared(\n",
    "            value=np.zeros(output_dim),\n",
    "            name='b',\n",
    "            borrow='True'\n",
    "        )\n",
    "\n",
    "    # Symbolic description of how to compute class membership probabilities\n",
    "    self.output = T.nnet.softmax(T.dot(input, self.W) + self.b)\n",
    "\n",
    "    # Symbolic description of the final prediction\n",
    "    self.predicted = T.argmax(self.output, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def logistic_network_cost(self, y, lambda_l2=0):\n",
    "    \"\"\"\n",
    "    Here we express the cost incurred by an example given the correct\n",
    "    distribution\n",
    "\n",
    "    PARAM y : theano.tensor.TensorType\n",
    "    These are the correct answers, and we compute the cost with \n",
    "    respect to this ground truth (over the entire minibatch). This \n",
    "    means that y is of size (minibatch_size, output_dim)\n",
    "\n",
    "    PARAM lambda : float\n",
    "    This is the L2 regularization parameter that we use to penalize large\n",
    "    values for components of W, thus discouraging potential overfitting\n",
    "    \"\"\"\n",
    "    # Calculate the log probabilities of the softmax output\n",
    "    log_probabilities = T.log(self.output)\n",
    "\n",
    "    # We use these log probabilities to compute the negative log likelihood\n",
    "    negative_log_likelihood = -T.mean(log_probabilities[T.arange(y.shape[0]), y])\n",
    "        \n",
    "    # Compute the L2 regularization component of the cost function\n",
    "    l2_regularization = lambda_l2 * (self.W ** 2).sum()\n",
    "        \n",
    "    # Return a symbolic description of the cost function\n",
    "    return negative_log_likelihood + l2_regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def error_rate(self, y):\n",
    "    \"\"\"\n",
    "    Here we return the error rate of the model over a set of given labels\n",
    "    (perhaps in a minibatch, in the validation set, or the test set)\n",
    "\n",
    "    PARAM y : theano.tensor.TensorType\n",
    "    These are the correct answers, and we compute the cost with \n",
    "    respect to this ground truth (over the entire minibatch). This \n",
    "    means that y is of size (minibatch_size, output_dim) \n",
    "    \"\"\"\n",
    "\n",
    "    # Make sure y is of the correct dimension \n",
    "    assert y.ndim == self.predicted.ndim\n",
    "\n",
    "    # Make sure that ys  contains values of the correct data type (ints)\n",
    "    assert y.dtype.startswith('int')\n",
    "\n",
    "    # Return the error rate on the data \n",
    "    return T.mean(T.neq(self.predicted, y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "We will use this class to represent a simple logistic regression\n",
    "classifier. We'll represent this in Theano as a neural network \n",
    "with no hidden layers. This is our first attempt at building a \n",
    "neural network model to solve interesting problems. Here, we'll\n",
    "use this class to crack the MNIST handwritten digit dataset problem,\n",
    "but this class has been constructed so that it can be reappropriated\n",
    "to any use!\n",
    "\n",
    "References:\n",
    "    - textbooks: \"Pattern Recognition and Machine Learning\", Christopher M. Bishop, section 4.3.2\n",
    "    - websites: http://deeplearning.net/tutorial, Lisa Lab\n",
    "\"\"\"\n",
    "\n",
    "import numpy as np\n",
    "import theano.tensor as T \n",
    "import theano\n",
    "\n",
    "class LogisticNetwork(object):\n",
    "    \"\"\"\n",
    "    The logistic regression class is described by two parameters (which\n",
    "    we will want to learn). The first is a weight matrix. We'll refer to\n",
    "    this weight matrix as W. The second is a bias vector b. Refer to the \n",
    "    text if you want to learn more about how this network works. Let's get\n",
    "    started!\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, input, input_dim, output_dim):\n",
    "        \"\"\"\n",
    "        We first initialize the logistic network object with some important\n",
    "        information.\n",
    "\n",
    "        PARAM input : theano.tensor.TensorType\n",
    "        A symbolic variable that we'll use to represent one minibatch of our\n",
    "        dataset\n",
    "\n",
    "        PARAM input_dim : int\n",
    "        This will represent the number of input neurons in our model\n",
    "\n",
    "        PARAM ouptut_dim : int \n",
    "        This will represent the number of neurons in the output layer (i.e. \n",
    "        the number of possible classifications for the input)\n",
    "        \"\"\"\n",
    "\n",
    "        # We initialize the weight matrix W of size (input_dim, output_dim)\n",
    "        self.W = theano.shared(\n",
    "                value=np.zeros((input_dim, output_dim)),\n",
    "                name='W',\n",
    "                borrow=True\n",
    "            )\n",
    "\n",
    "        # We initialize a bias vector for the neurons of the output layer\n",
    "        self.b = theano.shared(\n",
    "                value=np.zeros(output_dim),\n",
    "                name='b',\n",
    "                borrow='True'\n",
    "            )\n",
    "\n",
    "        # Symbolic description of how to compute class membership probabilities\n",
    "        self.output = T.nnet.softmax(T.dot(input, self.W) + self.b)\n",
    "\n",
    "        # Symbolic description of the final prediction\n",
    "        self.predicted = T.argmax(self.output, axis=1)\n",
    "\n",
    "    def logistic_network_cost(self, y, lambda_l2=0):\n",
    "        \"\"\"\n",
    "        Here we express the cost incurred by an example given the correct\n",
    "        distribution\n",
    "\n",
    "        PARAM y : theano.tensor.TensorType\n",
    "        These are the correct answers, and we compute the cost with \n",
    "        respect to this ground truth (over the entire minibatch). This \n",
    "        means that y is of size (minibatch_size, output_dim)\n",
    "\n",
    "        PARAM lambda : float\n",
    "        This is the L2 regularization parameter that we use to penalize large\n",
    "        values for components of W, thus discouraging potential overfitting\n",
    "        \"\"\"\n",
    "        # Calculate the log probabilities of the softmax output\n",
    "        log_probabilities = T.log(self.output)\n",
    "\n",
    "        # We use these log probabilities to compute the negative log likelihood\n",
    "        negative_log_likelihood = -T.mean(log_probabilities[T.arange(y.shape[0]), y])\n",
    "        \n",
    "        # Compute the L2 regularization component of the cost function\n",
    "        l2_regularization = lambda_l2 * (self.W ** 2).sum()\n",
    "        \n",
    "        # Return a symbolic description of the cost function\n",
    "        return negative_log_likelihood + l2_regularization\n",
    "\n",
    "    def error_rate(self, y):\n",
    "        \"\"\"\n",
    "        Here we return the error rate of the model over a set of given labels\n",
    "        (perhaps in a minibatch, in the validation set, or the test set)\n",
    "\n",
    "        PARAM y : theano.tensor.TensorType\n",
    "        These are the correct answers, and we compute the cost with \n",
    "        respect to this ground truth (over the entire minibatch). This \n",
    "        means that y is of size (minibatch_size, output_dim)\n",
    "        \"\"\"\n",
    "\n",
    "        # Make sure y is of the correct dimension \n",
    "        assert y.ndim == self.predicted.ndim\n",
    "\n",
    "        # Make sure that y contains values of the correct data type (ints)\n",
    "        assert y.dtype.startswith('int')\n",
    "\n",
    "        # Return the error rate on the data \n",
    "        return T.mean(T.neq(self.predicted, y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'training_set' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-17-83edc0b4289f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;31m# We now instantiate the shared datasets\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m \u001b[0mtraining_set_x\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0mtraining_set_y\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mshared_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtraining_set\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m \u001b[0mvalidation_set_x\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_set_y\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mshared_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalidation_set\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0mtest_set_x\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_set_y\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mshared_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_set\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'training_set' is not defined"
     ]
    }
   ],
   "source": [
    "def shared_dataset(data_xy):\n",
    "    \"\"\"\n",
    "    We store the data in a shared variable because it allows Theano to copy it\n",
    "    into GPU memory (if GPU utilization is enabled). By default, if a variable is\n",
    "    not shared, it is moved to GPU at every use. This results in a big performance\n",
    "    hit because that means the data will be copied one minibatch at a time. Instead,\n",
    "    if we use shared variables, we don't have to worry about copying data \n",
    "    repeatedly.\n",
    "    \"\"\"\n",
    "\n",
    "    data_x, data_y = data_xy\n",
    "    shared_x = shared(np.asarray(data_x, dtype=config.floatX), borrow=True)\n",
    "    shared_y = shared(np.asarray(data_y, dtype='int32'), borrow=True)\n",
    "    return shared_x, shared_y\n",
    "\n",
    "# We now instantiate the shared datasets\n",
    "training_set_x , training_set_y = shared_dataset(training_set)\n",
    "validation_set_x, validation_set_y = shared_dataset(validation_set)\n",
    "test_set_x, test_set_y = shared_dataset(test_set) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'training_set_x' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-18-eac71cf718bc>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Lets compute the number of minibatches for training, validation, and testing\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mn_training_batches\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtraining_set_x\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_value\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mborrow\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mBATCH_SIZE\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mn_validation_batches\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalidation_set_x\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_value\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mborrow\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mBATCH_SIZE\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mn_test_batches\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtest_set_x\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_value\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mborrow\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mBATCH_SIZE\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'training_set_x' is not defined"
     ]
    }
   ],
   "source": [
    "# Lets compute the number of minibatches for training, validation, and testing\n",
    "n_training_batches = training_set_x.get_value(borrow=True).shape[0] / BATCH_SIZE\n",
    "n_validation_batches = validation_set_x.get_value(borrow=True).shape[0] / BATCH_SIZE\n",
    "n_test_batches = test_set_x.get_value(borrow=True).shape[0] / BATCH_SIZE\n",
    "\n",
    "# Now it's time for us to build the model! \n",
    "#Let's start of with an index to the minibatch we're using\n",
    "index = T.lscalar() \n",
    "\n",
    "# Generate symbolic variables for the input (a minibatch)\n",
    "x = T.dmatrix('x')\n",
    "y = T.ivector('y')\n",
    "\n",
    "# Construct the logistic network model\n",
    "# Keep in mind MNIST image is of size (28, 28)\n",
    "# Also number of output class is is 10 (digits 0, 1, ..., 9)\n",
    "model = logistic_network.LogisticNetwork(input=x, input_dim=28*28, output_dim=10)\n",
    "\n",
    "# Obtain a symbolic expression for the objective function\n",
    "# EXPERIMENT!!! Play around with L2 regression parameter!\n",
    "objective = model.logistic_network_cost(y, lambda_l2=0.0001)\n",
    "\n",
    "# Obtain a symbolic expression for the error incurred\n",
    "error = model.error_rate(y)\n",
    "\n",
    "# Compute symbolic gradients of objective with respect to model parameters\n",
    "dW, db = T.grad(objective, model.W), T.grad(objective, model.b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'index' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-19-44504398f89b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Compile theano function for training with a minibatch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m train_model = function(\n\u001b[0;32m----> 3\u001b[0;31m         \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m         \u001b[0moutputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mobjective\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m         updates=[\n",
      "\u001b[0;31mNameError\u001b[0m: name 'index' is not defined"
     ]
    }
   ],
   "source": [
    "# Compile theano function for training with a minibatch\n",
    "train_model = function(\n",
    "        inputs=[index],\n",
    "        outputs=objective, \n",
    "        updates=[\n",
    "            (model.W, model.W - LEARNING_RATE * dW),\n",
    "            (model.b, model.b - LEARNING_RATE * db)\n",
    "        ],\n",
    "        givens={\n",
    "            x : training_set_x[index * BATCH_SIZE : (index + 1) * BATCH_SIZE],\n",
    "            y : training_set_y[index * BATCH_SIZE : (index + 1) * BATCH_SIZE]\n",
    "        }\n",
    "    )\n",
    "\n",
    "# Compile theano functions for validation and testing\n",
    "validate_model = function(\n",
    "        inputs=[index],\n",
    "        outputs=error,\n",
    "        givens={\n",
    "            x : validation_set_x[index * BATCH_SIZE : (index + 1) * BATCH_SIZE],\n",
    "            y : validation_set_y[index * BATCH_SIZE : (index + 1) * BATCH_SIZE]\n",
    "        }\n",
    "    )\n",
    "\n",
    "test_model = function(\n",
    "        inputs=[index],\n",
    "        outputs=error,\n",
    "        givens={\n",
    "            x : test_set_x[index * BATCH_SIZE : (index + 1) * BATCH_SIZE],\n",
    "            y : test_set_y[index * BATCH_SIZE : (index + 1) * BATCH_SIZE]\n",
    "        }\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'n_training_batches' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-21-a3360bbbcb0a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;31m# We go through this number of minbatches before we check on the validation set\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m \u001b[0mvalidation_freq\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_training_batches\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpatience\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;31m# We keep of the best loss on the validation set here\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'n_training_batches' is not defined"
     ]
    }
   ],
   "source": [
    "# Let's set up the early stopping parameters (based on the validation set)\n",
    "# Must look at this many examples no matter what\n",
    "patience = 5000\n",
    "\n",
    "# Wait this much longer if a new best is found                     \n",
    "patience_increase = 2\n",
    "\n",
    "# This is when an improvement is significant\n",
    "improvement_threshold = 0.995\n",
    "\n",
    "# We go through this number of minbatches before we check on the validation set\n",
    "validation_freq = min(n_training_batches, patience / 2)\n",
    "\n",
    "# We keep of the best loss on the validation set here\n",
    "best_loss = np.inf\n",
    "\n",
    "# We also keep track of the epoch we are in\n",
    "epoch = 0\n",
    "\n",
    "# A boolean flag that propagates when patience has been exceeded\n",
    "exceeded_patience = False\n",
    "\n",
    "# Now we're ready to start training the model\n",
    "print (\"... TRAINING MODEL ...\")\n",
    "start_time = time.clock()\n",
    "while (epoch < N_EPOCHS) and not exceeded_patience:\n",
    "    epoch = epoch + 1\n",
    "    for minibatch_index in xrange(n_training_batches):\n",
    "        minibatch_objective = train_model(minibatch_index)\n",
    "        iteration = (epoch - 1) * n_training_batches + minibatch_index\n",
    "\n",
    "        if (iteration + 1) % validation_freq == 0:\n",
    "            # Compute loss on validation set\n",
    "            validation_losses = [validate_model(i) for i in xrange(n_validation_batches)]\n",
    "            validation_loss = np.mean(validation_losses)\n",
    "\n",
    "            print ('epoch %i, minibatch %i/%i, validation error: %f %%' % (\n",
    "                    epoch,\n",
    "                    minibatch_index + 1,\n",
    "                    n_training_batches,\n",
    "                    validation_loss * 100\n",
    "                ))\n",
    "\n",
    "            if validation_loss < best_loss:\n",
    "                if validation_loss < best_loss * improvement_threshold:\n",
    "                    patience = max(patience, iteration * patience_increase)\n",
    "                best_loss = validation_loss\n",
    "\n",
    "        if patience <= iteration:\n",
    "            exceeded_patience = True\n",
    "            break\n",
    "end_time = time.clock()\n",
    "\n",
    "# Let's compute how well we do on the test set\n",
    "test_losses = [test_model(i) for i in xrange(n_test_batches)]\n",
    "test_loss = np.mean(test_losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "We'll now use the LogisticNetwork object we built in logistic_network.py in \n",
    "order to tackle the MNIST dataset challenge. We will use minibatch gradient\n",
    "descent to train this simplistic network model. \n",
    "\n",
    "References:\n",
    "    - textbooks: \"Pattern Recognition and Machine Learning\", Christopher M. Bishop, section 4.3.2\n",
    "    - websites: http://deeplearning.net/tutorial, Lisa Lab\n",
    "\"\"\"\n",
    "\n",
    "__docformat__ = 'restructedtext en'\n",
    "\n",
    "#import cPickle\n",
    "import gzip\n",
    "import os\n",
    "import time\n",
    "import urllib\n",
    "from theano import function, shared, config\n",
    "import theano.tensor as T \n",
    "import numpy as np\n",
    "import logistic_network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Let's start off by defining some constants\n",
    "# EXPERIMENT!!! Play around the the learning rate!\n",
    "LEARNING_RATE = 0.2\n",
    "N_EPOCHS = 1000\n",
    "DATASET = 'mnist.pkl.gz'\n",
    "BATCH_SIZE = 600\n",
    "\n",
    "# Time to check if we have the data and if we don't, let's download it \n",
    "print (\"... LOADING DATA ...\" )\n",
    "\n",
    "data_path = os.path.join(\n",
    "        os.path.split(__file__)[0],\n",
    "        \"..\",\n",
    "        \"data\",\n",
    "        DATASET\n",
    "    )\n",
    "\n",
    "if (not os.path.isfile(data_path)):\n",
    "    import urllib\n",
    "    origin = (\n",
    "            'http://www.iro.umontreal.ca/~lisa/deep/data/mnist/mnist.pkl.gz'\n",
    "        )\n",
    "    print ('Downloading data from %s' % origin)\n",
    "    urllib.urlretrieve(origin, data_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# Time to build our models\n",
    "print (\"... BUILDING MODEL ...\")\n",
    "\n",
    "# Load the dataset\n",
    "data_file = gzip.open(data_path, 'rb')\n",
    "training_set, validation_set, test_set = cPickle.load(data_file)\n",
    "data_file.close()\n",
    "\n",
    "# Define a quick function to established a shared dataset (for efficiency)\n",
    "\n",
    "def shared_dataset(data_xy):\n",
    "    \"\"\"\n",
    "    We store the data in a shared variable because it allows Theano to copy it\n",
    "    into GPU memory (if GPU utilization is enabled). By default, if a variable is\n",
    "    not shared, it is moved to GPU at every use. This results in a big performance\n",
    "    hit because that means the data will be copied one minibatch at a time. Instead,\n",
    "    if we use shared variables, we don't have to worry about copying data \n",
    "    repeatedly.\n",
    "    \"\"\"\n",
    "\n",
    "    data_x, data_y = data_xy\n",
    "    shared_x = shared(np.asarray(data_x, dtype=config.floatX), borrow=True)\n",
    "    shared_y = shared(np.asarray(data_y, dtype='int32'), borrow=True)\n",
    "    return shared_x, shared_y\n",
    "\n",
    "# We now instantiate the shared datasets\n",
    "training_set_x , training_set_y = shared_dataset(training_set)\n",
    "validation_set_x, validation_set_y = shared_dataset(validation_set)\n",
    "test_set_x, test_set_y = shared_dataset(test_set) \n",
    "\n",
    "# Lets compute the number of minibatches for training, validation, and testing\n",
    "n_training_batches = training_set_x.get_value(borrow=True).shape[0] / BATCH_SIZE\n",
    "n_validation_batches = validation_set_x.get_value(borrow=True).shape[0] / BATCH_SIZE\n",
    "n_test_batches = test_set_x.get_value(borrow=True).shape[0] / BATCH_SIZE\n",
    "\n",
    "# Now it's time for us to build the model! \n",
    "#Let's start of with an index to the minibatch we're using\n",
    "index = T.lscalar() \n",
    "\n",
    "# Generate symbolic variables for the input (a minibatch)\n",
    "x = T.dmatrix('x')\n",
    "y = T.ivector('y')\n",
    "\n",
    "# Construct the logistic network model\n",
    "# Keep in mind MNIST image is of size (28, 28)\n",
    "# Also number of output class is is 10 (digits 0, 1, ..., 9)\n",
    "#model = logistic_network.LogisticNetwork(input=x, input_dim=28*28, output_dim=10)\n",
    "model = LogisticNetwork(input=x, input_dim=28*28, output_dim=10)\n",
    "\n",
    "# Obtain a symbolic expression for the objective function\n",
    "# EXPERIMENT!!! Play around with L2 regression parameter!\n",
    "objective = model.logistic_network_cost(y, lambda_l2=0.0001)\n",
    "\n",
    "# Obtain a symbolic expression for the error incurred\n",
    "error = model.error_rate(y)\n",
    "\n",
    "# Compute symbolic gradients of objective with respect to model parameters\n",
    "dW, db = T.grad(objective, model.W), T.grad(objective, model.b)\n",
    "\n",
    "# Compile theano function for training with a minibatch\n",
    "train_model = function(\n",
    "        inputs=[index],\n",
    "        outputs=objective, \n",
    "        updates=[\n",
    "            (model.W, model.W - LEARNING_RATE * dW),\n",
    "            (model.b, model.b - LEARNING_RATE * db)\n",
    "        ],\n",
    "        givens={\n",
    "            x : training_set_x[index * BATCH_SIZE : (index + 1) * BATCH_SIZE],\n",
    "            y : training_set_y[index * BATCH_SIZE : (index + 1) * BATCH_SIZE]\n",
    "        }\n",
    "    )\n",
    "\n",
    "# Compile theano functions for validation and testing\n",
    "validate_model = function(\n",
    "        inputs=[index],\n",
    "        outputs=error,\n",
    "        givens={\n",
    "            x : validation_set_x[index * BATCH_SIZE : (index + 1) * BATCH_SIZE],\n",
    "            y : validation_set_y[index * BATCH_SIZE : (index + 1) * BATCH_SIZE]\n",
    "        }\n",
    "    )\n",
    "\n",
    "test_model = function(\n",
    "        inputs=[index],\n",
    "        outputs=error,\n",
    "        givens={\n",
    "            x : test_set_x[index * BATCH_SIZE : (index + 1) * BATCH_SIZE],\n",
    "            y : test_set_y[index * BATCH_SIZE : (index + 1) * BATCH_SIZE]\n",
    "        }\n",
    "    )\n",
    "\n",
    "# Let's set up the early stopping parameters (based on the validation set)\n",
    "\n",
    "# Must look at this many examples no matter what\n",
    "patience = 5000\n",
    "\n",
    "# Wait this much longer if a new best is found                     \n",
    "patience_increase = 2\n",
    "\n",
    "# This is when an improvement is significant\n",
    "improvement_threshold = 0.995\n",
    "\n",
    "# We go through this number of minbatches before we check on the validation set\n",
    "validation_freq = min(n_training_batches, patience / 2)\n",
    "\n",
    "# We keep of the best loss on the validation set here\n",
    "best_loss = np.inf\n",
    "\n",
    "# We also keep track of the epoch we are in\n",
    "epoch = 0\n",
    "\n",
    "# A boolean flag that propagates when patience has been exceeded\n",
    "exceeded_patience = False\n",
    "\n",
    "# Now we're ready to start training the model\n",
    "print (\"... TRAINING MODEL ...\")\n",
    "start_time = time.clock()\n",
    "while (epoch < N_EPOCHS) and not exceeded_patience:\n",
    "    epoch = epoch + 1\n",
    "    for minibatch_index in xrange(n_training_batches):\n",
    "        minibatch_objective = train_model(minibatch_index)\n",
    "        iteration = (epoch - 1) * n_training_batches + minibatch_index\n",
    "\n",
    "        if (iteration + 1) % validation_freq == 0:\n",
    "            # Compute loss on validation set\n",
    "            validation_losses = [validate_model(i) for i in xrange(n_validation_batches)]\n",
    "            validation_loss = np.mean(validation_losses)\n",
    "\n",
    "            print ('epoch %i, minibatch %i/%i, validation error: %f %%' % (\n",
    "                    epoch,\n",
    "                    minibatch_index + 1,\n",
    "                    n_training_batches,\n",
    "                    validation_loss * 100\n",
    "                ))\n",
    "\n",
    "            if validation_loss < best_loss:\n",
    "                if validation_loss < best_loss * improvement_threshold:\n",
    "                    patience = max(patience, iteration * patience_increase)\n",
    "                best_loss = validation_loss\n",
    "\n",
    "        if patience <= iteration:\n",
    "            exceeded_patience = True\n",
    "            break\n",
    "end_time = time.clock()\n",
    "\n",
    "# Let's compute how well we do on the test set\n",
    "test_losses = [test_model(i) for i in xrange(n_test_batches)]\n",
    "test_loss = np.mean(test_losses)\n",
    "\n",
    "# Print out the results!\n",
    "print ('\\n')\n",
    "print ('Optimization complete with best validation score of %f %%' % (best_loss * 100))\n",
    "print ('And with a test score of %f %%' % (test_loss * 100))\n",
    "print ('\\n')\n",
    "print ('The code ran for %d epochs and for a total time of %.1f seconds' % (epoch, end_time - start_time))\n",
    "print ('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
