{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Presentation on Clustering Methods\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## First, what is clustering?\n",
    "\n",
    "Clustering is a method of unsupervised learning in the ML world. The main difference between supervised and unsupervised methods is that supervised learning takes place based on a set of training data labeled with the value of interest.\n",
    "\n",
    "Unsupervised learning happens with unlabeled data points and the goal is to organize the data in a certain way as to explain it's structure. Examples include dimmensionality reduction, clustering and association rule learning.\n",
    "\n",
    "Clustering = assigning a set of observations into sub groups called clusters so that the sub groups contain observations which are in some way similar to each other."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How can we use it?\n",
    "\n",
    "MSD contains so many levels of description for each song that there must be a way to group in order to determine similarity.\n",
    "\n",
    "With similarity comes to potential to predict whether a new song that comes across our program will be of interest to our sample listener."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Which methods can we use?\n",
    "\n",
    "We would like to explore the K-means method of clustering. The method is one of the most simple as it just seeks to minimize the sum of squared distances between data and corresponding cluster centroids, a minimized distance signifying groups which indicate similarity amongst groupmates.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.16683608,  0.13191816,  0.15684959, ...,  0.1050344 ,\n",
       "         0.21577383,  0.10183515],\n",
       "       [ 0.42626993,  0.45810551,  0.75120848, ..., -0.23867008,\n",
       "         0.30173752,  0.40888344],\n",
       "       [ 0.3015672 ,  0.24165217,  0.45952304, ..., -0.07477049,\n",
       "         0.19042007, -0.00645329],\n",
       "       ..., \n",
       "       [ 0.83213269,  0.77370351,  0.96810357, ...,  0.94445609,\n",
       "         0.81291383,  1.09524607],\n",
       "       [ 0.92610938,  0.88605987,  1.05215255, ...,  0.62569843,\n",
       "         1.0173104 ,  0.82896229],\n",
       "       [ 0.83806377,  0.66844254,  0.70916445, ...,  1.01792605,\n",
       "         0.89693842,  1.06241922]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "def get_random_data():\n",
    "    x_1 = np.random.normal(loc=0.2,scale=0.2,size=(100,100))\n",
    "    x_2 = np.random.normal(loc=0.9,scale=0.1,size=(100,100))\n",
    "    x = np.r_[x_1,x_2]\n",
    "    return x\n",
    "\n",
    "x = get_random_data()\n",
    "x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These code examples are from the book to demonstrate the basic K-means clustering as applied to a sample set of data.\n",
    "\n",
    "Here we sample from two different normal distributions in order to create a 200x100 matrix of random numbers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_score\n",
    "\n",
    "def form_clusters(x,k):\n",
    "       \"\"\"\n",
    "       Build clusters\n",
    "       \"\"\"\n",
    "        \n",
    "       no_clusters = k\n",
    "       model = KMeans(n_clusters=no_clusters,init='random')\n",
    "       model.fit(x)\n",
    "       labels = model.labels_\n",
    "       print (labels)\n",
    "       # Cacluate the silhouette score\n",
    "       sh_score = silhouette_score(x,labels)\n",
    "       return sh_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this function, we are manually assigning the number of clusters, k, to use although we need to test later which is the optimal number to use...\n",
    "\n",
    "model.fix(x) actually runs the K-means algo which iteratively creates cluster points, measures the euclidean distances from each cluster and the closest data points, and adjusts the locations of the clusters to try to incrementally move to capture these groups of data.\n",
    "\n",
    "### Note the use of init='random'...where the clusters will be initially created can cause problems when done randomly\n",
    "\n",
    "The silouette score simply calculates a cluster radius relative to its distance from other clusters, positive number up to 1 means the cluster radius is less than the distance between clusters meaning less overlap and better quality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]\n",
      "[0 2 2 0 0 2 0 0 0 0 0 0 2 0 0 2 0 2 2 2 0 0 2 0 2 2 2 2 0 2 2 2 2 0 0 2 0\n",
      " 0 0 2 2 0 0 0 2 0 0 2 2 2 2 0 2 2 2 0 2 2 2 2 0 2 2 0 2 0 0 2 0 2 2 0 0 0\n",
      " 0 2 2 2 0 0 2 2 0 2 0 0 2 0 2 0 2 2 2 0 2 2 2 0 2 0 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]\n",
      "[1 3 3 3 0 3 0 0 3 0 3 3 3 3 0 1 0 3 3 3 0 3 3 1 1 1 3 3 3 1 3 3 3 3 3 1 3\n",
      " 1 0 3 1 1 1 0 3 1 3 3 0 0 1 1 3 1 3 3 3 1 3 0 0 0 3 3 1 3 1 1 0 1 1 0 3 0\n",
      " 3 3 3 3 3 0 1 1 3 3 0 3 0 1 0 3 1 1 3 1 0 3 1 0 3 0 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2]\n",
      "[0 4 4 3 0 4 0 0 4 0 0 3 3 4 2 0 3 4 4 4 0 3 0 0 3 4 4 3 4 2 3 4 0 2 4 0 4\n",
      " 4 4 0 4 0 0 4 3 2 4 4 4 4 3 3 4 0 4 3 0 2 0 4 0 2 4 0 4 4 2 4 3 4 0 4 2 4\n",
      " 3 3 3 0 0 4 3 3 0 0 0 3 4 3 4 3 3 2 4 0 4 4 4 0 0 2 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]\n"
     ]
    }
   ],
   "source": [
    "sh_scores = []\n",
    "for i in range(1,5):\n",
    "    sh_score = form_clusters(x,i+1)\n",
    "    sh_scores.append(sh_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "no_clusters = [i+1 for i in range(1,5)]\n",
    "plt.figure(2)\n",
    "plt.plot(no_clusters,sh_scores)\n",
    "plt.title(\"Cluster Quality\")\n",
    "plt.xlabel(\"No of clusters k\")\n",
    "plt.ylabel(\"Silhouette Coefficient\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we don't know the appropriate number of clusters to use, we can brute force the calculations of overall S-scores for varying cluster numbers, k, in order to find the maximized quality by k.\n",
    "\n",
    "### When we run this clustering method on our timbre/pitch data we will use a more robust calculation for optimal number of clusters called the Gap Statistic.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## So what's the catch?\n",
    "\n",
    "### Limitation: \n",
    "-It is a non-deterministic function meaning different starting points for clusters given the init='random' argument could lead to different clustering of data. If random points are chosen sub-optimally, the minimum that is reached at the end of iteration might not converge to the optimal solution and could just be a local minimum.\n",
    "\n",
    "### Potential Fix? K-means++\n",
    "\n",
    "This method created by Arthur and Vassilvitskii in 2007 (http://ilpubs.stanford.edu:8090/778/1/2006-13.pdf) differs from normal K-means by replacing random cluster seeding with a calculated seeding that should result in faster convergence to the optimal solution.\n",
    "\n",
    "The K-means++ algo is just a cluster initialization to run normal K-means on. It works roughly as follows sourced from https://datasciencelab.wordpress.com/2014/01/15/improved-seeding-for-clustering-with-k-means/."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from IPython.display import Image\n",
    "import os\n",
    "os.chdir(\"Users/mydlo_nich/desktop\")\n",
    "Image(\"cluster.png\")\n",
    "os.chdir(\"Users/mydlo_nich/documents\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
