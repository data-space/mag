{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This project uses a 10,000 record subset of the [Million Songs Dataset](http://labrosa.ee.columbia.edu/millionsong/) , which is a collection of audio features and metadata for a million music tracks collected by The Echo Nest and LabROSA, as well as lyrics supplied by [musiXmatch](http://labrosa.ee.columbia.edu/millionsong/musixmatch). \n",
    "\n",
    "With a selection of variables from the above dataset, lyric-based clusters, and lyric-based sentiment analysis, we will attempt to predict the variable `artist_hotttnesss` using a nueral network. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Datasets\n",
    "## Million Songs Subset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The [million songs subset](http://labrosa.ee.columbia.edu/millionsong/pages/getting-dataset#subset) provides 10,000 tracks in HDF5 file format, one file for each track which can be directly [downloaded](http://static.echonest.com/millionsongsubset_full.tar.gz) or found on the Million Songs website.\n",
    "\n",
    "The features we want to include are as follows (though not all will be used, we may want to include additional features in further analysis):\n",
    "\n",
    "* aritst_id: the Echo Nest artist ID\n",
    "* artist_latitude: latitude\n",
    "* artist_longitude: longitude\n",
    "* artist_location: the location name\n",
    "* song_id: the Echo Nest Song ID\n",
    "* artist_name: Artist name\n",
    "* title: Song title\n",
    "* mode: major/minor\n",
    "* mode_confidence: a confidence measure of the mode\n",
    "* loudness: overall loudness in dB\n",
    "* tempo: estimated tempo in BPM\n",
    "* time_signature: estimated number of beats per bar\n",
    "* time_signature_confidence: confidence measure of time signature\n",
    "* key: the key the song is in\n",
    "* key_confidence: confidence measure of the key\n",
    "* track_id: Echo Nest track ID\n",
    "* artist_hotttnesss: numerical description of how hot and artist currently is\n",
    "\n",
    "Information about more variables can be found at http://labrosa.ee.columbia.edu/millionsong/pages/field-list.\n",
    "\n",
    "In order to pull in these variables , we need to read each file and write the contents to a pandas Dataframe. We do that with the following code, available at [https://github.com/davidoury/MSD](https://github.com/davidoury/MSD)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import itertools as it\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import operator \n",
    "import functools"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Utility Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_filenames(path):\n",
    "    return([get_filenames(path+\"/\"+entry.name)\n",
    "            if entry.is_dir() \n",
    "            else path+\"/\"+entry.name \n",
    "            for entry \n",
    "            in os.scandir(path)\n",
    "           ])\n",
    "\n",
    "def unlist(alist):\n",
    "    return(list(it.chain.from_iterable(alist)\n",
    "               )\n",
    "          )\n",
    "\n",
    "def var_list(base,numof):\n",
    "    return([base+str(ndx) for ndx in range(numof)]\n",
    "          )\n",
    "\n",
    "def h1d_array(in_array,n): \n",
    "    # n1d is the number of elements in `in_array`\n",
    "    n1d = functools.reduce(operator.mul,\n",
    "                           list(in_array.shape))\n",
    "    # return a 1 row 2D array with `n` columns\n",
    "    b = np.ndarray(shape=(1,n1d),\n",
    "                   buffer=in_array,\n",
    "                   dtype=in_array.dtype\n",
    "                  )[0:1,0:n]\n",
    "    return(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def make_1row_df(filename='', metadata_vars=[], analysis_vars=[], remove=False):\n",
    "    # open `filename` as a HDF5 file\n",
    "    store = pd.HDFStore(filename,\"r\")\n",
    "    if remove==False:\n",
    "        # lists `metadata_vars` and `analysis_vars` contain the variables to keep\n",
    "        metadata_var_list = metadata_vars\n",
    "        analysis_var_list = analysis_vars\n",
    "    else: # these lists contain the variables to remove\n",
    "        metadata_var_list = list({item for item \n",
    "                                  in list(store.root.metadata.songs.read().dtype.names) \n",
    "                                  if item not in metadata_remove})\n",
    "        analysis_var_list = list({item for item \n",
    "                                  in list(store.root.analysis.songs.read().dtype.names) \n",
    "                                  if item not in analysis_remove})\n",
    "    ret = pd.concat([\n",
    "            # retrieve a single row dataframe from `/metadata/songs`\n",
    "            pd.DataFrame(store.root.metadata.songs.read(), \n",
    "                         columns=metadata_var_list),\n",
    "            # retrieve a single row dataframe from `/analysis/songs`\n",
    "            pd.DataFrame(store.root.analysis.songs.read(), \n",
    "                         columns=analysis_var_list)\n",
    "            ], \n",
    "            axis=1)\n",
    "    # close the HDF5 file\n",
    "    store.close()\n",
    "    # return the merged dataframe\n",
    "    return(ret)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "path = \"../Data/MillionSongSubset/data\"\n",
    "filenames = unlist(unlist(unlist(get_filenames(path))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "p = re.compile(\"\\.h5$\")\n",
    "filenames = [filename for filename \n",
    "             in filenames if p.search(filename)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, (1, 17))"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mss_df_list = [make_1row_df(filename=filename,\n",
    "                            metadata_vars=['artist_id','artist_latitude','artist_location',\\\n",
    "                                           'artist_longitude','song_id', 'artist_name', 'title','artist_hotttnesss'],\n",
    "                            # Omit: genre\n",
    "                            analysis_vars=['key','key_confidence',\\\n",
    "                                           'mode', 'mode_confidence',\\\n",
    "                                           'loudness', 'tempo', \\\n",
    "                                           'time_signature', 'time_signature_confidence',\\\n",
    "                                           'track_id'],\n",
    "                            # Omit: danceability, energy\n",
    "                            remove=False\n",
    "                           )\n",
    "                for filename in filenames[0:10000] # get data from all 10,000 files\n",
    "              ]\n",
    "len(mss_df_list), mss_df_list[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "project_df = pd.concat(mss_df_list,axis=0).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We look at the first three records and find they're as we expected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>artist_id</th>\n",
       "      <th>artist_latitude</th>\n",
       "      <th>artist_location</th>\n",
       "      <th>artist_longitude</th>\n",
       "      <th>song_id</th>\n",
       "      <th>artist_name</th>\n",
       "      <th>title</th>\n",
       "      <th>artist_hotttnesss</th>\n",
       "      <th>key</th>\n",
       "      <th>key_confidence</th>\n",
       "      <th>mode</th>\n",
       "      <th>mode_confidence</th>\n",
       "      <th>loudness</th>\n",
       "      <th>tempo</th>\n",
       "      <th>time_signature</th>\n",
       "      <th>time_signature_confidence</th>\n",
       "      <th>track_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>b'ARD7TVE1187B99BFB1'</td>\n",
       "      <td>NaN</td>\n",
       "      <td>b'California - LA'</td>\n",
       "      <td>NaN</td>\n",
       "      <td>b'SOMZWCG12A8C13C480'</td>\n",
       "      <td>b'Casual'</td>\n",
       "      <td>b\"I Didn't Mean To\"</td>\n",
       "      <td>0.401998</td>\n",
       "      <td>1</td>\n",
       "      <td>0.736</td>\n",
       "      <td>0</td>\n",
       "      <td>0.636</td>\n",
       "      <td>-11.197</td>\n",
       "      <td>92.198</td>\n",
       "      <td>4</td>\n",
       "      <td>0.778</td>\n",
       "      <td>b'TRAAAAW128F429D538'</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>b'ARMJAGH1187FB546F3'</td>\n",
       "      <td>35.14968</td>\n",
       "      <td>b'Memphis, TN'</td>\n",
       "      <td>-90.04892</td>\n",
       "      <td>b'SOCIWDW12A8C13D406'</td>\n",
       "      <td>b'The Box Tops'</td>\n",
       "      <td>b'Soul Deep'</td>\n",
       "      <td>0.417500</td>\n",
       "      <td>6</td>\n",
       "      <td>0.169</td>\n",
       "      <td>0</td>\n",
       "      <td>0.430</td>\n",
       "      <td>-9.843</td>\n",
       "      <td>121.274</td>\n",
       "      <td>4</td>\n",
       "      <td>0.384</td>\n",
       "      <td>b'TRAAABD128F429CF47'</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>b'ARKRRTF1187B9984DA'</td>\n",
       "      <td>NaN</td>\n",
       "      <td>b''</td>\n",
       "      <td>NaN</td>\n",
       "      <td>b'SOXVLOJ12AB0189215'</td>\n",
       "      <td>b'Sonora Santanera'</td>\n",
       "      <td>b'Amor De Cabaret'</td>\n",
       "      <td>0.343428</td>\n",
       "      <td>8</td>\n",
       "      <td>0.643</td>\n",
       "      <td>1</td>\n",
       "      <td>0.565</td>\n",
       "      <td>-9.689</td>\n",
       "      <td>100.070</td>\n",
       "      <td>1</td>\n",
       "      <td>0.000</td>\n",
       "      <td>b'TRAAADZ128F9348C2E'</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               artist_id  artist_latitude     artist_location  \\\n",
       "0  b'ARD7TVE1187B99BFB1'              NaN  b'California - LA'   \n",
       "1  b'ARMJAGH1187FB546F3'         35.14968      b'Memphis, TN'   \n",
       "2  b'ARKRRTF1187B9984DA'              NaN                 b''   \n",
       "\n",
       "   artist_longitude                song_id          artist_name  \\\n",
       "0               NaN  b'SOMZWCG12A8C13C480'            b'Casual'   \n",
       "1         -90.04892  b'SOCIWDW12A8C13D406'      b'The Box Tops'   \n",
       "2               NaN  b'SOXVLOJ12AB0189215'  b'Sonora Santanera'   \n",
       "\n",
       "                 title  artist_hotttnesss  key  key_confidence  mode  \\\n",
       "0  b\"I Didn't Mean To\"           0.401998    1           0.736     0   \n",
       "1         b'Soul Deep'           0.417500    6           0.169     0   \n",
       "2   b'Amor De Cabaret'           0.343428    8           0.643     1   \n",
       "\n",
       "   mode_confidence  loudness    tempo  time_signature  \\\n",
       "0            0.636   -11.197   92.198               4   \n",
       "1            0.430    -9.843  121.274               4   \n",
       "2            0.565    -9.689  100.070               1   \n",
       "\n",
       "   time_signature_confidence               track_id  \n",
       "0                      0.778  b'TRAAAAW128F429D538'  \n",
       "1                      0.384  b'TRAAABD128F429CF47'  \n",
       "2                      0.000  b'TRAAADZ128F9348C2E'  "
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "project_df.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll save `mss_df` to a *pickle* file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "save_load_path = '../Data/MillionSongSubset/data'\n",
    "project_df.to_pickle(save_load_path+'/project_df.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### VARIABLES"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## musiXmatch Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we need to pull the lyrics from musiXmatch. They lyrics are available as a sqlite database, [mxm_dataset.db](http://labrosa.ee.columbia.edu/millionsong/sites/default/files/AdditionalFiles/mxm_dataset.db). Details about the lyrics dataset can be found at http://labrosa.ee.columbia.edu/millionsong/musixmatch#getting. We'll also need a primer about how to use sqlite databases from within python, and how to load the sqlite3 package. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sqlite3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Connections/Query Database\n",
    "\n",
    "We first need to create a connection to each of our database files. The connection path specifies the directory in which the downloaded files have been stored. We'll use the connection every time we query the database. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "conn_lyrics = sqlite3.connect('../Data/mxm_dataset.db')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After we've created connections, there are a couple of ways we can pull data from the databases:\n",
    "\n",
    "1. We can use SQLite to create a cursor which then fetches records based on our query parameters. A cursor is a database object used to traverse records in a database. The .execute method creates a cursor and then calls the cursors execute method. It returns the results as a list.\n",
    "\n",
    "2. We can use the pandas read_sql function. It returns the results as a pandas DataFrame object\n",
    "\n",
    "First, we'll want to look at the database schema. We'll start off by querying the sqlite_master table from a single sqlite database file using a sqlite cursor. The sqlite_master table defines the schema for the database. We'll use that to get information about the database and create our first query. More information about the sqlite_master table can be found at https://www.sqlite.org/faq.html#q7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(list,\n",
       " [('table', 'words', 'words', 2, 'CREATE TABLE words (word TEXT PRIMARY KEY)'),\n",
       "  ('table',\n",
       "   'lyrics',\n",
       "   'lyrics',\n",
       "   4,\n",
       "   'CREATE TABLE lyrics (track_id, mxm_tid INT, word TEXT, count INT, is_test INT, FOREIGN KEY(word) REFERENCES words(word))')])"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res = conn_lyrics.execute(\"SELECT * FROM sqlite_master WHERE type = 'table'\")\n",
    "schema1 = res.fetchall()\n",
    "type(schema1), schema1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We find two tables in the mxm_dataset, `words` and `lyrics`. We can also see the fields and data types of the columns. The words table contains a list of the top 5000 words of the set. The lyrics table has a row for each track, word, and count of that word. \n",
    "\n",
    "We want to create a dictionary entry for each track, with the track_id as the key and the expanded lyrics as the value. For example, if a word exists in a track five times, we want five copies of that word instead of just a single copy. We'll save the dictionary as a json file and use it to create our clusters later.\n",
    "\n",
    "From the mxm_dataset.db, we want a list of unique tracks. We find lyrics for 237,662 tracks. We'll pare these down later to include only tracks that are common to both the lyrics database and the million song subset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "237662"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tracks = pd.read_sql(\"SELECT DISTINCT track_id FROM lyrics\", con = conn_lyrics)\n",
    "len(tracks) # 237662 tracks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>track_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>TRAAAAV128F421A322</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>TRAAABD128F429CF47</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>TRAAAED128E0783FAB</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>TRAAAEF128F4273421</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>TRAAAEW128F42930C0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             track_id\n",
       "0  TRAAAAV128F421A322\n",
       "1  TRAAABD128F429CF47\n",
       "2  TRAAAED128E0783FAB\n",
       "3  TRAAAEF128F4273421\n",
       "4  TRAAAEW128F42930C0"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tracks.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we'll use the project_df to restrict our lyric selection to those songs that are in both the lyrics database and the subset. \n",
    "\n",
    "We drop the first two characters (b'), and the last character('), from track_id, which are artifacts of the encoding process. We then turn it into a pandas dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "track_id = []\n",
    "track_id = project_df['track_id'].map(lambda x: str(x)[2:len(x)+2])\n",
    "track_id =pd.DataFrame(track_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we merge the tracks that have lyrics available with the tracks in the subset. Since we're only interested in the tracks for which there are lyrics, we use an inner join, which uses an intersection of keys from both dataframes. To learn more about different types of merges: http://pandas.pydata.org/pandas-docs/stable/merging.html#brief-primer-on-merge-methods-relational-algebra . \n",
    "\n",
    "We find that there are 2350 such tracks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2350"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_tracks = track_id.merge(tracks, how='inner', on='track_id')\n",
    "len(df_tracks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we use the tracks from the dataframe to pull the lyrics from the database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# set this to the number of tracks you want to pull from db\n",
    "num_tracks = len(df_tracks['track_id']) \n",
    "# intialize empty dictionary to store tracks and lyrics\n",
    "my_dict = {}\n",
    "\n",
    "for i in range(0,num_tracks): \n",
    "    # assign the value of the track at current index to current track\n",
    "    current_track = df_tracks.track_id[i]\n",
    "\n",
    "    # pull the lyrics for that track and store it in a list\n",
    "    res = conn_lyrics.execute(\"SELECT word, count FROM lyrics WHERE track_id = ?\", [current_track])\n",
    "    results = res.fetchall()\n",
    "\n",
    "    # multiply the word by the number of times it occurs for each word in list\n",
    "    li = [(x[0] + ' ') * x[1] for x in results]\n",
    "    \n",
    "    # use this version to get a single copy of each word\n",
    "    # li = [x[0]for x in results]\n",
    "\n",
    "    # get rid of commas between words\n",
    "    li = str(li).replace(',','')\n",
    "    \n",
    "    # get rid of quotes between words\n",
    "    li = str(li).replace(\"'\",'')\n",
    "    \n",
    "    # add track and lyrics to dictionary\n",
    "    my_dict[current_track] = li"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We verify the number of tracks processed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2350"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(my_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we save the data to a json file. Why json? A dictionary stores data in the same way json stores data, so it seemed appropriate to use. It's human readable. We can open a json file in notepad and see our data. It's also faster than a pickle file, as benchmarked [here](https://kovshenin.com/2010/pickle-vs-json-which-is-faster/) . "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# save to json file in same directory\n",
    "import json\n",
    "with open('lyrics_dict.json', 'w') as fp:\n",
    "    # arguments can include indent=n or None, sort_keys = True\n",
    "    json.dump(my_dict, fp, indent=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have our two data files, we can use the lyrics to perform sentiment analysis and cluster analysis, and include these two variables in our neural network."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clustering Lyrics Using Kmeans"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we transform the corpus into a tf-idf sparse matrix, which will be discussed later. We'll cluster the lyrics using the kmeans algorith. The final number of clusters will be selected based on silhouette score, which will also be discussed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import nltk\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_score\n",
    "from sklearn.metrics import silhouette_samples\n",
    "import matplotlib.pyplot as plt\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We first open our json file, which stores tracks as keys and a list of lyrics as values in a dictionary. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('lyrics_dict_subset.json', 'r') as fp:\n",
    "    data_json = json.load(fp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We access a random track to see the structure of the dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('TRBABAC128E07852A8',\n",
       " '[i i  the  to  a  me me me me me me  not not not  in in in  my my my my my  of of  do do  on on on  all  have  but  there  let  would  at  life  by by by  more  onli  find  call call  face  hear hear  cri cri  o  than  other other  sweet  while while  help  save  deep  pass pass pass  broken  earth  trust  spirit  grace  refrain refrain refrain refrain  merci  heal  wound  besid  seek  comfort  spring  gentl  thee thee  art art  thi thi thi thi  savior savior savior  thou thou thou  throne  kneel  humbl humbl  whom whom  relief ]')"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random.choice(list(data_json.items()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# make this into two lists: tracks and lyrics\n",
    "\n",
    "tracks = list(data_json.keys())\n",
    "lyrics = list(data_json.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### Term Frequency-Inverse Document Frequency"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to turn our list of lyrics into a term frequency-inverse document frequency (tf-idf) sparse matrix. The following definitions are useful to understand when discussing a tf-idf.\n",
    "\n",
    "* _term frequency_ is the number of times that word appears in the document, and it can be (but is not always) divided by the document length to normalize it.\n",
    "\n",
    "* _inverse document frequency_ is a measure of how important a word is. It is computed using the log of the total number of documents divided by the numer of documents with the term in it.\n",
    "\n",
    "* _tf-idf_ is the product of these two terms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fortunately, there is a scikit-learn algorithm, TfidfVectorizer, that does these calculations for us. But first we need to create a list of stopwords, which will be passed as a parameter to the algorithm.\n",
    "\n",
    "The NLTK library comes with a [number of corpora](http://www.nltk.org/howto/corpus.html#other-corpora), one of which includes stopwords in multiple languages. Since our dataset contains songs in more than one language, we combine these stopwords, along with a list of our own stopwords, to use in the TfidfVectorizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "english = stopwords.words('english')\n",
    "french = stopwords.words('french')\n",
    "spanish = stopwords.words('spanish')\n",
    "portuguese = stopwords.words('portuguese')\n",
    "mine = ['yeah', 'get', 'got', 'would', 'nan', 'ca']\n",
    "stop = english + french + spanish + portuguese + mine"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In addition to stopwords, there are a couple of other parameters we're interested in for the TfidvVectorizer:\n",
    "\n",
    "* **max\\_df**: When building the vocabulary ignore terms that have a document frequency  higher than the given threshold (corpus-specific stop words). \n",
    "\n",
    "* **min\\_df**: When building the vocabulary ignore terms that have a document frequency strictly lower than the given threshold. This value is also called cut-off in the literature. \n",
    "\n",
    "* **use\\_idf**: Enable inverse-document-frequency reweighting\n",
    "\n",
    "Source: http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html\n",
    "\n",
    "This tf-idf vectoizer creates one row for each document. Columns consist of the vocabulary of the corpus. Row entries include the tf-idf weight for each word that occurs in the document. If a word does not occur, a zero is entered.\n",
    "\n",
    "http://www.tfidf.com/\n",
    "\n",
    "https://en.wikipedia.org/wiki/Tf%E2%80%93idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tfidf_vectorizer = TfidfVectorizer(max_df=0.4,\n",
    "                                   min_df=0.3,\n",
    "                                   stop_words=stop,\n",
    "                                   use_idf=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we need to use our tfidf_vectorizer to fit the lyrics. We use the fit_transform method, which learns the vocabulary and idf weight, and returns a term-document matrix. Looking at the shape, we find that there are 2383 documents with a vocabulary of 4497 terms (there are only 6 terms showing up below since this was last run on a subset of data. I don't dare re-run it now, but it will be fixed later to reflect the proper number of rows)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2383, 6)\n"
     ]
    }
   ],
   "source": [
    "tfidf_matrix = tfidf_vectorizer.fit_transform(lyrics)\n",
    "\n",
    "print(tfidf_matrix.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<2383x6 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 4659 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Kmeans Clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have our TF-IDF, we can move forward with clustering. We'll first cluster the documents using Kmeans. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Kmeans Methods\n",
    "\n",
    "Since we're going to be using the kmeans algorithm, let's take a minute to understand what each of the functions does. We'll use a subset of 10 songs, with k=3.\n",
    "\n",
    "The scikit Kmeans class implements the following methods:\n",
    "\n",
    "* **fit(X[, y])** -\tCompute k-means clustering.\n",
    "* **fit_predict(X[, y])** -\tCompute cluster centers and predict cluster index for each sample.\n",
    "* **fit_transform(X[, y])** - Compute clustering and transform X to cluster-distance space.\n",
    "* **get_params([deep])** - Get parameters for this estimator.\n",
    "* **predict(X)** -Predict the closest cluster each sample in X belongs to.\n",
    "* **score(X[, y])**\t- Opposite of the value of X on the K-means objective.\n",
    "* **set_params(**params)** - Set the parameters of this estimator.\n",
    "* **transform(X[, y])** - Transform X to a cluster-distance space.\n",
    "\n",
    "We'll explore m of the methods to understand their return values a little better.\n",
    "\n",
    "Source: http://scikit-learn.org/stable/modules/generated/sklearn.cluster.KMeans.html#sklearn.cluster.KMeans.fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(KMeans(copy_x=True, init='k-means++', max_iter=300, n_clusters=3, n_init=10,\n",
       "     n_jobs=1, precompute_distances='auto', random_state=3, tol=0.0001,\n",
       "     verbose=0),\n",
       " KMeans(copy_x=True, init='k-means++', max_iter=300, n_clusters=3, n_init=10,\n",
       "     n_jobs=1, precompute_distances='auto', random_state=3, tol=0.0001,\n",
       "     verbose=0))"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lyric = lyrics[0:10]\n",
    "\n",
    "# create tf-idf\n",
    "tfidf_vectorizer = TfidfVectorizer(max_df=0.4,\n",
    "                                   min_df=0.3,\n",
    "                                   stop_words=stop,\n",
    "                                   use_idf=True)\n",
    "tfidf_matrix = tfidf_vectorizer.fit_transform(lyric)\n",
    "\n",
    "km = KMeans(n_clusters=3, init='k-means++', random_state=3)\n",
    "\n",
    "# does this actually do anything?\n",
    "km_fit = km.fit(tfidf_matrix)\n",
    "km_fit, km.fit(tfidf_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([2, 0, 1, 1, 1, 1, 0, 1, 2, 2]), array([2, 0, 1, 1, 1, 1, 0, 1, 2, 2]))"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# these returns cluster labels\n",
    "km_fit_predict = km.fit_predict(tfidf_matrix)\n",
    "km_predict = km.predict(tfidf_matrix)\n",
    "km_fit_predict, km_predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[ 1.2555158 ,  1.12609952,  0.77639247],\n",
       "        [ 0.61464526,  1.11372311,  1.20294427],\n",
       "        [ 1.12529314,  0.81076633,  1.21113623],\n",
       "        [ 1.1938697 ,  0.76702746,  1.06623655],\n",
       "        [ 1.17384819,  0.6669022 ,  1.09523193],\n",
       "        [ 1.17252196,  0.72341774,  1.0856744 ],\n",
       "        [ 0.61464526,  1.05859831,  1.17549681],\n",
       "        [ 1.23366002,  0.8595982 ,  1.21113623],\n",
       "        [ 1.27366055,  1.08961178,  0.76235779],\n",
       "        [ 1.22916683,  1.11434253,  0.64457151]]),\n",
       " array([[ 1.2555158 ,  1.12609952,  0.77639247],\n",
       "        [ 0.61464526,  1.11372311,  1.20294427],\n",
       "        [ 1.12529314,  0.81076633,  1.21113623],\n",
       "        [ 1.1938697 ,  0.76702746,  1.06623655],\n",
       "        [ 1.17384819,  0.6669022 ,  1.09523193],\n",
       "        [ 1.17252196,  0.72341774,  1.0856744 ],\n",
       "        [ 0.61464526,  1.05859831,  1.17549681],\n",
       "        [ 1.23366002,  0.8595982 ,  1.21113623],\n",
       "        [ 1.27366055,  1.08961178,  0.76235779],\n",
       "        [ 1.22916683,  1.11434253,  0.64457151]]))"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# these transform x to cluster-distance space using euclidian distance and return a numpy ndarray\n",
    "km_fit_transform = km.fit_transform(tfidf_matrix)\n",
    "km_transform = km.transform(tfidf_matrix)\n",
    "km_fit_transform, km_transform"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluating Clusters: Silhouette Scores\n",
    "\n",
    "How do we decide how well our documents are clustered? One method is to use the silhouette coefficient. From [wikipedia](https://en.wikipedia.org/wiki/Silhouette_(clustering)):\n",
    "    \n",
    "    The silhouette value is a measure of how similar an object is to its own cluster (cohesion) compared to other clusters (separation). The silhouette ranges from -1 to 1, where a high value a indicates that the object is well matched to its own cluster and badly matched to neighboring clusters. If most objects have a high value, then the clustering configuration is appropriate. If many points have a low or negative value, then the clustering configuration may have too many or too few clusters.\n",
    "    \n",
    "Scikit-Learn has two functions that calculate the silhouette coefficient. \n",
    "    \n",
    "From http://scikit-learn.org/stable/modules/generated/sklearn.metrics.silhouette_score.html:\n",
    "    \n",
    "    The Silhouette Coefficient is calculated using the mean intra-cluster distance (a) and the mean nearest-cluster distance (b) for each sample. The Silhouette Coefficient for a sample is (b - a) / max(a, b). To clarify, b is the distance between a sample and the nearest cluster that the sample is not a part of.\"\n",
    "    \n",
    "    The silhouette score is the mean silhouette score over all samples. The score ranges from -1 to 1. We want a score that's close to 1.\n",
    "    \n",
    "From http://scikit-learn.org/stable/auto_examples/cluster/plot_kmeans_silhouette_analysis.html:\n",
    "    \n",
    "    Silhoette coefficients (as these values are referred to as) near +1 indicate that the sample is far away from the neighboring clusters. A value of 0 indicates that the sample is on or very close to the decision boundary between two neighboring clusters and negative values indicate that those samples might have been assigned to the wrong cluster.\n",
    "    \n",
    "2.  After look at the sillhouette average across a range of clusters, we'll select numbers of clusters that look promising and look at the silhouette plots. This will give us a better understanding of how well our data is clustered. We'll also look at the top words in each of the clusters.\n",
    "\n",
    "\n",
    "http://scikit-learn.org/stable/auto_examples/cluster/plot_kmeans_silhouette_analysis.html\n",
    "\n",
    "We'll use the silhouette_score function to compare different numbers of clusters, using euclidean distance for the metric parameter."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we define the functions we'll use to compare clusters, create clusters, create the total vocabulary, and to print the clusters. The create_vocab and print_clusters have been adapted from http://brandonrose.org/clustering. The compare_clusters function was adapted from the text Python Data Science Cookbook, chapter 5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# function definitions---------------------------------\n",
    "def compare_clusters(lyrics, k_range, max_df, min_df, stop):    \n",
    "    # create vectorizer\n",
    "    tfidf_vectorizer = TfidfVectorizer(max_df=max_df,\n",
    "                                       min_df=min_df,\n",
    "                                       stop_words=stop,\n",
    "                                       use_idf=True)   \n",
    "    # fit it to lyrics\n",
    "    tfidf_matrix = tfidf_vectorizer.fit_transform(lyrics)   \n",
    "    \n",
    "    sil_score = []\n",
    "    for num_clust in k_range:\n",
    "        km = KMeans(n_clusters=num_clust, init='k-means++', random_state=10)\n",
    "        cluster_labels = km.fit_predict(tfidf_matrix)\n",
    "        silhouette_avg = silhouette_score(tfidf_matrix, cluster_labels, metric='euclidean')\n",
    "        sil_score.append(silhouette_avg)\n",
    "\n",
    "    #return sil_score\n",
    "    plt.figure(2)\n",
    "    plt.plot(k_range, sil_score)\n",
    "    plt.grid(b=True, which='both')\n",
    "    plt.locator_params(nbins=max(k_range)/2)\n",
    "    plt.title(\"Cluster Quality\")\n",
    "    plt.xlabel(\"Number of Clusters\")\n",
    "    plt.ylabel(\"Silhouette Coefficient\")\n",
    "    plt.show()\n",
    "    \n",
    "\n",
    "def create_clusters(lyrics, tracks, k, maxdf, mindf):\n",
    "       \n",
    "    # create vectorizer\n",
    "    tfidf_vectorizer = TfidfVectorizer(max_df=maxdf,\n",
    "                                       min_df=mindf,\n",
    "                                       stop_words=stop,\n",
    "                                       use_idf=True)\n",
    "    \n",
    "    # fit it to lyrics\n",
    "    tfidf_matrix = tfidf_vectorizer.fit_transform(lyrics)   \n",
    "    terms = tfidf_vectorizer.get_feature_names()\n",
    "    \n",
    "    # run Kmeans on the tf-idf\n",
    "    km = KMeans(n_clusters=k, init='k-means++', random_state=10)\n",
    "    cluster_labels = km.fit_predict(tfidf_matrix)\n",
    "    \n",
    "    # make pandas dataframe\n",
    "    cluster_df = pd.DataFrame(lyrics, columns = ['lyrics'])\n",
    "    cluster_df['clusters'] = cluster_labels\n",
    "    cluster_df['sil_scores'] = silhouette_samples(tfidf_matrix, cluster_labels, metric='euclidean')\n",
    "    cluster_df['tracks'] = tracks\n",
    "       \n",
    "    return (cluster_df, terms, km)\n",
    "    \n",
    "# creates total vocabulary to use in print function\n",
    "def create_vocab(text):\n",
    "    def tokenize(text):\n",
    "        tokens = [word.lower() for word in nltk.word_tokenize(text)]\n",
    "        filtered_tokens = []\n",
    "        # filter out any tokens not containing letters (e.g., numeric tokens, raw punctuation)\n",
    "        for token in tokens:\n",
    "            if re.search('[a-zA-Z]', token):\n",
    "                filtered_tokens.append(token)\n",
    "        return filtered_tokens\n",
    "    \n",
    "    total_vocab = []\n",
    "    \n",
    "    for i in lyrics:\n",
    "        allword= tokenize(i) #for each item in 'lyrics'\n",
    "        total_vocab.extend(allword) #extend the 'totalvocab_stemmed' list\n",
    "        \n",
    "    return total_vocab\n",
    "\n",
    "\n",
    "# print clusters ---------------------------------------\n",
    "def print_clusters(lyrics, terms, km):\n",
    "\n",
    "    total_vocab = create_vocab(lyrics)\n",
    "    vocab_frame = pd.DataFrame({'words': total_vocab}, index = total_vocab)    \n",
    "        \n",
    "    print(\"Top terms per cluster:\")\n",
    "    print()\n",
    "    #sort cluster centers by proximity to centroid\n",
    "    order_centroids = km.cluster_centers_.argsort()[:, ::-1] \n",
    "    \n",
    "    for i in range(num_clusters):\n",
    "        print(\"Cluster %d words:\" % i, end='')\n",
    "        \n",
    "        for ind in order_centroids[i, :12]: #replace 6 with n words per cluster\n",
    "            print(' %s' % vocab_frame.ix[terms[ind].split(' ')].values.tolist()[0][0], end=',')\n",
    "        print() #add whitespace\n",
    "        print() #add whitespace"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll use the compare_clusters function to narrow down our final cluster choices. \n",
    "\n",
    "Disclosure: I tried a lot of different max_df, min_df, and cluster number combinations. There did not seem to be a correlation between silhouette plot score and \"good\" clusters. The best silhouette scores were with a min_df of .3 and a max_df of .4 and when I printed the clusters they all looked the same. But we'll still use it as a starting point. /end disclosure\n",
    "\n",
    "We also need to include `%matplotlib inline` so our graphical output shows up in the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZkAAAEZCAYAAABFFVgWAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1ef6bed2cc0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline \n",
    "compare_clusters(lyrics, range(10,36), .8, .0, stop) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have a spikes in the silhouette coefficient at 24, so we'll explore that cluster. We'll look at the top ten words from each cluster, as well as the cluster frequency distribution.\n",
    "\n",
    "Note: despite seeding the algorithm, this function seems to be random, so it may not show a spike at 24 in further runs of this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top terms per cluster:\n",
      "\n",
      "Cluster 0 words: oh, ooh, whoa, way, know, love, go, come, everyth, day, babi, make,\n",
      "\n",
      "Cluster 1 words: come, home, feel, around, know, time, babi, make, back, hous, never, us,\n",
      "\n",
      "Cluster 2 words: girl, world, boy, love, like, one, time, make, know, tri, littl, oh,\n",
      "\n",
      "Cluster 3 words: go, gone, back, whi, bad, could, like, think, know, littl, never, better,\n",
      "\n",
      "Cluster 4 words: night, tomorrow, darl, last, forev, saturday, lone, way, one, time, love, see,\n",
      "\n",
      "Cluster 5 words: song, sing, alon, one, night, love, togeth, life, mmm, come, nan, maria,\n",
      "\n",
      "Cluster 6 words: like, nigga, shit, fuck, yall, niggaz, caus, know, wit, bitch, nan, see,\n",
      "\n",
      "Cluster 7 words: amor, quiero, si, vida, nunca, vez, tiempo, solo, corazón, voy, porqu, ser,\n",
      "\n",
      "Cluster 8 words: love, know, say, babi, never, one, heart, make, way, feel, oh, time,\n",
      "\n",
      "Cluster 9 words: said, believ, look, eye, lie, caus, find, see, could, way, back, know,\n",
      "\n",
      "Cluster 10 words: away, make, chang, say, beauti, life, hold, see, thing, feel, never, love,\n",
      "\n",
      "Cluster 11 words: ich, und, der, nicht, ist, die, mich, ein, auf, wenn, mir, sie,\n",
      "\n",
      "Cluster 12 words: noth, time, onli, never, wait, one, say, end, life, know, ever, right,\n",
      "\n",
      "Cluster 13 words: day, wish, christma, everi, one, anoth, away, merri, could, know, come, littl,\n",
      "\n",
      "Cluster 14 words: lord, prais, jesus, holi, bless, lead, god, good, woman, chorus, yes, shall,\n",
      "\n",
      "Cluster 15 words: know, need, love, say, feel, want, tell, thing, give, never, right, well,\n",
      "\n",
      "Cluster 16 words: dan, cest, tout, ell, plus, comm, mai, si, san, ça, vie, jai,\n",
      "\n",
      "Cluster 17 words: light, burn, dark, blood, heart, eye, death, fall, one, die, sky, save,\n",
      "\n",
      "Cluster 18 words: hey, oh, say, love, go, daddi, want, time, life, sorri, well, woah,\n",
      "\n",
      "Cluster 19 words: babi, love, go, know, time, let, girl, oh, caus, need, take, nan,\n",
      "\n",
      "Cluster 20 words: let, nan, go, rock, know, alright, right, rain, love, fall, tell, one,\n",
      "\n",
      "Cluster 21 words: danc, oh, night, come, feel, take, see, want, like, go, light, michael,\n",
      "\n",
      "Cluster 22 words: whoa, call, love, god, wo, like, stand, morn, fight, around, nah, know,\n",
      "\n",
      "Cluster 23 words: want, know, hear, way, tell, need, say, see, thing, alway, come, never,\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "22    309\n",
       "12    214\n",
       "17    170\n",
       "3     168\n",
       "7     150\n",
       "10    149\n",
       "6     139\n",
       "8     122\n",
       "9     121\n",
       "15     96\n",
       "20     93\n",
       "16     76\n",
       "19     70\n",
       "0      69\n",
       "1      62\n",
       "13     59\n",
       "2      52\n",
       "23     52\n",
       "5      50\n",
       "4      39\n",
       "14     32\n",
       "21     31\n",
       "18     30\n",
       "11     30\n",
       "Name: clusters, dtype: int64"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_clusters = 24 \n",
    "cluster_output = create_clusters(lyrics, tracks, num_clusters, .8, .0)\n",
    "clus_df = cluster_output[0]\n",
    "terms = cluster_output[1]\n",
    "km = cluster_output[2]\n",
    "print_clusters(lyrics, terms, km)\n",
    "clus_df['clusters'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We've used 24 clusters as the final input for our model, but this is something that could be tweaked. We may also find that the clusters aren't altogether meaningful. The silhouette scores were pretty low, indicated a lack of meaningful clusters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>clusters</th>\n",
       "      <th>tracks</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10</td>\n",
       "      <td>TRAWFGF128E0792FE0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>22</td>\n",
       "      <td>TRBGIPU128F4290454</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>15</td>\n",
       "      <td>TRAWNSM12903CA71FC</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>TRADFAQ128F428AC13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>19</td>\n",
       "      <td>TRAGBPI128E0792998</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   clusters              tracks\n",
       "0        10  TRAWFGF128E0792FE0\n",
       "1        22  TRBGIPU128F4290454\n",
       "2        15  TRAWNSM12903CA71FC\n",
       "3         3  TRADFAQ128F428AC13\n",
       "4        19  TRAGBPI128E0792998"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clus_output = clus_df.iloc[:,[1,3]]\n",
    "clus_output.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "save_load_path = '../Data/MillionSongSubset/data'\n",
    "clus_output.to_pickle(save_load_path+'/clusters.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Sentiment Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import glob, os\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sentiment analysis is the process of using natural language processing, text analysis and computational linguistics to identify and extract subjective information in source materials, according to [wikipedia](https://en.wikipedia.org/wiki/Sentiment_analysis). \n",
    "\n",
    "Words (unigrams) or groups of words (n-grams) are analyzed and given a positive, negative, and/or neutral score. For example, the word \"good\" would tend to indicate a positive word while \"bad\" would indicate a negative word. \n",
    "\n",
    "When looking at n-grams this becomes a little more complicated since the words \"not good\" would indicate a negative designation, but for the scope of this project we'll only be looking at unigrams. We're given the lyrics as an unordered list of words, so we don't have groups of words to analyze."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We perform sentiment analysis using the opinion-lexicon file, which contains most of the positive and negative words used in daily peech and literature. The algorithm goes into each file and compares each word in the lyrics to the words from the positive and negative word list. \n",
    "\n",
    "Reference: https://www.cs.uic.edu/~liub/FBS/sentiment-analysis.html#lexicon"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First set your directory path to the folder in which the files are stored. Then read in the file of positive and negative words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mydir = \"/Users/AkshaysMacBookPro/Documents/MA755/million_song_dataset/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Set your directory here\n",
    "mydir = \"/Users/AkshaysMacBookPro/Documents/MA755/million_song_dataset/\"\n",
    "\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "\n",
    "# Make sure you read in all the files for the +ve and -ve list of words\n",
    "positive_words = open('/Users/AkshaysMacBookPro/Downloads/opinion-lexicon-English/positive-words.txt','r').read()\n",
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "positive_words = tokenizer.tokenize(positive_words)\n",
    "\n",
    "# Make sure you read in all the files for the +ve and -ve list of words\n",
    "negative_words = open('/Users/AkshaysMacBookPro/Downloads/opinion-lexicon-English/negative-words.txt','r').read()\n",
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "negative_words = tokenizer.tokenize(negative_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we need to create an empty dataframe and navigate to the directory we set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Create a dataframe for the variables\n",
    "import pandas as pd\n",
    "import os\n",
    "df = pd.DataFrame({'trackID': 'TRAAAAV128F421A322.txt',\n",
    "                   'pos_score': 0.1,\n",
    "                   'neg_score': 0.09,\n",
    "                   'neu_score': 0.76,\n",
    "                   'word_count': 103},index=[0])\n",
    "\n",
    "\n",
    "# Change directory to your directory\n",
    "os.chdir(mydir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we tokenize the words. This takes a lyrics file and breaks it up into individual words. Each word is tested against the positive and negative word files. If they match, the count is incremented. If a match is not found, the count of neutral words is incremented. We do this for each word in the lyrics file. We also record the length of the sentence, which we'll use to account for varying file lengths."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Parse through the entire folder of lyrics files\n",
    "for file in glob.glob(\"*.txt\"):\n",
    "\n",
    "    # Read each file\n",
    "    data_file = open(file).read()\n",
    "\n",
    "    #Tokenize all words in each file\n",
    "    tokenizer = RegexpTokenizer(r'\\w+')\n",
    "    data_file = tokenizer.tokenize(data_file)\n",
    "\n",
    "    #Find the length of each data file\n",
    "    sentence_len = len(data_file)\n",
    "    #print(sentence_len)\n",
    "    print(file)\n",
    "\n",
    "    #Initialize each variable\n",
    "    count_neg = 0\n",
    "    count_pos = 0\n",
    "    count_neutral = 0\n",
    "\n",
    "    #Run for loop for each word and check\n",
    "    for words in data_file:\n",
    "        if words in negative_words:\n",
    "            count_neg += 1\n",
    "        elif words in positive_words:\n",
    "            count_pos += 1\n",
    "        else:\n",
    "            count_neutral += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we compute sentiment scores for each track. Each category, negative, positive, and neutral, is calculated by dividing the count of words in that category by the length of the sentence. Finally, we write these scores to a file to be used as input for further analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "    # Compute scores\n",
    "    neg_sentiment = count_neg/sentence_len\n",
    "    #print(neg_sentiment)\n",
    "    pos_sentiment = count_pos/sentence_len\n",
    "    #print(pos_sentiment)\n",
    "    neu_sentiment = count_neutral/sentence_len\n",
    "    #print(neu_sentiment)\n",
    "    #df = df.append([file,pos_sentiment,neg_sentiment,neu_sentiment, sentence_len])\n",
    "\n",
    "    # Append scores\n",
    "    df1 = pd.DataFrame({'trackID': [file],\n",
    "                        'pos_score': [pos_sentiment],\n",
    "                        'neg_score': [neg_sentiment],\n",
    "                        'neu_score': [neu_sentiment],\n",
    "                        'word_count': [sentence_len]} )\n",
    "    #print(df1)\n",
    "    df = df.append(df1, ignore_index= True)\n",
    "    print(file+\"appended\")\n",
    "\n",
    "df.to_csv('/Users/AkshaysMacBookPro/Documents/MA755/million_song_dataset/senti_file.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Final Dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we've done our cluster and text analysis using the lyrics, we can pull the data together with the musical data and use a neural network to predict `song_hotttnesss`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we load the dataframe we created from the million song subset, and get rid of the leading \"'b\" and trailing \"'\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "save_load_path = '../Data/MillionSongSubset/data'\n",
    "project_df = pd.read_pickle(save_load_path+'/project_df.pkl')\n",
    "\n",
    "# get rid of weird encoding artifact\n",
    "project_df['track_id'] = project_df['track_id'].map(lambda x: str(x)[2:len(x)+2])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "clusters = pd.read_pickle(save_load_path+'/clusters.pkl')\n",
    "\n",
    "# rename tracks to track_id for merge\n",
    "clusters.columns=['cluster','track_id']\n",
    "# need to create dummy variables from cluster\n",
    "dummies = pd.get_dummies(clusters['cluster'])\n",
    "\n",
    "clusters = pd.concat([clusters, dummies], axis=1)\n",
    "\n",
    "\n",
    "ml_df = project_df.merge(clusters, on='track_id', how='inner')\n",
    "\n",
    "ml_df.to_pickle(save_load_path+'/project_mla.pkl')\n",
    "\n",
    "# need to create dummy variables from cluster\n",
    "\n",
    "sentiment_df = pd.read_csv(save_load_path+ '/senti_file.csv')\n",
    "sentiment_df.columns=['row_id','neg_score','neu_score','pos_score','track_id', 'word_count']\n",
    "nn_df = ml_df.merge(sentiment_df, on='track_id', how='inner')\n",
    "\n",
    "\n",
    "nn_df.to_csv(save_load_path+'/nn.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sknn.mlp import Regressor, Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('/Users/AkshaysMacBookPro/Documents/MA755/nn.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>artist_id</th>\n",
       "      <th>artist_latitude</th>\n",
       "      <th>artist_location</th>\n",
       "      <th>artist_longitude</th>\n",
       "      <th>song_id</th>\n",
       "      <th>artist_name</th>\n",
       "      <th>title</th>\n",
       "      <th>artist_hotttnesss</th>\n",
       "      <th>key_confidence</th>\n",
       "      <th>mode</th>\n",
       "      <th>...</th>\n",
       "      <th>16</th>\n",
       "      <th>17</th>\n",
       "      <th>18</th>\n",
       "      <th>19</th>\n",
       "      <th>20</th>\n",
       "      <th>row_id</th>\n",
       "      <th>neg_score</th>\n",
       "      <th>neu_score</th>\n",
       "      <th>pos_score</th>\n",
       "      <th>word_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>b'ARMJAGH1187FB546F3'</td>\n",
       "      <td>35.14968</td>\n",
       "      <td>b'Memphis, TN'</td>\n",
       "      <td>-90.04892</td>\n",
       "      <td>b'SOCIWDW12A8C13D406'</td>\n",
       "      <td>b'The Box Tops'</td>\n",
       "      <td>b'Soul Deep'</td>\n",
       "      <td>0.417500</td>\n",
       "      <td>0.169</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0.088496</td>\n",
       "      <td>0.694690</td>\n",
       "      <td>0.216814</td>\n",
       "      <td>226</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>b'AR7G5I41187FB4CE6C'</td>\n",
       "      <td>NaN</td>\n",
       "      <td>b'London, England'</td>\n",
       "      <td>NaN</td>\n",
       "      <td>b'SONHOTT12A8C13493C'</td>\n",
       "      <td>b'Adam Ant'</td>\n",
       "      <td>b'Something Girls'</td>\n",
       "      <td>0.454231</td>\n",
       "      <td>0.751</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>0.057554</td>\n",
       "      <td>0.769784</td>\n",
       "      <td>0.172662</td>\n",
       "      <td>139</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>b'ARXR32B1187FB57099'</td>\n",
       "      <td>NaN</td>\n",
       "      <td>b''</td>\n",
       "      <td>NaN</td>\n",
       "      <td>b'SOFSOCN12A8C143F5D'</td>\n",
       "      <td>b'Gob'</td>\n",
       "      <td>b'Face the Ashes'</td>\n",
       "      <td>0.401724</td>\n",
       "      <td>0.092</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>0.087500</td>\n",
       "      <td>0.650000</td>\n",
       "      <td>0.262500</td>\n",
       "      <td>160</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>b'AR8ZCNI1187B9A069B'</td>\n",
       "      <td>NaN</td>\n",
       "      <td>b''</td>\n",
       "      <td>NaN</td>\n",
       "      <td>b'SOIAZJW12AB01853F1'</td>\n",
       "      <td>b'Planet P Project'</td>\n",
       "      <td>b'Pink World'</td>\n",
       "      <td>0.332276</td>\n",
       "      <td>0.717</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>12</td>\n",
       "      <td>0.098522</td>\n",
       "      <td>0.689655</td>\n",
       "      <td>0.211823</td>\n",
       "      <td>203</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>b'ARIK43K1187B9AE54C'</td>\n",
       "      <td>NaN</td>\n",
       "      <td>b'Beverly Hills, CA'</td>\n",
       "      <td>NaN</td>\n",
       "      <td>b'SOBONFF12A6D4F84D8'</td>\n",
       "      <td>b'Lionel Richie'</td>\n",
       "      <td>b'Tonight Will Be Alright'</td>\n",
       "      <td>0.553072</td>\n",
       "      <td>0.524</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>20</td>\n",
       "      <td>0.089947</td>\n",
       "      <td>0.703704</td>\n",
       "      <td>0.206349</td>\n",
       "      <td>189</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 60 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "               artist_id  artist_latitude       artist_location  \\\n",
       "0  b'ARMJAGH1187FB546F3'         35.14968        b'Memphis, TN'   \n",
       "1  b'AR7G5I41187FB4CE6C'              NaN    b'London, England'   \n",
       "2  b'ARXR32B1187FB57099'              NaN                   b''   \n",
       "3  b'AR8ZCNI1187B9A069B'              NaN                   b''   \n",
       "4  b'ARIK43K1187B9AE54C'              NaN  b'Beverly Hills, CA'   \n",
       "\n",
       "   artist_longitude                song_id          artist_name  \\\n",
       "0         -90.04892  b'SOCIWDW12A8C13D406'      b'The Box Tops'   \n",
       "1               NaN  b'SONHOTT12A8C13493C'          b'Adam Ant'   \n",
       "2               NaN  b'SOFSOCN12A8C143F5D'               b'Gob'   \n",
       "3               NaN  b'SOIAZJW12AB01853F1'  b'Planet P Project'   \n",
       "4               NaN  b'SOBONFF12A6D4F84D8'     b'Lionel Richie'   \n",
       "\n",
       "                        title  artist_hotttnesss  key_confidence  mode  \\\n",
       "0                b'Soul Deep'           0.417500           0.169     0   \n",
       "1          b'Something Girls'           0.454231           0.751     1   \n",
       "2           b'Face the Ashes'           0.401724           0.092     1   \n",
       "3               b'Pink World'           0.332276           0.717     1   \n",
       "4  b'Tonight Will Be Alright'           0.553072           0.524     1   \n",
       "\n",
       "      ...      16  17  18  19 20  row_id  neg_score  neu_score  pos_score  \\\n",
       "0     ...       0   0   0   0  0       2   0.088496   0.694690   0.216814   \n",
       "1     ...       0   0   0   1  0       4   0.057554   0.769784   0.172662   \n",
       "2     ...       0   0   0   0  0       6   0.087500   0.650000   0.262500   \n",
       "3     ...       0   0   0   0  0      12   0.098522   0.689655   0.211823   \n",
       "4     ...       0   0   0   0  0      20   0.089947   0.703704   0.206349   \n",
       "\n",
       "   word_count  \n",
       "0         226  \n",
       "1         139  \n",
       "2         160  \n",
       "3         203  \n",
       "4         189  \n",
       "\n",
       "[5 rows x 60 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>artist_id</th>\n",
       "      <th>artist_latitude</th>\n",
       "      <th>artist_location</th>\n",
       "      <th>artist_longitude</th>\n",
       "      <th>song_id</th>\n",
       "      <th>artist_name</th>\n",
       "      <th>title</th>\n",
       "      <th>artist_hotttnesss</th>\n",
       "      <th>key_confidence</th>\n",
       "      <th>mode</th>\n",
       "      <th>...</th>\n",
       "      <th>16</th>\n",
       "      <th>17</th>\n",
       "      <th>18</th>\n",
       "      <th>19</th>\n",
       "      <th>20</th>\n",
       "      <th>row_id</th>\n",
       "      <th>neg_score</th>\n",
       "      <th>neu_score</th>\n",
       "      <th>pos_score</th>\n",
       "      <th>word_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2345</th>\n",
       "      <td>b'AR5S26K1187B9A3141'</td>\n",
       "      <td>31.19224</td>\n",
       "      <td>b'Alexandria, Egypt'</td>\n",
       "      <td>29.88987</td>\n",
       "      <td>b'SOOWVUN12AB0185435'</td>\n",
       "      <td>b'Chantal Chamandy'</td>\n",
       "      <td>b'Zindegi'</td>\n",
       "      <td>0.315026</td>\n",
       "      <td>0.679</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>11018</td>\n",
       "      <td>0.082192</td>\n",
       "      <td>0.660959</td>\n",
       "      <td>0.256849</td>\n",
       "      <td>292</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2346</th>\n",
       "      <td>b'AR0ILWV1187FB47CAA'</td>\n",
       "      <td>39.75911</td>\n",
       "      <td>b'Dayton, OH'</td>\n",
       "      <td>-84.19444</td>\n",
       "      <td>b'SOQSJJL12AC46874BC'</td>\n",
       "      <td>b'Mouth Of The Architect'</td>\n",
       "      <td>b'Harboring An Apparition'</td>\n",
       "      <td>0.469056</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>11023</td>\n",
       "      <td>0.131579</td>\n",
       "      <td>0.776316</td>\n",
       "      <td>0.092105</td>\n",
       "      <td>76</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2347</th>\n",
       "      <td>b'ARJGQJD1187B997A00'</td>\n",
       "      <td>NaN</td>\n",
       "      <td>b'Buffalo, NY'</td>\n",
       "      <td>NaN</td>\n",
       "      <td>b'SOFOBSK12A8C1447C4'</td>\n",
       "      <td>b'It Dies Today'</td>\n",
       "      <td>b'One the road (to Damnation)'</td>\n",
       "      <td>0.511577</td>\n",
       "      <td>0.060</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>11024</td>\n",
       "      <td>0.061674</td>\n",
       "      <td>0.832599</td>\n",
       "      <td>0.105727</td>\n",
       "      <td>227</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2348</th>\n",
       "      <td>b'AR4C6V01187FB3BAF4'</td>\n",
       "      <td>39.55792</td>\n",
       "      <td>b'Portugal'</td>\n",
       "      <td>-7.84481</td>\n",
       "      <td>b'SOLXXPY12A67ADABA0'</td>\n",
       "      <td>b'Moonspell'</td>\n",
       "      <td>b'The Hanged Man'</td>\n",
       "      <td>0.499826</td>\n",
       "      <td>0.374</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>11027</td>\n",
       "      <td>0.076190</td>\n",
       "      <td>0.723810</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>210</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2349</th>\n",
       "      <td>b'ARYXOV81187B99831D'</td>\n",
       "      <td>NaN</td>\n",
       "      <td>b''</td>\n",
       "      <td>NaN</td>\n",
       "      <td>b'SOFAOMI12A6D4FA2D8'</td>\n",
       "      <td>b'Seventh Day Slumber'</td>\n",
       "      <td>b'Shattered Life'</td>\n",
       "      <td>0.509243</td>\n",
       "      <td>0.315</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>11037</td>\n",
       "      <td>0.101382</td>\n",
       "      <td>0.668203</td>\n",
       "      <td>0.230415</td>\n",
       "      <td>217</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 60 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                  artist_id  artist_latitude       artist_location  \\\n",
       "2345  b'AR5S26K1187B9A3141'         31.19224  b'Alexandria, Egypt'   \n",
       "2346  b'AR0ILWV1187FB47CAA'         39.75911         b'Dayton, OH'   \n",
       "2347  b'ARJGQJD1187B997A00'              NaN        b'Buffalo, NY'   \n",
       "2348  b'AR4C6V01187FB3BAF4'         39.55792           b'Portugal'   \n",
       "2349  b'ARYXOV81187B99831D'              NaN                   b''   \n",
       "\n",
       "      artist_longitude                song_id                artist_name  \\\n",
       "2345          29.88987  b'SOOWVUN12AB0185435'        b'Chantal Chamandy'   \n",
       "2346         -84.19444  b'SOQSJJL12AC46874BC'  b'Mouth Of The Architect'   \n",
       "2347               NaN  b'SOFOBSK12A8C1447C4'           b'It Dies Today'   \n",
       "2348          -7.84481  b'SOLXXPY12A67ADABA0'               b'Moonspell'   \n",
       "2349               NaN  b'SOFAOMI12A6D4FA2D8'     b'Seventh Day Slumber'   \n",
       "\n",
       "                               title  artist_hotttnesss  key_confidence  mode  \\\n",
       "2345                      b'Zindegi'           0.315026           0.679     0   \n",
       "2346      b'Harboring An Apparition'           0.469056           0.000     1   \n",
       "2347  b'One the road (to Damnation)'           0.511577           0.060     1   \n",
       "2348               b'The Hanged Man'           0.499826           0.374     1   \n",
       "2349               b'Shattered Life'           0.509243           0.315     0   \n",
       "\n",
       "         ...      16  17  18  19 20  row_id  neg_score  neu_score  pos_score  \\\n",
       "2345     ...       0   0   0   0  0   11018   0.082192   0.660959   0.256849   \n",
       "2346     ...       0   0   0   0  0   11023   0.131579   0.776316   0.092105   \n",
       "2347     ...       0   0   0   0  0   11024   0.061674   0.832599   0.105727   \n",
       "2348     ...       0   0   0   0  0   11027   0.076190   0.723810   0.200000   \n",
       "2349     ...       0   0   0   0  0   11037   0.101382   0.668203   0.230415   \n",
       "\n",
       "      word_count  \n",
       "2345         292  \n",
       "2346          76  \n",
       "2347         227  \n",
       "2348         210  \n",
       "2349         217  \n",
       "\n",
       "[5 rows x 60 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df.working = df[['artist_hotttnesss','key_confidence','loudness','tempo','neg_score','pos_score','neu_score',\n",
    "                 '4','9','13','14','16','17','18','19','20']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>artist_hotttnesss</th>\n",
       "      <th>key_confidence</th>\n",
       "      <th>loudness</th>\n",
       "      <th>tempo</th>\n",
       "      <th>neg_score</th>\n",
       "      <th>pos_score</th>\n",
       "      <th>neu_score</th>\n",
       "      <th>4</th>\n",
       "      <th>9</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "      <th>16</th>\n",
       "      <th>17</th>\n",
       "      <th>18</th>\n",
       "      <th>19</th>\n",
       "      <th>20</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.417500</td>\n",
       "      <td>0.169</td>\n",
       "      <td>-9.843</td>\n",
       "      <td>121.274</td>\n",
       "      <td>0.088496</td>\n",
       "      <td>0.216814</td>\n",
       "      <td>0.694690</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.454231</td>\n",
       "      <td>0.751</td>\n",
       "      <td>-9.013</td>\n",
       "      <td>119.293</td>\n",
       "      <td>0.057554</td>\n",
       "      <td>0.172662</td>\n",
       "      <td>0.769784</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.401724</td>\n",
       "      <td>0.092</td>\n",
       "      <td>-4.501</td>\n",
       "      <td>129.738</td>\n",
       "      <td>0.087500</td>\n",
       "      <td>0.262500</td>\n",
       "      <td>0.650000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.332276</td>\n",
       "      <td>0.717</td>\n",
       "      <td>-13.496</td>\n",
       "      <td>86.643</td>\n",
       "      <td>0.098522</td>\n",
       "      <td>0.211823</td>\n",
       "      <td>0.689655</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.553072</td>\n",
       "      <td>0.524</td>\n",
       "      <td>-8.346</td>\n",
       "      <td>125.197</td>\n",
       "      <td>0.089947</td>\n",
       "      <td>0.206349</td>\n",
       "      <td>0.703704</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   artist_hotttnesss  key_confidence  loudness    tempo  neg_score  pos_score  \\\n",
       "0           0.417500           0.169    -9.843  121.274   0.088496   0.216814   \n",
       "1           0.454231           0.751    -9.013  119.293   0.057554   0.172662   \n",
       "2           0.401724           0.092    -4.501  129.738   0.087500   0.262500   \n",
       "3           0.332276           0.717   -13.496   86.643   0.098522   0.211823   \n",
       "4           0.553072           0.524    -8.346  125.197   0.089947   0.206349   \n",
       "\n",
       "   neu_score  4  9  13  14  16  17  18  19  20  \n",
       "0   0.694690  0  0   0   0   0   0   0   0   0  \n",
       "1   0.769784  0  0   0   0   0   0   0   1   0  \n",
       "2   0.650000  0  0   1   0   0   0   0   0   0  \n",
       "3   0.689655  0  0   0   0   0   0   0   0   0  \n",
       "4   0.703704  0  0   0   0   0   0   0   0   0  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.working.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.cross_validation import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df.train, df.test = train_test_split(df.working, test_size=0.2, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>artist_hotttnesss</th>\n",
       "      <th>key_confidence</th>\n",
       "      <th>loudness</th>\n",
       "      <th>tempo</th>\n",
       "      <th>neg_score</th>\n",
       "      <th>pos_score</th>\n",
       "      <th>neu_score</th>\n",
       "      <th>4</th>\n",
       "      <th>9</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "      <th>16</th>\n",
       "      <th>17</th>\n",
       "      <th>18</th>\n",
       "      <th>19</th>\n",
       "      <th>20</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>402</th>\n",
       "      <td>0.598459</td>\n",
       "      <td>0.593</td>\n",
       "      <td>-6.962</td>\n",
       "      <td>133.903</td>\n",
       "      <td>0.003984</td>\n",
       "      <td>0.398406</td>\n",
       "      <td>0.597610</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1553</th>\n",
       "      <td>0.305579</td>\n",
       "      <td>0.462</td>\n",
       "      <td>-7.736</td>\n",
       "      <td>57.091</td>\n",
       "      <td>0.266667</td>\n",
       "      <td>0.012121</td>\n",
       "      <td>0.721212</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1056</th>\n",
       "      <td>0.578934</td>\n",
       "      <td>0.308</td>\n",
       "      <td>-4.829</td>\n",
       "      <td>89.890</td>\n",
       "      <td>0.093458</td>\n",
       "      <td>0.168224</td>\n",
       "      <td>0.738318</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1914</th>\n",
       "      <td>0.417413</td>\n",
       "      <td>0.273</td>\n",
       "      <td>-3.779</td>\n",
       "      <td>147.910</td>\n",
       "      <td>0.082251</td>\n",
       "      <td>0.134199</td>\n",
       "      <td>0.783550</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2064</th>\n",
       "      <td>0.517170</td>\n",
       "      <td>0.532</td>\n",
       "      <td>-6.076</td>\n",
       "      <td>61.593</td>\n",
       "      <td>0.125000</td>\n",
       "      <td>0.187500</td>\n",
       "      <td>0.687500</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      artist_hotttnesss  key_confidence  loudness    tempo  neg_score  \\\n",
       "402            0.598459           0.593    -6.962  133.903   0.003984   \n",
       "1553           0.305579           0.462    -7.736   57.091   0.266667   \n",
       "1056           0.578934           0.308    -4.829   89.890   0.093458   \n",
       "1914           0.417413           0.273    -3.779  147.910   0.082251   \n",
       "2064           0.517170           0.532    -6.076   61.593   0.125000   \n",
       "\n",
       "      pos_score  neu_score  4  9  13  14  16  17  18  19  20  \n",
       "402    0.398406   0.597610  0  0   0   0   0   0   0   0   0  \n",
       "1553   0.012121   0.721212  0  0   0   0   0   0   0   0   0  \n",
       "1056   0.168224   0.738318  0  0   1   0   0   0   0   0   0  \n",
       "1914   0.134199   0.783550  0  0   0   0   0   0   0   0   0  \n",
       "2064   0.187500   0.687500  0  0   1   0   0   0   0   0   0  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>artist_hotttnesss</th>\n",
       "      <th>key_confidence</th>\n",
       "      <th>loudness</th>\n",
       "      <th>tempo</th>\n",
       "      <th>neg_score</th>\n",
       "      <th>pos_score</th>\n",
       "      <th>neu_score</th>\n",
       "      <th>4</th>\n",
       "      <th>9</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "      <th>16</th>\n",
       "      <th>17</th>\n",
       "      <th>18</th>\n",
       "      <th>19</th>\n",
       "      <th>20</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>342</th>\n",
       "      <td>0.449254</td>\n",
       "      <td>0.585</td>\n",
       "      <td>-8.508</td>\n",
       "      <td>148.642</td>\n",
       "      <td>0.221739</td>\n",
       "      <td>0.017391</td>\n",
       "      <td>0.760870</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>831</th>\n",
       "      <td>0.547764</td>\n",
       "      <td>0.262</td>\n",
       "      <td>-3.183</td>\n",
       "      <td>158.340</td>\n",
       "      <td>0.155251</td>\n",
       "      <td>0.068493</td>\n",
       "      <td>0.776256</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1713</th>\n",
       "      <td>0.389107</td>\n",
       "      <td>0.364</td>\n",
       "      <td>-8.777</td>\n",
       "      <td>167.312</td>\n",
       "      <td>0.052174</td>\n",
       "      <td>0.160870</td>\n",
       "      <td>0.786957</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>616</th>\n",
       "      <td>0.349483</td>\n",
       "      <td>0.891</td>\n",
       "      <td>-5.059</td>\n",
       "      <td>139.562</td>\n",
       "      <td>0.016216</td>\n",
       "      <td>0.081081</td>\n",
       "      <td>0.902703</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2241</th>\n",
       "      <td>0.448840</td>\n",
       "      <td>0.818</td>\n",
       "      <td>-4.620</td>\n",
       "      <td>90.356</td>\n",
       "      <td>0.051471</td>\n",
       "      <td>0.183824</td>\n",
       "      <td>0.764706</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      artist_hotttnesss  key_confidence  loudness    tempo  neg_score  \\\n",
       "342            0.449254           0.585    -8.508  148.642   0.221739   \n",
       "831            0.547764           0.262    -3.183  158.340   0.155251   \n",
       "1713           0.389107           0.364    -8.777  167.312   0.052174   \n",
       "616            0.349483           0.891    -5.059  139.562   0.016216   \n",
       "2241           0.448840           0.818    -4.620   90.356   0.051471   \n",
       "\n",
       "      pos_score  neu_score  4  9  13  14  16  17  18  19  20  \n",
       "342    0.017391   0.760870  0  0   0   0   0   0   0   0   0  \n",
       "831    0.068493   0.776256  0  0   0   0   0   0   0   0   0  \n",
       "1713   0.160870   0.786957  0  0   0   0   0   0   0   0   0  \n",
       "616    0.081081   0.902703  1  0   0   0   0   0   0   0   0  \n",
       "2241   0.183824   0.764706  0  0   0   0   0   0   0   0   0  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1880, 470)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df.train), len(df.test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df.train.X = df.train.ix[:,'key_confidence':]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df.train.Y = df.train.iloc[:,[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>key_confidence</th>\n",
       "      <th>loudness</th>\n",
       "      <th>tempo</th>\n",
       "      <th>neg_score</th>\n",
       "      <th>pos_score</th>\n",
       "      <th>neu_score</th>\n",
       "      <th>4</th>\n",
       "      <th>9</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "      <th>16</th>\n",
       "      <th>17</th>\n",
       "      <th>18</th>\n",
       "      <th>19</th>\n",
       "      <th>20</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>342</th>\n",
       "      <td>0.585</td>\n",
       "      <td>-8.508</td>\n",
       "      <td>148.642</td>\n",
       "      <td>0.221739</td>\n",
       "      <td>0.017391</td>\n",
       "      <td>0.760870</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>831</th>\n",
       "      <td>0.262</td>\n",
       "      <td>-3.183</td>\n",
       "      <td>158.340</td>\n",
       "      <td>0.155251</td>\n",
       "      <td>0.068493</td>\n",
       "      <td>0.776256</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1713</th>\n",
       "      <td>0.364</td>\n",
       "      <td>-8.777</td>\n",
       "      <td>167.312</td>\n",
       "      <td>0.052174</td>\n",
       "      <td>0.160870</td>\n",
       "      <td>0.786957</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>616</th>\n",
       "      <td>0.891</td>\n",
       "      <td>-5.059</td>\n",
       "      <td>139.562</td>\n",
       "      <td>0.016216</td>\n",
       "      <td>0.081081</td>\n",
       "      <td>0.902703</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2241</th>\n",
       "      <td>0.818</td>\n",
       "      <td>-4.620</td>\n",
       "      <td>90.356</td>\n",
       "      <td>0.051471</td>\n",
       "      <td>0.183824</td>\n",
       "      <td>0.764706</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      key_confidence  loudness    tempo  neg_score  pos_score  neu_score  4  \\\n",
       "342            0.585    -8.508  148.642   0.221739   0.017391   0.760870  0   \n",
       "831            0.262    -3.183  158.340   0.155251   0.068493   0.776256  0   \n",
       "1713           0.364    -8.777  167.312   0.052174   0.160870   0.786957  0   \n",
       "616            0.891    -5.059  139.562   0.016216   0.081081   0.902703  1   \n",
       "2241           0.818    -4.620   90.356   0.051471   0.183824   0.764706  0   \n",
       "\n",
       "      9  13  14  16  17  18  19  20  \n",
       "342   0   0   0   0   0   0   0   0  \n",
       "831   0   0   0   0   0   0   0   0  \n",
       "1713  0   0   0   0   0   0   0   0  \n",
       "616   0   0   0   0   0   0   0   0  \n",
       "2241  0   0   0   0   0   0   0   0  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.train.X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>artist_hotttnesss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>342</th>\n",
       "      <td>0.449254</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>831</th>\n",
       "      <td>0.547764</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1713</th>\n",
       "      <td>0.389107</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>616</th>\n",
       "      <td>0.349483</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2241</th>\n",
       "      <td>0.448840</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      artist_hotttnesss\n",
       "342            0.449254\n",
       "831            0.547764\n",
       "1713           0.389107\n",
       "616            0.349483\n",
       "2241           0.448840"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.train.Y.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "numpyMatrixTrainX = df.train.X.as_matrix()\n",
    "numpyMatrixTrainY = df.train.Y.as_matrix()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[   0.585,   -8.508,  148.642, ...,    0.   ,    0.   ,    0.   ],\n",
       "       [   0.262,   -3.183,  158.34 , ...,    0.   ,    0.   ,    0.   ],\n",
       "       [   0.364,   -8.777,  167.312, ...,    0.   ,    0.   ,    0.   ],\n",
       "       ..., \n",
       "       [   0.834,   -4.422,  113.608, ...,    0.   ,    0.   ,    0.   ],\n",
       "       [   1.   ,  -13.45 ,   71.233, ...,    0.   ,    0.   ,    0.   ],\n",
       "       [   0.972,   -9.417,  141.271, ...,    0.   ,    0.   ,    0.   ]])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "numpyMatrixTrainX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "nn = Regressor(\n",
    "    layers=[\n",
    "        Layer(\"Rectifier\", units=2),\n",
    "        Layer(\"Linear\")],\n",
    "    learning_rate=0.001,\n",
    "    n_iter=2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Regressor(batch_size=1, callback=None, debug=False, dropout_rate=None,\n",
       "     f_stable=0.001,\n",
       "     hidden0=<sknn.nn.Layer `Rectifier`: frozen=False, units=2, name='hidden0'>,\n",
       "     layers=[<sknn.nn.Layer `Rectifier`: frozen=False, units=2, name='hidden0'>, <sknn.nn.Layer `Linear`: frozen=False, units=1, name='output'>],\n",
       "     learning_momentum=0.9, learning_rate=0.001, learning_rule='sgd',\n",
       "     loss_type=None, n_iter=2000, n_stable=10, normalize=None,\n",
       "     output=<sknn.nn.Layer `Linear`: frozen=False, units=1, name='output'>,\n",
       "     parameters=None, random_state=None, regularize=None, valid_set=None,\n",
       "     valid_size=0.0, verbose=None, warning=None, weight_decay=None)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nn.fit(numpyMatrixTrainX,\n",
    "       numpyMatrixTrainY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[   0.593,   -6.962,  133.903, ...,    0.   ,    0.   ,    0.   ],\n",
       "       [   0.462,   -7.736,   57.091, ...,    0.   ,    0.   ,    0.   ],\n",
       "       [   0.308,   -4.829,   89.89 , ...,    0.   ,    0.   ,    0.   ],\n",
       "       ..., \n",
       "       [   0.814,   -8.647,   83.195, ...,    0.   ,    0.   ,    0.   ],\n",
       "       [   0.184,   -7.736,  110.255, ...,    0.   ,    0.   ,    0.   ],\n",
       "       [   0.386,   -9.37 ,  102.677, ...,    0.   ,    0.   ,    0.   ]])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.test.X = df.test.ix[:,'key_confidence':]\n",
    "df.test.Y = df.test.iloc[:,[0]]\n",
    "numpyMatrixTestX = df.test.X.as_matrix()\n",
    "numpyMatrixTestY = df.test.Y.as_matrix()\n",
    "numpyMatrixTestX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Root Mean Squared Error: 0.12410\n"
     ]
    }
   ],
   "source": [
    "print(\"Root Mean Squared Error: %.5f\"%np.mean((nn.predict(numpyMatrixTestX) - numpyMatrixTestY) ** 2)**(0.5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variance score: -0.00025\n"
     ]
    }
   ],
   "source": [
    "print('Variance score: %.5f' % nn.score(numpyMatrixTestX, numpyMatrixTestY))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "470\n",
      "470\n"
     ]
    }
   ],
   "source": [
    "print(len(numpyMatrixTestX))\n",
    "print(len(numpyMatrixTestY))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAV0AAADtCAYAAAAcNaZ2AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAABMhJREFUeJzt2k1qVFkYgOHvmqRRURSHrsBtOHTkJtyCI4fi2BW4CREE\nZzpxJ/4MBEVRIcjtQXWjNjHa3clbap4HLknl3tQ5Bclbh3NrWdd1AGic2vYEAE4S0QUIiS5ASHQB\nQqILEBJdgNDuYSeXZfF5MoD/YF3X5aCfHxrdv37x6GcD8BtblgN7OzO2FwBSogsQEl2AkOgChEQX\nICS6ACHRBQiJLkBIdAFCogsQEl2AkOgChEQXICS6ACHRBQiJLkBIdAFCogsQEl2AkOgChEQXICS6\nACHRBQiJLkBIdAFCogsQEl2AkOgChEQXICS6ACHRBQiJLkBIdAFCogsQEl2AkOgChEQXICS6ACHR\nBQiJLkBIdAFCogsQEl2AkOgChEQXICS6ACHRBQiJLkBIdAFCogsQEl2AkOgChEQXICS6ACHRBQiJ\nLkBIdAFCogsQEl2AkOgChEQXICS6ACHRBQiJLkBIdAFCogsQEl2AkOgChEQXICS6ACHRBQiJLkBI\ndAFCogsQEl2AkOgChEQXICS6ACHRBQiJLkBIdAFCogsQEl2AkOgChEQXICS6ACHRBQiJLkBIdAFC\nogsQEl2AkOgChHbLwV6+nLl5c+b+/Zk3b45vnGvXZh4+/Pz4/fuZM2eOb7yZmXWduXt35smT71/7\n+vXM48fHOx/YhuvXj/45r1yZuX17Zm/v6J97G5Z1Xb99clnWw87/G58+zeymif/aEb2Mb7p8eeb5\n8+MdA06y4/4fPkrLssy6rstB57IMHufK9kfs7898/Djz4cNm5fv27cy7d4cfP3LN/v52Xxfwa8mi\ne+nSzK1bM3fuVCN+7Y8/tjMu8P89eLDtGRydbHvhON24MXPv3ub7Cxc2+7cvXmx3Tj+znZ3NVs/f\nx97e14+3dRTzOOXWMYHDthd+i+g+fbq5eXbx4syjRzPPns1cvdqMvbc3c+7cwcf585uvZ89uVtrb\nCs2Xx87OzHLgnwJwVH776M5s9lZ3dj6vZL4My6tXM6dPW+0AjZ/iRtpx++fHSX6R9wrghLHmAwiJ\nLkBIdAFCogsQEl2AkOgChEQXICS6ACHRBQiJLkBIdAFCogsQEl2AkOgChEQXICS6ACHRBQiJLkBI\ndAFCogsQEl2AkOgChEQXICS6ACHRBQiJLkBIdAFCogsQEl2AkOgChEQXICS6ACHRBQiJLkBIdAFC\nogsQEl2AkOgChEQXICS6ACHRBQiJLkBIdAFCogsQEl2AkOgChEQXICS6ACHRBQiJLkBIdAFCogsQ\nEl2AkOgChEQXICS6ACHRBQiJLkBIdAFCogsQEl2AkOgChEQXICS6ACHRBQiJLkBIdAFCogsQEl2A\nkOgChEQXICS6ACHRBQiJLkBIdAFCogsQEl2AkOgChEQXICS6ACHRBQiJLkBIdAFCogsQEl2AkOgC\nhEQXICS6ACHRBQiJLkBIdAFCogsQEl2AkOgChEQXICS6ACHRBQiJLkBIdAFCogsQEl2AkOgChEQX\nICS6ACHRBQiJLkBIdAFCogsQEl2AkOgChEQXICS6ACHRBQiJLkBIdAFCogsQEl2AkOgChEQXICS6\nACHRBQjtfu+CZVmKeQCcCMu6rtueA8CJYXsBICS6ACHRBQiJLkBIdAFCfwLWZJhYIRIcIQAAAABJ\nRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1196adba8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "#plt.scatter(numpyMatrixTestX, numpyMatrixTestY,  color='black')\n",
    "plt.plot(numpyMatrixTestX, nn.predict(numpyMatrixTestX), color='blue',\n",
    "         linewidth=3)\n",
    "\n",
    "plt.xticks(())\n",
    "plt.yticks(())\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "We started with the basic variables. There was an RMSE of 0.12451. Then we brought in the sentiment scores. They decreased the RMSE by 0.00012. Last, we brought in the clusters and they increased the RMSE. This wasn't surpising given that the cluster silhouette scores were quite low.\n",
    "\n",
    "Next steps: given more time we would compare the outcome of the neural network analysis with SVM and ensemble regressors, such as random forest, and bagging/boosting. We also may bring in some of the confidence variables. For example, key has a companion confidence variable that quantifies how confident the data is that they key is correct. \n",
    "\n",
    "We would also like to extend this to include more of the lyrics. Since we used the subset we were only left with 2350 records after bringing in the lyrics. However, if we used a greater part of the million songs dataset we might be able to work with more records. This would be particularly important if we were using the confidence variables since many records wouldn't make it into our set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### References\n",
    "\n",
    "**TF/IDF**\n",
    "\n",
    "http://aimotion.blogspot.com/2011/12/machine-learning-with-python-meeting-tf.html\n",
    "\n",
    "http://stevenloria.com/finding-important-words-in-a-document-using-tf-idf/\n",
    "\n",
    "**Scikit-Learn Functions**\n",
    "\n",
    "http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html\n",
    "\n",
    "http://scikit-learn.org/stable/modules/generated/sklearn.metrics.pairwise.pairwise_distances.html#sklearn.metrics.pairwise.pairwise_distances\n",
    "\n",
    "http://scikit-learn.org/stable/auto_examples/text/document_clustering.html\n",
    "\n",
    "**Document Clustering Examples**\n",
    "\n",
    "https://de.dariah.eu/tatom/working_with_text.html\n",
    "\n",
    "https://github.com/grfiv/haiku_analysis/blob/master/Bag-of-Words%20Analysis%20Haiku%20(unsupervised,%20tf-idf%20%26%20kmeans,%20Silhouette).ipynb\n",
    "\n",
    "http://brandonrose.org/clustering    \n",
    "\n",
    "Chapter 5 of the text: Python Data Science Cookbook"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
